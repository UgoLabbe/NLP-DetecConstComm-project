{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3240,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009259259259259259,
      "grad_norm": 4.439280033111572,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.7277,
      "step": 10
    },
    {
      "epoch": 0.018518518518518517,
      "grad_norm": 8.950873374938965,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.7228,
      "step": 20
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 3.9286797046661377,
      "learning_rate": 3e-06,
      "loss": 0.6769,
      "step": 30
    },
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 8.793628692626953,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.6457,
      "step": 40
    },
    {
      "epoch": 0.046296296296296294,
      "grad_norm": 5.118696689605713,
      "learning_rate": 5e-06,
      "loss": 0.5729,
      "step": 50
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 9.963205337524414,
      "learning_rate": 6e-06,
      "loss": 0.521,
      "step": 60
    },
    {
      "epoch": 0.06481481481481481,
      "grad_norm": 7.262749671936035,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.5126,
      "step": 70
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 18.62522315979004,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.4464,
      "step": 80
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 7.297521591186523,
      "learning_rate": 9e-06,
      "loss": 0.4239,
      "step": 90
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 9.70535659790039,
      "learning_rate": 1e-05,
      "loss": 0.3768,
      "step": 100
    },
    {
      "epoch": 0.10185185185185185,
      "grad_norm": 8.339592933654785,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.3766,
      "step": 110
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 8.104011535644531,
      "learning_rate": 1.2e-05,
      "loss": 0.3453,
      "step": 120
    },
    {
      "epoch": 0.12037037037037036,
      "grad_norm": 3.6527509689331055,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.3225,
      "step": 130
    },
    {
      "epoch": 0.12962962962962962,
      "grad_norm": 3.9925076961517334,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.2968,
      "step": 140
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 14.856379508972168,
      "learning_rate": 1.5e-05,
      "loss": 0.2846,
      "step": 150
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 26.045665740966797,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.2059,
      "step": 160
    },
    {
      "epoch": 0.1574074074074074,
      "grad_norm": 7.1177496910095215,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.182,
      "step": 170
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 13.809427261352539,
      "learning_rate": 1.8e-05,
      "loss": 0.3152,
      "step": 180
    },
    {
      "epoch": 0.17592592592592593,
      "grad_norm": 13.220126152038574,
      "learning_rate": 1.9e-05,
      "loss": 0.1773,
      "step": 190
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 1.443801760673523,
      "learning_rate": 2e-05,
      "loss": 0.279,
      "step": 200
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 34.226741790771484,
      "learning_rate": 2.1e-05,
      "loss": 0.1139,
      "step": 210
    },
    {
      "epoch": 0.2037037037037037,
      "grad_norm": 11.968201637268066,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.3312,
      "step": 220
    },
    {
      "epoch": 0.21296296296296297,
      "grad_norm": 0.835019588470459,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.1139,
      "step": 230
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.294230580329895,
      "learning_rate": 2.4e-05,
      "loss": 0.4039,
      "step": 240
    },
    {
      "epoch": 0.23148148148148148,
      "grad_norm": 2.2741663455963135,
      "learning_rate": 2.5e-05,
      "loss": 0.0633,
      "step": 250
    },
    {
      "epoch": 0.24074074074074073,
      "grad_norm": 0.23479712009429932,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.2107,
      "step": 260
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.5181750059127808,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.3893,
      "step": 270
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 13.907724380493164,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.315,
      "step": 280
    },
    {
      "epoch": 0.26851851851851855,
      "grad_norm": 3.4592015743255615,
      "learning_rate": 2.9e-05,
      "loss": 0.1825,
      "step": 290
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 6.731849193572998,
      "learning_rate": 3e-05,
      "loss": 0.3639,
      "step": 300
    },
    {
      "epoch": 0.28703703703703703,
      "grad_norm": 0.5060593485832214,
      "learning_rate": 3.1e-05,
      "loss": 0.2353,
      "step": 310
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 16.037940979003906,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.362,
      "step": 320
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 21.230981826782227,
      "learning_rate": 3.3e-05,
      "loss": 0.3276,
      "step": 330
    },
    {
      "epoch": 0.3148148148148148,
      "grad_norm": 1.3190861940383911,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.5283,
      "step": 340
    },
    {
      "epoch": 0.32407407407407407,
      "grad_norm": 44.62590789794922,
      "learning_rate": 3.5e-05,
      "loss": 0.2121,
      "step": 350
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5778866410255432,
      "learning_rate": 3.6e-05,
      "loss": 0.2728,
      "step": 360
    },
    {
      "epoch": 0.3425925925925926,
      "grad_norm": 25.428504943847656,
      "learning_rate": 3.7e-05,
      "loss": 0.451,
      "step": 370
    },
    {
      "epoch": 0.35185185185185186,
      "grad_norm": 5.642666816711426,
      "learning_rate": 3.8e-05,
      "loss": 0.3977,
      "step": 380
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 0.845439076423645,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.4699,
      "step": 390
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 5.4814677238464355,
      "learning_rate": 4e-05,
      "loss": 0.4322,
      "step": 400
    },
    {
      "epoch": 0.37962962962962965,
      "grad_norm": 4.990299224853516,
      "learning_rate": 4.1e-05,
      "loss": 0.196,
      "step": 410
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.4359472990036011,
      "learning_rate": 4.2e-05,
      "loss": 0.1486,
      "step": 420
    },
    {
      "epoch": 0.39814814814814814,
      "grad_norm": 12.695402145385742,
      "learning_rate": 4.3e-05,
      "loss": 0.368,
      "step": 430
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 17.017972946166992,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.4757,
      "step": 440
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.9060226678848267,
      "learning_rate": 4.5e-05,
      "loss": 0.2318,
      "step": 450
    },
    {
      "epoch": 0.42592592592592593,
      "grad_norm": 32.93205261230469,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.3045,
      "step": 460
    },
    {
      "epoch": 0.4351851851851852,
      "grad_norm": 0.1469859927892685,
      "learning_rate": 4.7e-05,
      "loss": 0.089,
      "step": 470
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.09950914233922958,
      "learning_rate": 4.8e-05,
      "loss": 0.2796,
      "step": 480
    },
    {
      "epoch": 0.4537037037037037,
      "grad_norm": 0.2805453836917877,
      "learning_rate": 4.9e-05,
      "loss": 0.1928,
      "step": 490
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 16.2227840423584,
      "learning_rate": 5e-05,
      "loss": 0.4741,
      "step": 500
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.7126441597938538,
      "learning_rate": 4.981751824817518e-05,
      "loss": 0.3374,
      "step": 510
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 0.6404715776443481,
      "learning_rate": 4.963503649635037e-05,
      "loss": 0.2822,
      "step": 520
    },
    {
      "epoch": 0.49074074074074076,
      "grad_norm": 1.6676695346832275,
      "learning_rate": 4.945255474452555e-05,
      "loss": 0.1263,
      "step": 530
    },
    {
      "epoch": 0.5,
      "grad_norm": 4.41133975982666,
      "learning_rate": 4.927007299270073e-05,
      "loss": 0.1778,
      "step": 540
    },
    {
      "epoch": 0.5092592592592593,
      "grad_norm": 0.1842283457517624,
      "learning_rate": 4.908759124087591e-05,
      "loss": 0.26,
      "step": 550
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 0.1986425369977951,
      "learning_rate": 4.89051094890511e-05,
      "loss": 0.2939,
      "step": 560
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 0.8916694521903992,
      "learning_rate": 4.872262773722628e-05,
      "loss": 0.2802,
      "step": 570
    },
    {
      "epoch": 0.5370370370370371,
      "grad_norm": 23.042749404907227,
      "learning_rate": 4.854014598540147e-05,
      "loss": 0.3836,
      "step": 580
    },
    {
      "epoch": 0.5462962962962963,
      "grad_norm": 12.652247428894043,
      "learning_rate": 4.835766423357664e-05,
      "loss": 0.5146,
      "step": 590
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.8540802001953125,
      "learning_rate": 4.817518248175183e-05,
      "loss": 0.2038,
      "step": 600
    },
    {
      "epoch": 0.5648148148148148,
      "grad_norm": 5.484358787536621,
      "learning_rate": 4.799270072992701e-05,
      "loss": 0.3813,
      "step": 610
    },
    {
      "epoch": 0.5740740740740741,
      "grad_norm": 0.2810669243335724,
      "learning_rate": 4.7810218978102196e-05,
      "loss": 0.1666,
      "step": 620
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.2794305682182312,
      "learning_rate": 4.762773722627738e-05,
      "loss": 0.178,
      "step": 630
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 8.35552978515625,
      "learning_rate": 4.744525547445256e-05,
      "loss": 0.3932,
      "step": 640
    },
    {
      "epoch": 0.6018518518518519,
      "grad_norm": 1.274025321006775,
      "learning_rate": 4.726277372262774e-05,
      "loss": 0.2676,
      "step": 650
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.08667869865894318,
      "learning_rate": 4.708029197080292e-05,
      "loss": 0.0823,
      "step": 660
    },
    {
      "epoch": 0.6203703703703703,
      "grad_norm": 33.97312927246094,
      "learning_rate": 4.6897810218978106e-05,
      "loss": 0.2274,
      "step": 670
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 1.1098508834838867,
      "learning_rate": 4.6715328467153287e-05,
      "loss": 0.4919,
      "step": 680
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 4.724392414093018,
      "learning_rate": 4.6532846715328474e-05,
      "loss": 0.454,
      "step": 690
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 4.841236591339111,
      "learning_rate": 4.635036496350365e-05,
      "loss": 0.159,
      "step": 700
    },
    {
      "epoch": 0.6574074074074074,
      "grad_norm": 0.6893365383148193,
      "learning_rate": 4.6167883211678835e-05,
      "loss": 0.3592,
      "step": 710
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 7.267207622528076,
      "learning_rate": 4.5985401459854016e-05,
      "loss": 0.3921,
      "step": 720
    },
    {
      "epoch": 0.6759259259259259,
      "grad_norm": 0.6260819435119629,
      "learning_rate": 4.58029197080292e-05,
      "loss": 0.2319,
      "step": 730
    },
    {
      "epoch": 0.6851851851851852,
      "grad_norm": 6.07451057434082,
      "learning_rate": 4.5620437956204383e-05,
      "loss": 0.2954,
      "step": 740
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.40631103515625,
      "learning_rate": 4.5437956204379564e-05,
      "loss": 0.2651,
      "step": 750
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 0.30304569005966187,
      "learning_rate": 4.5255474452554745e-05,
      "loss": 0.264,
      "step": 760
    },
    {
      "epoch": 0.7129629629629629,
      "grad_norm": 7.231122016906738,
      "learning_rate": 4.5072992700729925e-05,
      "loss": 0.1915,
      "step": 770
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.2893381118774414,
      "learning_rate": 4.489051094890511e-05,
      "loss": 0.1831,
      "step": 780
    },
    {
      "epoch": 0.7314814814814815,
      "grad_norm": 10.34627914428711,
      "learning_rate": 4.470802919708029e-05,
      "loss": 0.2437,
      "step": 790
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 4.937499046325684,
      "learning_rate": 4.452554744525548e-05,
      "loss": 0.3024,
      "step": 800
    },
    {
      "epoch": 0.75,
      "grad_norm": 6.9688191413879395,
      "learning_rate": 4.434306569343066e-05,
      "loss": 0.2544,
      "step": 810
    },
    {
      "epoch": 0.7592592592592593,
      "grad_norm": 0.4539767801761627,
      "learning_rate": 4.416058394160584e-05,
      "loss": 0.3285,
      "step": 820
    },
    {
      "epoch": 0.7685185185185185,
      "grad_norm": 0.8286867737770081,
      "learning_rate": 4.397810218978102e-05,
      "loss": 0.2803,
      "step": 830
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 7.3748087882995605,
      "learning_rate": 4.379562043795621e-05,
      "loss": 0.3551,
      "step": 840
    },
    {
      "epoch": 0.7870370370370371,
      "grad_norm": 2.4606430530548096,
      "learning_rate": 4.361313868613139e-05,
      "loss": 0.2191,
      "step": 850
    },
    {
      "epoch": 0.7962962962962963,
      "grad_norm": 4.6889848709106445,
      "learning_rate": 4.343065693430657e-05,
      "loss": 0.1092,
      "step": 860
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 0.4675692915916443,
      "learning_rate": 4.324817518248175e-05,
      "loss": 0.214,
      "step": 870
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 16.90774154663086,
      "learning_rate": 4.306569343065693e-05,
      "loss": 0.255,
      "step": 880
    },
    {
      "epoch": 0.8240740740740741,
      "grad_norm": 0.7255963087081909,
      "learning_rate": 4.288321167883212e-05,
      "loss": 0.2293,
      "step": 890
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.1807517558336258,
      "learning_rate": 4.27007299270073e-05,
      "loss": 0.2189,
      "step": 900
    },
    {
      "epoch": 0.8425925925925926,
      "grad_norm": 0.17579303681850433,
      "learning_rate": 4.251824817518249e-05,
      "loss": 0.2353,
      "step": 910
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 0.48084840178489685,
      "learning_rate": 4.233576642335767e-05,
      "loss": 0.2998,
      "step": 920
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 4.438077449798584,
      "learning_rate": 4.215328467153285e-05,
      "loss": 0.1686,
      "step": 930
    },
    {
      "epoch": 0.8703703703703703,
      "grad_norm": 0.4420095384120941,
      "learning_rate": 4.197080291970803e-05,
      "loss": 0.1852,
      "step": 940
    },
    {
      "epoch": 0.8796296296296297,
      "grad_norm": 48.188045501708984,
      "learning_rate": 4.1788321167883216e-05,
      "loss": 0.2445,
      "step": 950
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.7552152872085571,
      "learning_rate": 4.16058394160584e-05,
      "loss": 0.244,
      "step": 960
    },
    {
      "epoch": 0.8981481481481481,
      "grad_norm": 3.6436641216278076,
      "learning_rate": 4.1423357664233584e-05,
      "loss": 0.268,
      "step": 970
    },
    {
      "epoch": 0.9074074074074074,
      "grad_norm": 0.30063924193382263,
      "learning_rate": 4.124087591240876e-05,
      "loss": 0.2608,
      "step": 980
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.3663531541824341,
      "learning_rate": 4.1058394160583945e-05,
      "loss": 0.2863,
      "step": 990
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 7.820816993713379,
      "learning_rate": 4.0875912408759126e-05,
      "loss": 0.2105,
      "step": 1000
    },
    {
      "epoch": 0.9351851851851852,
      "grad_norm": 12.176673889160156,
      "learning_rate": 4.0693430656934306e-05,
      "loss": 0.2019,
      "step": 1010
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 10.591497421264648,
      "learning_rate": 4.0510948905109494e-05,
      "loss": 0.3357,
      "step": 1020
    },
    {
      "epoch": 0.9537037037037037,
      "grad_norm": 0.48256218433380127,
      "learning_rate": 4.0328467153284674e-05,
      "loss": 0.2952,
      "step": 1030
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 1.1705836057662964,
      "learning_rate": 4.0145985401459855e-05,
      "loss": 0.3076,
      "step": 1040
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 10.397361755371094,
      "learning_rate": 3.9963503649635035e-05,
      "loss": 0.1307,
      "step": 1050
    },
    {
      "epoch": 0.9814814814814815,
      "grad_norm": 4.195181846618652,
      "learning_rate": 3.978102189781022e-05,
      "loss": 0.2612,
      "step": 1060
    },
    {
      "epoch": 0.9907407407407407,
      "grad_norm": 0.3822416067123413,
      "learning_rate": 3.95985401459854e-05,
      "loss": 0.4037,
      "step": 1070
    },
    {
      "epoch": 1.0,
      "grad_norm": 76.28103637695312,
      "learning_rate": 3.941605839416059e-05,
      "loss": 0.1608,
      "step": 1080
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.23229672014713287,
      "eval_runtime": 7.0351,
      "eval_samples_per_second": 136.459,
      "eval_steps_per_second": 8.529,
      "step": 1080
    },
    {
      "epoch": 1.0092592592592593,
      "grad_norm": 0.5451074242591858,
      "learning_rate": 3.9233576642335764e-05,
      "loss": 0.2136,
      "step": 1090
    },
    {
      "epoch": 1.0185185185185186,
      "grad_norm": 70.72721862792969,
      "learning_rate": 3.905109489051095e-05,
      "loss": 0.3003,
      "step": 1100
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 15.410810470581055,
      "learning_rate": 3.886861313868613e-05,
      "loss": 0.4374,
      "step": 1110
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 9.219738960266113,
      "learning_rate": 3.868613138686132e-05,
      "loss": 0.3728,
      "step": 1120
    },
    {
      "epoch": 1.0462962962962963,
      "grad_norm": 4.477297306060791,
      "learning_rate": 3.85036496350365e-05,
      "loss": 0.2057,
      "step": 1130
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 1.0985395908355713,
      "learning_rate": 3.832116788321168e-05,
      "loss": 0.34,
      "step": 1140
    },
    {
      "epoch": 1.0648148148148149,
      "grad_norm": 0.7224826216697693,
      "learning_rate": 3.813868613138686e-05,
      "loss": 0.1282,
      "step": 1150
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 0.3357544541358948,
      "learning_rate": 3.795620437956204e-05,
      "loss": 0.1826,
      "step": 1160
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 5.047911643981934,
      "learning_rate": 3.777372262773723e-05,
      "loss": 0.2714,
      "step": 1170
    },
    {
      "epoch": 1.0925925925925926,
      "grad_norm": 0.5495461821556091,
      "learning_rate": 3.759124087591241e-05,
      "loss": 0.1997,
      "step": 1180
    },
    {
      "epoch": 1.1018518518518519,
      "grad_norm": 10.729269981384277,
      "learning_rate": 3.74087591240876e-05,
      "loss": 0.3091,
      "step": 1190
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.12493829429149628,
      "learning_rate": 3.722627737226278e-05,
      "loss": 0.0942,
      "step": 1200
    },
    {
      "epoch": 1.1203703703703705,
      "grad_norm": 0.14373348653316498,
      "learning_rate": 3.704379562043796e-05,
      "loss": 0.2733,
      "step": 1210
    },
    {
      "epoch": 1.1296296296296295,
      "grad_norm": 7.311362266540527,
      "learning_rate": 3.686131386861314e-05,
      "loss": 0.1632,
      "step": 1220
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 8.133875846862793,
      "learning_rate": 3.6678832116788326e-05,
      "loss": 0.6002,
      "step": 1230
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 0.5473870635032654,
      "learning_rate": 3.649635036496351e-05,
      "loss": 0.2669,
      "step": 1240
    },
    {
      "epoch": 1.1574074074074074,
      "grad_norm": 10.900237083435059,
      "learning_rate": 3.631386861313869e-05,
      "loss": 0.2427,
      "step": 1250
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.6567394733428955,
      "learning_rate": 3.613138686131387e-05,
      "loss": 0.2181,
      "step": 1260
    },
    {
      "epoch": 1.175925925925926,
      "grad_norm": 0.9485209584236145,
      "learning_rate": 3.594890510948905e-05,
      "loss": 0.2947,
      "step": 1270
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 3.9583888053894043,
      "learning_rate": 3.5766423357664236e-05,
      "loss": 0.2967,
      "step": 1280
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 0.8656381964683533,
      "learning_rate": 3.5583941605839416e-05,
      "loss": 0.2153,
      "step": 1290
    },
    {
      "epoch": 1.2037037037037037,
      "grad_norm": 0.5189133286476135,
      "learning_rate": 3.5401459854014604e-05,
      "loss": 0.3087,
      "step": 1300
    },
    {
      "epoch": 1.212962962962963,
      "grad_norm": 0.31142711639404297,
      "learning_rate": 3.5218978102189784e-05,
      "loss": 0.1915,
      "step": 1310
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.2655770182609558,
      "learning_rate": 3.5036496350364965e-05,
      "loss": 0.166,
      "step": 1320
    },
    {
      "epoch": 1.2314814814814814,
      "grad_norm": 0.2360234558582306,
      "learning_rate": 3.4854014598540145e-05,
      "loss": 0.1113,
      "step": 1330
    },
    {
      "epoch": 1.2407407407407407,
      "grad_norm": 2.618722438812256,
      "learning_rate": 3.467153284671533e-05,
      "loss": 0.2323,
      "step": 1340
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.6978002190589905,
      "learning_rate": 3.448905109489051e-05,
      "loss": 0.2922,
      "step": 1350
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 12.812004089355469,
      "learning_rate": 3.43065693430657e-05,
      "loss": 0.2527,
      "step": 1360
    },
    {
      "epoch": 1.2685185185185186,
      "grad_norm": 0.287331759929657,
      "learning_rate": 3.4124087591240875e-05,
      "loss": 0.2072,
      "step": 1370
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.8153550028800964,
      "learning_rate": 3.3941605839416055e-05,
      "loss": 0.3383,
      "step": 1380
    },
    {
      "epoch": 1.287037037037037,
      "grad_norm": 0.49449458718299866,
      "learning_rate": 3.375912408759124e-05,
      "loss": 0.1344,
      "step": 1390
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 45.83900451660156,
      "learning_rate": 3.357664233576642e-05,
      "loss": 0.4218,
      "step": 1400
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 0.1926574558019638,
      "learning_rate": 3.339416058394161e-05,
      "loss": 0.0696,
      "step": 1410
    },
    {
      "epoch": 1.3148148148148149,
      "grad_norm": 0.22313913702964783,
      "learning_rate": 3.321167883211679e-05,
      "loss": 0.4258,
      "step": 1420
    },
    {
      "epoch": 1.324074074074074,
      "grad_norm": 4.107224464416504,
      "learning_rate": 3.302919708029197e-05,
      "loss": 0.2013,
      "step": 1430
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 6.190626621246338,
      "learning_rate": 3.284671532846715e-05,
      "loss": 0.1957,
      "step": 1440
    },
    {
      "epoch": 1.3425925925925926,
      "grad_norm": 0.31058794260025024,
      "learning_rate": 3.266423357664234e-05,
      "loss": 0.2911,
      "step": 1450
    },
    {
      "epoch": 1.3518518518518519,
      "grad_norm": 26.323379516601562,
      "learning_rate": 3.248175182481752e-05,
      "loss": 0.2278,
      "step": 1460
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 36.89936065673828,
      "learning_rate": 3.229927007299271e-05,
      "loss": 0.1943,
      "step": 1470
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 0.47398218512535095,
      "learning_rate": 3.211678832116788e-05,
      "loss": 0.2322,
      "step": 1480
    },
    {
      "epoch": 1.3796296296296298,
      "grad_norm": 0.34481504559516907,
      "learning_rate": 3.193430656934307e-05,
      "loss": 0.4456,
      "step": 1490
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 20.06075668334961,
      "learning_rate": 3.175182481751825e-05,
      "loss": 0.335,
      "step": 1500
    },
    {
      "epoch": 1.3981481481481481,
      "grad_norm": 3.8078272342681885,
      "learning_rate": 3.156934306569343e-05,
      "loss": 0.3709,
      "step": 1510
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 0.914741575717926,
      "learning_rate": 3.138686131386862e-05,
      "loss": 0.3249,
      "step": 1520
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.8248832821846008,
      "learning_rate": 3.12043795620438e-05,
      "loss": 0.2307,
      "step": 1530
    },
    {
      "epoch": 1.425925925925926,
      "grad_norm": 4.541648864746094,
      "learning_rate": 3.102189781021898e-05,
      "loss": 0.0799,
      "step": 1540
    },
    {
      "epoch": 1.4351851851851851,
      "grad_norm": 0.19060221314430237,
      "learning_rate": 3.083941605839416e-05,
      "loss": 0.098,
      "step": 1550
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.1483784019947052,
      "learning_rate": 3.0656934306569346e-05,
      "loss": 0.2541,
      "step": 1560
    },
    {
      "epoch": 1.4537037037037037,
      "grad_norm": 0.25329700112342834,
      "learning_rate": 3.0474452554744527e-05,
      "loss": 0.2929,
      "step": 1570
    },
    {
      "epoch": 1.462962962962963,
      "grad_norm": 0.2854469418525696,
      "learning_rate": 3.029197080291971e-05,
      "loss": 0.215,
      "step": 1580
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.4885724186897278,
      "learning_rate": 3.010948905109489e-05,
      "loss": 0.3316,
      "step": 1590
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 1.107353925704956,
      "learning_rate": 2.992700729927008e-05,
      "loss": 0.1669,
      "step": 1600
    },
    {
      "epoch": 1.4907407407407407,
      "grad_norm": 0.7464072108268738,
      "learning_rate": 2.9744525547445256e-05,
      "loss": 0.2457,
      "step": 1610
    },
    {
      "epoch": 1.5,
      "grad_norm": 8.983071327209473,
      "learning_rate": 2.9562043795620443e-05,
      "loss": 0.3256,
      "step": 1620
    },
    {
      "epoch": 1.5092592592592593,
      "grad_norm": 17.070825576782227,
      "learning_rate": 2.9379562043795624e-05,
      "loss": 0.2942,
      "step": 1630
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 8.516557693481445,
      "learning_rate": 2.91970802919708e-05,
      "loss": 0.1436,
      "step": 1640
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 22.638832092285156,
      "learning_rate": 2.9014598540145988e-05,
      "loss": 0.409,
      "step": 1650
    },
    {
      "epoch": 1.5370370370370372,
      "grad_norm": 9.128459930419922,
      "learning_rate": 2.883211678832117e-05,
      "loss": 0.2724,
      "step": 1660
    },
    {
      "epoch": 1.5462962962962963,
      "grad_norm": 0.7110962271690369,
      "learning_rate": 2.8649635036496353e-05,
      "loss": 0.3317,
      "step": 1670
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 2.2748355865478516,
      "learning_rate": 2.8467153284671533e-05,
      "loss": 0.1346,
      "step": 1680
    },
    {
      "epoch": 1.5648148148148149,
      "grad_norm": 4.999617576599121,
      "learning_rate": 2.8284671532846717e-05,
      "loss": 0.1863,
      "step": 1690
    },
    {
      "epoch": 1.574074074074074,
      "grad_norm": 12.128273963928223,
      "learning_rate": 2.8102189781021898e-05,
      "loss": 0.1161,
      "step": 1700
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 6.144622325897217,
      "learning_rate": 2.7919708029197085e-05,
      "loss": 0.1683,
      "step": 1710
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 2.374804973602295,
      "learning_rate": 2.7737226277372262e-05,
      "loss": 0.1749,
      "step": 1720
    },
    {
      "epoch": 1.6018518518518519,
      "grad_norm": 3.0614876747131348,
      "learning_rate": 2.755474452554745e-05,
      "loss": 0.1792,
      "step": 1730
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 29.24116325378418,
      "learning_rate": 2.737226277372263e-05,
      "loss": 0.4572,
      "step": 1740
    },
    {
      "epoch": 1.6203703703703702,
      "grad_norm": 0.34388449788093567,
      "learning_rate": 2.7189781021897807e-05,
      "loss": 0.0852,
      "step": 1750
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 1.5720161199569702,
      "learning_rate": 2.7007299270072995e-05,
      "loss": 0.2039,
      "step": 1760
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 1.541945457458496,
      "learning_rate": 2.6824817518248175e-05,
      "loss": 0.1614,
      "step": 1770
    },
    {
      "epoch": 1.6481481481481481,
      "grad_norm": 0.6747615933418274,
      "learning_rate": 2.664233576642336e-05,
      "loss": 0.0271,
      "step": 1780
    },
    {
      "epoch": 1.6574074074074074,
      "grad_norm": 21.335628509521484,
      "learning_rate": 2.645985401459854e-05,
      "loss": 0.1801,
      "step": 1790
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.11601562052965164,
      "learning_rate": 2.6277372262773724e-05,
      "loss": 0.1954,
      "step": 1800
    },
    {
      "epoch": 1.675925925925926,
      "grad_norm": 0.18991528451442719,
      "learning_rate": 2.6094890510948904e-05,
      "loss": 0.3,
      "step": 1810
    },
    {
      "epoch": 1.6851851851851851,
      "grad_norm": 0.1529638171195984,
      "learning_rate": 2.591240875912409e-05,
      "loss": 0.0742,
      "step": 1820
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 2.776864767074585,
      "learning_rate": 2.572992700729927e-05,
      "loss": 0.0717,
      "step": 1830
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 0.14020057022571564,
      "learning_rate": 2.5547445255474456e-05,
      "loss": 0.3575,
      "step": 1840
    },
    {
      "epoch": 1.7129629629629628,
      "grad_norm": 0.16283315420150757,
      "learning_rate": 2.5364963503649637e-05,
      "loss": 0.0764,
      "step": 1850
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.1283709704875946,
      "learning_rate": 2.518248175182482e-05,
      "loss": 0.0841,
      "step": 1860
    },
    {
      "epoch": 1.7314814814814814,
      "grad_norm": 0.11467273533344269,
      "learning_rate": 2.5e-05,
      "loss": 0.2123,
      "step": 1870
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 4.040525436401367,
      "learning_rate": 2.4817518248175185e-05,
      "loss": 0.3342,
      "step": 1880
    },
    {
      "epoch": 1.75,
      "grad_norm": 4.591585636138916,
      "learning_rate": 2.4635036496350366e-05,
      "loss": 0.2401,
      "step": 1890
    },
    {
      "epoch": 1.7592592592592593,
      "grad_norm": 5.797786712646484,
      "learning_rate": 2.445255474452555e-05,
      "loss": 0.2139,
      "step": 1900
    },
    {
      "epoch": 1.7685185185185186,
      "grad_norm": 4.243651866912842,
      "learning_rate": 2.4270072992700734e-05,
      "loss": 0.3124,
      "step": 1910
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.3549323081970215,
      "learning_rate": 2.4087591240875914e-05,
      "loss": 0.1942,
      "step": 1920
    },
    {
      "epoch": 1.7870370370370372,
      "grad_norm": 25.92784309387207,
      "learning_rate": 2.3905109489051098e-05,
      "loss": 0.3216,
      "step": 1930
    },
    {
      "epoch": 1.7962962962962963,
      "grad_norm": 1.55165696144104,
      "learning_rate": 2.372262773722628e-05,
      "loss": 0.1865,
      "step": 1940
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 3.722073554992676,
      "learning_rate": 2.354014598540146e-05,
      "loss": 0.2988,
      "step": 1950
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 29.899763107299805,
      "learning_rate": 2.3357664233576643e-05,
      "loss": 0.1923,
      "step": 1960
    },
    {
      "epoch": 1.824074074074074,
      "grad_norm": 14.896507263183594,
      "learning_rate": 2.3175182481751824e-05,
      "loss": 0.1925,
      "step": 1970
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.21017082035541534,
      "learning_rate": 2.2992700729927008e-05,
      "loss": 0.1539,
      "step": 1980
    },
    {
      "epoch": 1.8425925925925926,
      "grad_norm": 2.2367942333221436,
      "learning_rate": 2.2810218978102192e-05,
      "loss": 0.1087,
      "step": 1990
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 7.382864475250244,
      "learning_rate": 2.2627737226277372e-05,
      "loss": 0.326,
      "step": 2000
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 0.3131502568721771,
      "learning_rate": 2.2445255474452556e-05,
      "loss": 0.4711,
      "step": 2010
    },
    {
      "epoch": 1.8703703703703702,
      "grad_norm": 0.2502964735031128,
      "learning_rate": 2.226277372262774e-05,
      "loss": 0.0845,
      "step": 2020
    },
    {
      "epoch": 1.8796296296296298,
      "grad_norm": 5.2541961669921875,
      "learning_rate": 2.208029197080292e-05,
      "loss": 0.1631,
      "step": 2030
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.5123298764228821,
      "learning_rate": 2.1897810218978105e-05,
      "loss": 0.2944,
      "step": 2040
    },
    {
      "epoch": 1.8981481481481481,
      "grad_norm": 6.264063835144043,
      "learning_rate": 2.1715328467153285e-05,
      "loss": 0.2515,
      "step": 2050
    },
    {
      "epoch": 1.9074074074074074,
      "grad_norm": 49.08097457885742,
      "learning_rate": 2.1532846715328466e-05,
      "loss": 0.1502,
      "step": 2060
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.28697410225868225,
      "learning_rate": 2.135036496350365e-05,
      "loss": 0.1541,
      "step": 2070
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 24.845882415771484,
      "learning_rate": 2.1167883211678834e-05,
      "loss": 0.1398,
      "step": 2080
    },
    {
      "epoch": 1.9351851851851851,
      "grad_norm": 13.570403099060059,
      "learning_rate": 2.0985401459854014e-05,
      "loss": 0.1408,
      "step": 2090
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.6536846160888672,
      "learning_rate": 2.08029197080292e-05,
      "loss": 0.2202,
      "step": 2100
    },
    {
      "epoch": 1.9537037037037037,
      "grad_norm": 5.0920915603637695,
      "learning_rate": 2.062043795620438e-05,
      "loss": 0.2606,
      "step": 2110
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 2.5583908557891846,
      "learning_rate": 2.0437956204379563e-05,
      "loss": 0.2779,
      "step": 2120
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 0.15033558011054993,
      "learning_rate": 2.0255474452554747e-05,
      "loss": 0.1099,
      "step": 2130
    },
    {
      "epoch": 1.9814814814814814,
      "grad_norm": 0.08266875892877579,
      "learning_rate": 2.0072992700729927e-05,
      "loss": 0.3431,
      "step": 2140
    },
    {
      "epoch": 1.9907407407407407,
      "grad_norm": 1.0091238021850586,
      "learning_rate": 1.989051094890511e-05,
      "loss": 0.2793,
      "step": 2150
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2615530788898468,
      "learning_rate": 1.9708029197080295e-05,
      "loss": 0.202,
      "step": 2160
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.19398297369480133,
      "eval_runtime": 6.839,
      "eval_samples_per_second": 140.372,
      "eval_steps_per_second": 8.773,
      "step": 2160
    },
    {
      "epoch": 2.009259259259259,
      "grad_norm": 4.240015983581543,
      "learning_rate": 1.9525547445255476e-05,
      "loss": 0.2459,
      "step": 2170
    },
    {
      "epoch": 2.0185185185185186,
      "grad_norm": 0.8892775177955627,
      "learning_rate": 1.934306569343066e-05,
      "loss": 0.1069,
      "step": 2180
    },
    {
      "epoch": 2.0277777777777777,
      "grad_norm": 17.839080810546875,
      "learning_rate": 1.916058394160584e-05,
      "loss": 0.0925,
      "step": 2190
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 4.107739448547363,
      "learning_rate": 1.897810218978102e-05,
      "loss": 0.2696,
      "step": 2200
    },
    {
      "epoch": 2.0462962962962963,
      "grad_norm": 0.11767221987247467,
      "learning_rate": 1.8795620437956205e-05,
      "loss": 0.0876,
      "step": 2210
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 0.12730038166046143,
      "learning_rate": 1.861313868613139e-05,
      "loss": 0.1582,
      "step": 2220
    },
    {
      "epoch": 2.064814814814815,
      "grad_norm": 0.17337104678153992,
      "learning_rate": 1.843065693430657e-05,
      "loss": 0.2463,
      "step": 2230
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 18.36329460144043,
      "learning_rate": 1.8248175182481753e-05,
      "loss": 0.4185,
      "step": 2240
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.5044969320297241,
      "learning_rate": 1.8065693430656934e-05,
      "loss": 0.1457,
      "step": 2250
    },
    {
      "epoch": 2.0925925925925926,
      "grad_norm": 13.390515327453613,
      "learning_rate": 1.7883211678832118e-05,
      "loss": 0.236,
      "step": 2260
    },
    {
      "epoch": 2.1018518518518516,
      "grad_norm": 0.5791191458702087,
      "learning_rate": 1.7700729927007302e-05,
      "loss": 0.263,
      "step": 2270
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 48.14034652709961,
      "learning_rate": 1.7518248175182482e-05,
      "loss": 0.306,
      "step": 2280
    },
    {
      "epoch": 2.1203703703703702,
      "grad_norm": 0.4147697985172272,
      "learning_rate": 1.7335766423357666e-05,
      "loss": 0.2995,
      "step": 2290
    },
    {
      "epoch": 2.1296296296296298,
      "grad_norm": 0.226301372051239,
      "learning_rate": 1.715328467153285e-05,
      "loss": 0.1643,
      "step": 2300
    },
    {
      "epoch": 2.138888888888889,
      "grad_norm": 1.0502383708953857,
      "learning_rate": 1.6970802919708028e-05,
      "loss": 0.1249,
      "step": 2310
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 10.04446792602539,
      "learning_rate": 1.678832116788321e-05,
      "loss": 0.125,
      "step": 2320
    },
    {
      "epoch": 2.1574074074074074,
      "grad_norm": 6.581087589263916,
      "learning_rate": 1.6605839416058395e-05,
      "loss": 0.282,
      "step": 2330
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.6406977772712708,
      "learning_rate": 1.6423357664233576e-05,
      "loss": 0.0086,
      "step": 2340
    },
    {
      "epoch": 2.175925925925926,
      "grad_norm": 0.09758570790290833,
      "learning_rate": 1.624087591240876e-05,
      "loss": 0.0759,
      "step": 2350
    },
    {
      "epoch": 2.185185185185185,
      "grad_norm": 0.20422443747520447,
      "learning_rate": 1.605839416058394e-05,
      "loss": 0.187,
      "step": 2360
    },
    {
      "epoch": 2.1944444444444446,
      "grad_norm": 0.14086861908435822,
      "learning_rate": 1.5875912408759125e-05,
      "loss": 0.148,
      "step": 2370
    },
    {
      "epoch": 2.2037037037037037,
      "grad_norm": 0.1213226392865181,
      "learning_rate": 1.569343065693431e-05,
      "loss": 0.153,
      "step": 2380
    },
    {
      "epoch": 2.212962962962963,
      "grad_norm": 2.6055896282196045,
      "learning_rate": 1.551094890510949e-05,
      "loss": 0.1588,
      "step": 2390
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 18.1203670501709,
      "learning_rate": 1.5328467153284673e-05,
      "loss": 0.179,
      "step": 2400
    },
    {
      "epoch": 2.2314814814814814,
      "grad_norm": 4.0438923835754395,
      "learning_rate": 1.5145985401459855e-05,
      "loss": 0.3036,
      "step": 2410
    },
    {
      "epoch": 2.240740740740741,
      "grad_norm": 0.14823326468467712,
      "learning_rate": 1.496350364963504e-05,
      "loss": 0.107,
      "step": 2420
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.2694200873374939,
      "learning_rate": 1.4781021897810221e-05,
      "loss": 0.079,
      "step": 2430
    },
    {
      "epoch": 2.259259259259259,
      "grad_norm": 0.5480892658233643,
      "learning_rate": 1.45985401459854e-05,
      "loss": 0.1142,
      "step": 2440
    },
    {
      "epoch": 2.2685185185185186,
      "grad_norm": 7.612310886383057,
      "learning_rate": 1.4416058394160584e-05,
      "loss": 0.3024,
      "step": 2450
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 12.109585762023926,
      "learning_rate": 1.4233576642335767e-05,
      "loss": 0.3197,
      "step": 2460
    },
    {
      "epoch": 2.287037037037037,
      "grad_norm": 0.45524102449417114,
      "learning_rate": 1.4051094890510949e-05,
      "loss": 0.2161,
      "step": 2470
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 0.11474527418613434,
      "learning_rate": 1.3868613138686131e-05,
      "loss": 0.215,
      "step": 2480
    },
    {
      "epoch": 2.3055555555555554,
      "grad_norm": 0.7469318509101868,
      "learning_rate": 1.3686131386861315e-05,
      "loss": 0.2239,
      "step": 2490
    },
    {
      "epoch": 2.314814814814815,
      "grad_norm": 0.4865613281726837,
      "learning_rate": 1.3503649635036497e-05,
      "loss": 0.1584,
      "step": 2500
    },
    {
      "epoch": 2.324074074074074,
      "grad_norm": 0.4829758107662201,
      "learning_rate": 1.332116788321168e-05,
      "loss": 0.1124,
      "step": 2510
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.2056412249803543,
      "learning_rate": 1.3138686131386862e-05,
      "loss": 0.3387,
      "step": 2520
    },
    {
      "epoch": 2.3425925925925926,
      "grad_norm": 96.59685516357422,
      "learning_rate": 1.2956204379562046e-05,
      "loss": 0.146,
      "step": 2530
    },
    {
      "epoch": 2.351851851851852,
      "grad_norm": 15.029600143432617,
      "learning_rate": 1.2773722627737228e-05,
      "loss": 0.1342,
      "step": 2540
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 3.972198963165283,
      "learning_rate": 1.259124087591241e-05,
      "loss": 0.1408,
      "step": 2550
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 16.707111358642578,
      "learning_rate": 1.2408759124087593e-05,
      "loss": 0.0899,
      "step": 2560
    },
    {
      "epoch": 2.3796296296296298,
      "grad_norm": 0.1578131467103958,
      "learning_rate": 1.2226277372262775e-05,
      "loss": 0.2043,
      "step": 2570
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 0.08522962778806686,
      "learning_rate": 1.2043795620437957e-05,
      "loss": 0.1586,
      "step": 2580
    },
    {
      "epoch": 2.398148148148148,
      "grad_norm": 0.12188362330198288,
      "learning_rate": 1.186131386861314e-05,
      "loss": 0.0787,
      "step": 2590
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 6.093868732452393,
      "learning_rate": 1.1678832116788322e-05,
      "loss": 0.101,
      "step": 2600
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.20033028721809387,
      "learning_rate": 1.1496350364963504e-05,
      "loss": 0.2694,
      "step": 2610
    },
    {
      "epoch": 2.425925925925926,
      "grad_norm": 0.2674160897731781,
      "learning_rate": 1.1313868613138686e-05,
      "loss": 0.2955,
      "step": 2620
    },
    {
      "epoch": 2.435185185185185,
      "grad_norm": 33.90544891357422,
      "learning_rate": 1.113138686131387e-05,
      "loss": 0.3028,
      "step": 2630
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.18016457557678223,
      "learning_rate": 1.0948905109489052e-05,
      "loss": 0.1907,
      "step": 2640
    },
    {
      "epoch": 2.4537037037037037,
      "grad_norm": 0.24831433594226837,
      "learning_rate": 1.0766423357664233e-05,
      "loss": 0.2333,
      "step": 2650
    },
    {
      "epoch": 2.462962962962963,
      "grad_norm": 1.6836082935333252,
      "learning_rate": 1.0583941605839417e-05,
      "loss": 0.0757,
      "step": 2660
    },
    {
      "epoch": 2.4722222222222223,
      "grad_norm": 3.8062033653259277,
      "learning_rate": 1.04014598540146e-05,
      "loss": 0.1656,
      "step": 2670
    },
    {
      "epoch": 2.4814814814814814,
      "grad_norm": 0.21045559644699097,
      "learning_rate": 1.0218978102189781e-05,
      "loss": 0.3144,
      "step": 2680
    },
    {
      "epoch": 2.490740740740741,
      "grad_norm": 4.108794212341309,
      "learning_rate": 1.0036496350364964e-05,
      "loss": 0.0768,
      "step": 2690
    },
    {
      "epoch": 2.5,
      "grad_norm": 34.025787353515625,
      "learning_rate": 9.854014598540148e-06,
      "loss": 0.3227,
      "step": 2700
    },
    {
      "epoch": 2.5092592592592595,
      "grad_norm": 0.4783933162689209,
      "learning_rate": 9.67153284671533e-06,
      "loss": 0.2436,
      "step": 2710
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 2.1021382808685303,
      "learning_rate": 9.48905109489051e-06,
      "loss": 0.2321,
      "step": 2720
    },
    {
      "epoch": 2.5277777777777777,
      "grad_norm": 24.647502899169922,
      "learning_rate": 9.306569343065694e-06,
      "loss": 0.1753,
      "step": 2730
    },
    {
      "epoch": 2.537037037037037,
      "grad_norm": 22.161121368408203,
      "learning_rate": 9.124087591240877e-06,
      "loss": 0.2536,
      "step": 2740
    },
    {
      "epoch": 2.5462962962962963,
      "grad_norm": 0.18267226219177246,
      "learning_rate": 8.941605839416059e-06,
      "loss": 0.0857,
      "step": 2750
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 8.810466766357422,
      "learning_rate": 8.759124087591241e-06,
      "loss": 0.243,
      "step": 2760
    },
    {
      "epoch": 2.564814814814815,
      "grad_norm": 3.9875662326812744,
      "learning_rate": 8.576642335766425e-06,
      "loss": 0.3726,
      "step": 2770
    },
    {
      "epoch": 2.574074074074074,
      "grad_norm": 8.026987075805664,
      "learning_rate": 8.394160583941606e-06,
      "loss": 0.1297,
      "step": 2780
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 4.002756595611572,
      "learning_rate": 8.211678832116788e-06,
      "loss": 0.2845,
      "step": 2790
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 38.08171081542969,
      "learning_rate": 8.02919708029197e-06,
      "loss": 0.4791,
      "step": 2800
    },
    {
      "epoch": 2.601851851851852,
      "grad_norm": 0.26937660574913025,
      "learning_rate": 7.846715328467154e-06,
      "loss": 0.1247,
      "step": 2810
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 4.020223617553711,
      "learning_rate": 7.664233576642336e-06,
      "loss": 0.1869,
      "step": 2820
    },
    {
      "epoch": 2.6203703703703702,
      "grad_norm": 65.686767578125,
      "learning_rate": 7.48175182481752e-06,
      "loss": 0.0446,
      "step": 2830
    },
    {
      "epoch": 2.6296296296296298,
      "grad_norm": 0.29415470361709595,
      "learning_rate": 7.2992700729927e-06,
      "loss": 0.1274,
      "step": 2840
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 7.765114784240723,
      "learning_rate": 7.116788321167883e-06,
      "loss": 0.2385,
      "step": 2850
    },
    {
      "epoch": 2.648148148148148,
      "grad_norm": 0.1306288242340088,
      "learning_rate": 6.9343065693430655e-06,
      "loss": 0.0688,
      "step": 2860
    },
    {
      "epoch": 2.6574074074074074,
      "grad_norm": 0.6288871169090271,
      "learning_rate": 6.751824817518249e-06,
      "loss": 0.2519,
      "step": 2870
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 3.9508187770843506,
      "learning_rate": 6.569343065693431e-06,
      "loss": 0.2439,
      "step": 2880
    },
    {
      "epoch": 2.675925925925926,
      "grad_norm": 0.16552723944187164,
      "learning_rate": 6.386861313868614e-06,
      "loss": 0.1114,
      "step": 2890
    },
    {
      "epoch": 2.685185185185185,
      "grad_norm": 0.1259353905916214,
      "learning_rate": 6.204379562043796e-06,
      "loss": 0.2009,
      "step": 2900
    },
    {
      "epoch": 2.6944444444444446,
      "grad_norm": 0.244041308760643,
      "learning_rate": 6.0218978102189786e-06,
      "loss": 0.1917,
      "step": 2910
    },
    {
      "epoch": 2.7037037037037037,
      "grad_norm": 3.7213332653045654,
      "learning_rate": 5.839416058394161e-06,
      "loss": 0.2633,
      "step": 2920
    },
    {
      "epoch": 2.712962962962963,
      "grad_norm": 0.404834121465683,
      "learning_rate": 5.656934306569343e-06,
      "loss": 0.2138,
      "step": 2930
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 0.3822720944881439,
      "learning_rate": 5.474452554744526e-06,
      "loss": 0.1047,
      "step": 2940
    },
    {
      "epoch": 2.7314814814814814,
      "grad_norm": 0.604572594165802,
      "learning_rate": 5.2919708029197084e-06,
      "loss": 0.1623,
      "step": 2950
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 0.1382262259721756,
      "learning_rate": 5.109489051094891e-06,
      "loss": 0.1073,
      "step": 2960
    },
    {
      "epoch": 2.75,
      "grad_norm": 3.3636763095855713,
      "learning_rate": 4.927007299270074e-06,
      "loss": 0.3864,
      "step": 2970
    },
    {
      "epoch": 2.7592592592592595,
      "grad_norm": 0.14923423528671265,
      "learning_rate": 4.744525547445255e-06,
      "loss": 0.0698,
      "step": 2980
    },
    {
      "epoch": 2.7685185185185186,
      "grad_norm": 22.737110137939453,
      "learning_rate": 4.562043795620438e-06,
      "loss": 0.2616,
      "step": 2990
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.18129505217075348,
      "learning_rate": 4.379562043795621e-06,
      "loss": 0.3066,
      "step": 3000
    },
    {
      "epoch": 2.787037037037037,
      "grad_norm": 25.055286407470703,
      "learning_rate": 4.197080291970803e-06,
      "loss": 0.2876,
      "step": 3010
    },
    {
      "epoch": 2.7962962962962963,
      "grad_norm": 0.2261570245027542,
      "learning_rate": 4.014598540145985e-06,
      "loss": 0.1641,
      "step": 3020
    },
    {
      "epoch": 2.8055555555555554,
      "grad_norm": 38.17695236206055,
      "learning_rate": 3.832116788321168e-06,
      "loss": 0.1961,
      "step": 3030
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 0.6032215356826782,
      "learning_rate": 3.64963503649635e-06,
      "loss": 0.1574,
      "step": 3040
    },
    {
      "epoch": 2.824074074074074,
      "grad_norm": 0.21763397753238678,
      "learning_rate": 3.4671532846715328e-06,
      "loss": 0.0978,
      "step": 3050
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.1729101538658142,
      "learning_rate": 3.2846715328467155e-06,
      "loss": 0.1698,
      "step": 3060
    },
    {
      "epoch": 2.8425925925925926,
      "grad_norm": 0.22007280588150024,
      "learning_rate": 3.102189781021898e-06,
      "loss": 0.1452,
      "step": 3070
    },
    {
      "epoch": 2.851851851851852,
      "grad_norm": 0.313767671585083,
      "learning_rate": 2.9197080291970804e-06,
      "loss": 0.0549,
      "step": 3080
    },
    {
      "epoch": 2.861111111111111,
      "grad_norm": 0.42687293887138367,
      "learning_rate": 2.737226277372263e-06,
      "loss": 0.2112,
      "step": 3090
    },
    {
      "epoch": 2.8703703703703702,
      "grad_norm": 0.10430824756622314,
      "learning_rate": 2.5547445255474454e-06,
      "loss": 0.0869,
      "step": 3100
    },
    {
      "epoch": 2.8796296296296298,
      "grad_norm": 0.11928918957710266,
      "learning_rate": 2.3722627737226276e-06,
      "loss": 0.0762,
      "step": 3110
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.13053879141807556,
      "learning_rate": 2.1897810218978103e-06,
      "loss": 0.1422,
      "step": 3120
    },
    {
      "epoch": 2.898148148148148,
      "grad_norm": 0.12232078611850739,
      "learning_rate": 2.0072992700729926e-06,
      "loss": 0.1148,
      "step": 3130
    },
    {
      "epoch": 2.9074074074074074,
      "grad_norm": 0.1609068512916565,
      "learning_rate": 1.824817518248175e-06,
      "loss": 0.2246,
      "step": 3140
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.15394479036331177,
      "learning_rate": 1.6423357664233577e-06,
      "loss": 0.146,
      "step": 3150
    },
    {
      "epoch": 2.925925925925926,
      "grad_norm": 3.9535927772521973,
      "learning_rate": 1.4598540145985402e-06,
      "loss": 0.1973,
      "step": 3160
    },
    {
      "epoch": 2.935185185185185,
      "grad_norm": 66.45625305175781,
      "learning_rate": 1.2773722627737227e-06,
      "loss": 0.2102,
      "step": 3170
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 13.113862037658691,
      "learning_rate": 1.0948905109489052e-06,
      "loss": 0.1752,
      "step": 3180
    },
    {
      "epoch": 2.9537037037037037,
      "grad_norm": 0.1340145617723465,
      "learning_rate": 9.124087591240875e-07,
      "loss": 0.175,
      "step": 3190
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 0.15904024243354797,
      "learning_rate": 7.299270072992701e-07,
      "loss": 0.1577,
      "step": 3200
    },
    {
      "epoch": 2.9722222222222223,
      "grad_norm": 0.2619703710079193,
      "learning_rate": 5.474452554744526e-07,
      "loss": 0.0971,
      "step": 3210
    },
    {
      "epoch": 2.9814814814814814,
      "grad_norm": 0.5985182523727417,
      "learning_rate": 3.6496350364963505e-07,
      "loss": 0.2786,
      "step": 3220
    },
    {
      "epoch": 2.9907407407407405,
      "grad_norm": 0.2590939402580261,
      "learning_rate": 1.8248175182481753e-07,
      "loss": 0.0679,
      "step": 3230
    },
    {
      "epoch": 3.0,
      "grad_norm": 4.633773326873779,
      "learning_rate": 0.0,
      "loss": 0.2867,
      "step": 3240
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.2287292629480362,
      "eval_runtime": 6.8262,
      "eval_samples_per_second": 140.634,
      "eval_steps_per_second": 8.79,
      "step": 3240
    }
  ],
  "logging_steps": 10,
  "max_steps": 3240,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1704762305441280.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
