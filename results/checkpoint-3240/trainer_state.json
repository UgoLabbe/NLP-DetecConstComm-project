{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3240,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009259259259259259,
      "grad_norm": 4.550492286682129,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.6916,
      "step": 10
    },
    {
      "epoch": 0.018518518518518517,
      "grad_norm": 6.232330799102783,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6656,
      "step": 20
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 4.445793151855469,
      "learning_rate": 3e-06,
      "loss": 0.6812,
      "step": 30
    },
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 5.60944128036499,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.6474,
      "step": 40
    },
    {
      "epoch": 0.046296296296296294,
      "grad_norm": 3.5997769832611084,
      "learning_rate": 5e-06,
      "loss": 0.6418,
      "step": 50
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 7.984439373016357,
      "learning_rate": 6e-06,
      "loss": 0.5944,
      "step": 60
    },
    {
      "epoch": 0.06481481481481481,
      "grad_norm": 6.693225383758545,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.5496,
      "step": 70
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 8.582216262817383,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.4586,
      "step": 80
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 5.682147026062012,
      "learning_rate": 9e-06,
      "loss": 0.4478,
      "step": 90
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 7.547580718994141,
      "learning_rate": 1e-05,
      "loss": 0.441,
      "step": 100
    },
    {
      "epoch": 0.10185185185185185,
      "grad_norm": 3.3105263710021973,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.4048,
      "step": 110
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 6.392953395843506,
      "learning_rate": 1.2e-05,
      "loss": 0.3265,
      "step": 120
    },
    {
      "epoch": 0.12037037037037036,
      "grad_norm": 3.6467247009277344,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.3488,
      "step": 130
    },
    {
      "epoch": 0.12962962962962962,
      "grad_norm": 4.149466514587402,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.2552,
      "step": 140
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 22.582124710083008,
      "learning_rate": 1.5e-05,
      "loss": 0.2068,
      "step": 150
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 32.74368667602539,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.2276,
      "step": 160
    },
    {
      "epoch": 0.1574074074074074,
      "grad_norm": 16.098928451538086,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.1763,
      "step": 170
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 4.606746196746826,
      "learning_rate": 1.8e-05,
      "loss": 0.2836,
      "step": 180
    },
    {
      "epoch": 0.17592592592592593,
      "grad_norm": 10.924084663391113,
      "learning_rate": 1.9e-05,
      "loss": 0.2803,
      "step": 190
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 1.0930407047271729,
      "learning_rate": 2e-05,
      "loss": 0.2244,
      "step": 200
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 51.22146224975586,
      "learning_rate": 2.1e-05,
      "loss": 0.1463,
      "step": 210
    },
    {
      "epoch": 0.2037037037037037,
      "grad_norm": 3.85090970993042,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.2529,
      "step": 220
    },
    {
      "epoch": 0.21296296296296297,
      "grad_norm": 2.4174838066101074,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0817,
      "step": 230
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.6196881532669067,
      "learning_rate": 2.4e-05,
      "loss": 0.435,
      "step": 240
    },
    {
      "epoch": 0.23148148148148148,
      "grad_norm": 0.7118971943855286,
      "learning_rate": 2.5e-05,
      "loss": 0.1463,
      "step": 250
    },
    {
      "epoch": 0.24074074074074073,
      "grad_norm": 0.3623069226741791,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.2622,
      "step": 260
    },
    {
      "epoch": 0.25,
      "grad_norm": 20.690284729003906,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.482,
      "step": 270
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 20.324012756347656,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.426,
      "step": 280
    },
    {
      "epoch": 0.26851851851851855,
      "grad_norm": 24.113147735595703,
      "learning_rate": 2.9e-05,
      "loss": 0.2448,
      "step": 290
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 6.845015048980713,
      "learning_rate": 3e-05,
      "loss": 0.3658,
      "step": 300
    },
    {
      "epoch": 0.28703703703703703,
      "grad_norm": 2.4010114669799805,
      "learning_rate": 3.1e-05,
      "loss": 0.2103,
      "step": 310
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 24.51532745361328,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.3415,
      "step": 320
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 18.402429580688477,
      "learning_rate": 3.3e-05,
      "loss": 0.3464,
      "step": 330
    },
    {
      "epoch": 0.3148148148148148,
      "grad_norm": 1.2668803930282593,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.3851,
      "step": 340
    },
    {
      "epoch": 0.32407407407407407,
      "grad_norm": 15.53378677368164,
      "learning_rate": 3.5e-05,
      "loss": 0.1622,
      "step": 350
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.29672160744667053,
      "learning_rate": 3.6e-05,
      "loss": 0.2889,
      "step": 360
    },
    {
      "epoch": 0.3425925925925926,
      "grad_norm": 20.29379653930664,
      "learning_rate": 3.7e-05,
      "loss": 0.4608,
      "step": 370
    },
    {
      "epoch": 0.35185185185185186,
      "grad_norm": 7.3528151512146,
      "learning_rate": 3.8e-05,
      "loss": 0.3412,
      "step": 380
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 9.858256340026855,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.5653,
      "step": 390
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 11.61242961883545,
      "learning_rate": 4e-05,
      "loss": 0.3345,
      "step": 400
    },
    {
      "epoch": 0.37962962962962965,
      "grad_norm": 4.379803657531738,
      "learning_rate": 4.1e-05,
      "loss": 0.1512,
      "step": 410
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.32314178347587585,
      "learning_rate": 4.2e-05,
      "loss": 0.1792,
      "step": 420
    },
    {
      "epoch": 0.39814814814814814,
      "grad_norm": 29.356653213500977,
      "learning_rate": 4.3e-05,
      "loss": 0.4072,
      "step": 430
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 20.914627075195312,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.4486,
      "step": 440
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.5001393556594849,
      "learning_rate": 4.5e-05,
      "loss": 0.1879,
      "step": 450
    },
    {
      "epoch": 0.42592592592592593,
      "grad_norm": 16.836387634277344,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.3982,
      "step": 460
    },
    {
      "epoch": 0.4351851851851852,
      "grad_norm": 0.21681749820709229,
      "learning_rate": 4.7e-05,
      "loss": 0.1575,
      "step": 470
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.40036243200302124,
      "learning_rate": 4.8e-05,
      "loss": 0.2753,
      "step": 480
    },
    {
      "epoch": 0.4537037037037037,
      "grad_norm": 12.110563278198242,
      "learning_rate": 4.9e-05,
      "loss": 0.1984,
      "step": 490
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 7.3547468185424805,
      "learning_rate": 5e-05,
      "loss": 0.4596,
      "step": 500
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 3.484039783477783,
      "learning_rate": 4.981751824817518e-05,
      "loss": 0.3587,
      "step": 510
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 8.64892578125,
      "learning_rate": 4.963503649635037e-05,
      "loss": 0.2969,
      "step": 520
    },
    {
      "epoch": 0.49074074074074076,
      "grad_norm": 14.839223861694336,
      "learning_rate": 4.945255474452555e-05,
      "loss": 0.1554,
      "step": 530
    },
    {
      "epoch": 0.5,
      "grad_norm": 3.990370273590088,
      "learning_rate": 4.927007299270073e-05,
      "loss": 0.2158,
      "step": 540
    },
    {
      "epoch": 0.5092592592592593,
      "grad_norm": 0.3234359323978424,
      "learning_rate": 4.908759124087591e-05,
      "loss": 0.1685,
      "step": 550
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 0.23462995886802673,
      "learning_rate": 4.89051094890511e-05,
      "loss": 0.2804,
      "step": 560
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 4.2551422119140625,
      "learning_rate": 4.872262773722628e-05,
      "loss": 0.3071,
      "step": 570
    },
    {
      "epoch": 0.5370370370370371,
      "grad_norm": 3.469034433364868,
      "learning_rate": 4.854014598540147e-05,
      "loss": 0.3387,
      "step": 580
    },
    {
      "epoch": 0.5462962962962963,
      "grad_norm": 4.88865852355957,
      "learning_rate": 4.835766423357664e-05,
      "loss": 0.3919,
      "step": 590
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 4.449126720428467,
      "learning_rate": 4.817518248175183e-05,
      "loss": 0.2205,
      "step": 600
    },
    {
      "epoch": 0.5648148148148148,
      "grad_norm": 28.342832565307617,
      "learning_rate": 4.799270072992701e-05,
      "loss": 0.1963,
      "step": 610
    },
    {
      "epoch": 0.5740740740740741,
      "grad_norm": 0.346371591091156,
      "learning_rate": 4.7810218978102196e-05,
      "loss": 0.1804,
      "step": 620
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.3225829005241394,
      "learning_rate": 4.762773722627738e-05,
      "loss": 0.1624,
      "step": 630
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 13.402389526367188,
      "learning_rate": 4.744525547445256e-05,
      "loss": 0.4383,
      "step": 640
    },
    {
      "epoch": 0.6018518518518519,
      "grad_norm": 0.4054418206214905,
      "learning_rate": 4.726277372262774e-05,
      "loss": 0.2762,
      "step": 650
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.2290119081735611,
      "learning_rate": 4.708029197080292e-05,
      "loss": 0.1282,
      "step": 660
    },
    {
      "epoch": 0.6203703703703703,
      "grad_norm": 11.055265426635742,
      "learning_rate": 4.6897810218978106e-05,
      "loss": 0.1868,
      "step": 670
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 3.10210919380188,
      "learning_rate": 4.6715328467153287e-05,
      "loss": 0.2918,
      "step": 680
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 29.018381118774414,
      "learning_rate": 4.6532846715328474e-05,
      "loss": 0.6204,
      "step": 690
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 5.172561168670654,
      "learning_rate": 4.635036496350365e-05,
      "loss": 0.2357,
      "step": 700
    },
    {
      "epoch": 0.6574074074074074,
      "grad_norm": 2.1695382595062256,
      "learning_rate": 4.6167883211678835e-05,
      "loss": 0.4439,
      "step": 710
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 10.699932098388672,
      "learning_rate": 4.5985401459854016e-05,
      "loss": 0.2682,
      "step": 720
    },
    {
      "epoch": 0.6759259259259259,
      "grad_norm": 0.3140656352043152,
      "learning_rate": 4.58029197080292e-05,
      "loss": 0.1988,
      "step": 730
    },
    {
      "epoch": 0.6851851851851852,
      "grad_norm": 8.165070533752441,
      "learning_rate": 4.5620437956204383e-05,
      "loss": 0.2658,
      "step": 740
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.25319764018058777,
      "learning_rate": 4.5437956204379564e-05,
      "loss": 0.2199,
      "step": 750
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 0.4599943161010742,
      "learning_rate": 4.5255474452554745e-05,
      "loss": 0.192,
      "step": 760
    },
    {
      "epoch": 0.7129629629629629,
      "grad_norm": 8.760616302490234,
      "learning_rate": 4.5072992700729925e-05,
      "loss": 0.2613,
      "step": 770
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.4046090543270111,
      "learning_rate": 4.489051094890511e-05,
      "loss": 0.1515,
      "step": 780
    },
    {
      "epoch": 0.7314814814814815,
      "grad_norm": 6.534804344177246,
      "learning_rate": 4.470802919708029e-05,
      "loss": 0.2307,
      "step": 790
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 33.493465423583984,
      "learning_rate": 4.452554744525548e-05,
      "loss": 0.2796,
      "step": 800
    },
    {
      "epoch": 0.75,
      "grad_norm": 5.194834232330322,
      "learning_rate": 4.434306569343066e-05,
      "loss": 0.2841,
      "step": 810
    },
    {
      "epoch": 0.7592592592592593,
      "grad_norm": 0.43566134572029114,
      "learning_rate": 4.416058394160584e-05,
      "loss": 0.2849,
      "step": 820
    },
    {
      "epoch": 0.7685185185185185,
      "grad_norm": 0.9222869277000427,
      "learning_rate": 4.397810218978102e-05,
      "loss": 0.2522,
      "step": 830
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 8.53311824798584,
      "learning_rate": 4.379562043795621e-05,
      "loss": 0.3323,
      "step": 840
    },
    {
      "epoch": 0.7870370370370371,
      "grad_norm": 1.1674165725708008,
      "learning_rate": 4.361313868613139e-05,
      "loss": 0.1874,
      "step": 850
    },
    {
      "epoch": 0.7962962962962963,
      "grad_norm": 3.8547515869140625,
      "learning_rate": 4.343065693430657e-05,
      "loss": 0.1403,
      "step": 860
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 0.3197712302207947,
      "learning_rate": 4.324817518248175e-05,
      "loss": 0.1841,
      "step": 870
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 3.7556185722351074,
      "learning_rate": 4.306569343065693e-05,
      "loss": 0.2527,
      "step": 880
    },
    {
      "epoch": 0.8240740740740741,
      "grad_norm": 0.4065990746021271,
      "learning_rate": 4.288321167883212e-05,
      "loss": 0.202,
      "step": 890
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.5276533365249634,
      "learning_rate": 4.27007299270073e-05,
      "loss": 0.3149,
      "step": 900
    },
    {
      "epoch": 0.8425925925925926,
      "grad_norm": 0.5230464935302734,
      "learning_rate": 4.251824817518249e-05,
      "loss": 0.1537,
      "step": 910
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 0.975096583366394,
      "learning_rate": 4.233576642335767e-05,
      "loss": 0.4384,
      "step": 920
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 0.9170145988464355,
      "learning_rate": 4.215328467153285e-05,
      "loss": 0.1259,
      "step": 930
    },
    {
      "epoch": 0.8703703703703703,
      "grad_norm": 0.22495612502098083,
      "learning_rate": 4.197080291970803e-05,
      "loss": 0.2648,
      "step": 940
    },
    {
      "epoch": 0.8796296296296297,
      "grad_norm": 18.3745174407959,
      "learning_rate": 4.1788321167883216e-05,
      "loss": 0.2273,
      "step": 950
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.0534762144088745,
      "learning_rate": 4.16058394160584e-05,
      "loss": 0.2582,
      "step": 960
    },
    {
      "epoch": 0.8981481481481481,
      "grad_norm": 3.871079683303833,
      "learning_rate": 4.1423357664233584e-05,
      "loss": 0.2521,
      "step": 970
    },
    {
      "epoch": 0.9074074074074074,
      "grad_norm": 0.4285843074321747,
      "learning_rate": 4.124087591240876e-05,
      "loss": 0.1914,
      "step": 980
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.28197136521339417,
      "learning_rate": 4.1058394160583945e-05,
      "loss": 0.3096,
      "step": 990
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 1.2274856567382812,
      "learning_rate": 4.0875912408759126e-05,
      "loss": 0.1183,
      "step": 1000
    },
    {
      "epoch": 0.9351851851851852,
      "grad_norm": 32.35182189941406,
      "learning_rate": 4.0693430656934306e-05,
      "loss": 0.3863,
      "step": 1010
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 20.8210506439209,
      "learning_rate": 4.0510948905109494e-05,
      "loss": 0.3694,
      "step": 1020
    },
    {
      "epoch": 0.9537037037037037,
      "grad_norm": 0.30224674940109253,
      "learning_rate": 4.0328467153284674e-05,
      "loss": 0.2402,
      "step": 1030
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 30.352813720703125,
      "learning_rate": 4.0145985401459855e-05,
      "loss": 0.3605,
      "step": 1040
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 6.807135105133057,
      "learning_rate": 3.9963503649635035e-05,
      "loss": 0.1855,
      "step": 1050
    },
    {
      "epoch": 0.9814814814814815,
      "grad_norm": 12.593941688537598,
      "learning_rate": 3.978102189781022e-05,
      "loss": 0.2936,
      "step": 1060
    },
    {
      "epoch": 0.9907407407407407,
      "grad_norm": 0.15950970351696014,
      "learning_rate": 3.95985401459854e-05,
      "loss": 0.3429,
      "step": 1070
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8164092898368835,
      "learning_rate": 3.941605839416059e-05,
      "loss": 0.119,
      "step": 1080
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.20536135137081146,
      "eval_runtime": 6.8938,
      "eval_samples_per_second": 139.255,
      "eval_steps_per_second": 8.703,
      "step": 1080
    },
    {
      "epoch": 1.0092592592592593,
      "grad_norm": 0.3097158372402191,
      "learning_rate": 3.9233576642335764e-05,
      "loss": 0.1818,
      "step": 1090
    },
    {
      "epoch": 1.0185185185185186,
      "grad_norm": 50.9395866394043,
      "learning_rate": 3.905109489051095e-05,
      "loss": 0.3067,
      "step": 1100
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 3.985818386077881,
      "learning_rate": 3.886861313868613e-05,
      "loss": 0.3658,
      "step": 1110
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 0.5821428894996643,
      "learning_rate": 3.868613138686132e-05,
      "loss": 0.2305,
      "step": 1120
    },
    {
      "epoch": 1.0462962962962963,
      "grad_norm": 0.39441341161727905,
      "learning_rate": 3.85036496350365e-05,
      "loss": 0.1389,
      "step": 1130
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 18.54142951965332,
      "learning_rate": 3.832116788321168e-05,
      "loss": 0.3114,
      "step": 1140
    },
    {
      "epoch": 1.0648148148148149,
      "grad_norm": 1.2478320598602295,
      "learning_rate": 3.813868613138686e-05,
      "loss": 0.0899,
      "step": 1150
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 0.39251360297203064,
      "learning_rate": 3.795620437956204e-05,
      "loss": 0.1312,
      "step": 1160
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.3670021891593933,
      "learning_rate": 3.777372262773723e-05,
      "loss": 0.2359,
      "step": 1170
    },
    {
      "epoch": 1.0925925925925926,
      "grad_norm": 32.63560485839844,
      "learning_rate": 3.759124087591241e-05,
      "loss": 0.2043,
      "step": 1180
    },
    {
      "epoch": 1.1018518518518519,
      "grad_norm": 24.48093032836914,
      "learning_rate": 3.74087591240876e-05,
      "loss": 0.33,
      "step": 1190
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.21976271271705627,
      "learning_rate": 3.722627737226278e-05,
      "loss": 0.1404,
      "step": 1200
    },
    {
      "epoch": 1.1203703703703705,
      "grad_norm": 0.12458951026201248,
      "learning_rate": 3.704379562043796e-05,
      "loss": 0.2366,
      "step": 1210
    },
    {
      "epoch": 1.1296296296296295,
      "grad_norm": 3.090413808822632,
      "learning_rate": 3.686131386861314e-05,
      "loss": 0.0819,
      "step": 1220
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 34.408775329589844,
      "learning_rate": 3.6678832116788326e-05,
      "loss": 0.4742,
      "step": 1230
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 0.10971461981534958,
      "learning_rate": 3.649635036496351e-05,
      "loss": 0.2826,
      "step": 1240
    },
    {
      "epoch": 1.1574074074074074,
      "grad_norm": 0.27093034982681274,
      "learning_rate": 3.631386861313869e-05,
      "loss": 0.223,
      "step": 1250
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.8391903042793274,
      "learning_rate": 3.613138686131387e-05,
      "loss": 0.2742,
      "step": 1260
    },
    {
      "epoch": 1.175925925925926,
      "grad_norm": 8.285849571228027,
      "learning_rate": 3.594890510948905e-05,
      "loss": 0.1088,
      "step": 1270
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 10.635578155517578,
      "learning_rate": 3.5766423357664236e-05,
      "loss": 0.3244,
      "step": 1280
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 0.29189932346343994,
      "learning_rate": 3.5583941605839416e-05,
      "loss": 0.2264,
      "step": 1290
    },
    {
      "epoch": 1.2037037037037037,
      "grad_norm": 0.6712425351142883,
      "learning_rate": 3.5401459854014604e-05,
      "loss": 0.2409,
      "step": 1300
    },
    {
      "epoch": 1.212962962962963,
      "grad_norm": 0.37120822072029114,
      "learning_rate": 3.5218978102189784e-05,
      "loss": 0.1112,
      "step": 1310
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.0975128710269928,
      "learning_rate": 3.5036496350364965e-05,
      "loss": 0.0624,
      "step": 1320
    },
    {
      "epoch": 1.2314814814814814,
      "grad_norm": 0.4452650547027588,
      "learning_rate": 3.4854014598540145e-05,
      "loss": 0.1715,
      "step": 1330
    },
    {
      "epoch": 1.2407407407407407,
      "grad_norm": 0.6575754284858704,
      "learning_rate": 3.467153284671533e-05,
      "loss": 0.2457,
      "step": 1340
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.2158951759338379,
      "learning_rate": 3.448905109489051e-05,
      "loss": 0.2642,
      "step": 1350
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 0.23279589414596558,
      "learning_rate": 3.43065693430657e-05,
      "loss": 0.2027,
      "step": 1360
    },
    {
      "epoch": 1.2685185185185186,
      "grad_norm": 0.19058243930339813,
      "learning_rate": 3.4124087591240875e-05,
      "loss": 0.249,
      "step": 1370
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 50.673316955566406,
      "learning_rate": 3.3941605839416055e-05,
      "loss": 0.2586,
      "step": 1380
    },
    {
      "epoch": 1.287037037037037,
      "grad_norm": 24.35654640197754,
      "learning_rate": 3.375912408759124e-05,
      "loss": 0.1819,
      "step": 1390
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 40.97999572753906,
      "learning_rate": 3.357664233576642e-05,
      "loss": 0.2722,
      "step": 1400
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 0.09508853405714035,
      "learning_rate": 3.339416058394161e-05,
      "loss": 0.0471,
      "step": 1410
    },
    {
      "epoch": 1.3148148148148149,
      "grad_norm": 0.32565051317214966,
      "learning_rate": 3.321167883211679e-05,
      "loss": 0.3057,
      "step": 1420
    },
    {
      "epoch": 1.324074074074074,
      "grad_norm": 4.2595720291137695,
      "learning_rate": 3.302919708029197e-05,
      "loss": 0.1593,
      "step": 1430
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 6.147101402282715,
      "learning_rate": 3.284671532846715e-05,
      "loss": 0.2351,
      "step": 1440
    },
    {
      "epoch": 1.3425925925925926,
      "grad_norm": 0.46087366342544556,
      "learning_rate": 3.266423357664234e-05,
      "loss": 0.2452,
      "step": 1450
    },
    {
      "epoch": 1.3518518518518519,
      "grad_norm": 15.56445598602295,
      "learning_rate": 3.248175182481752e-05,
      "loss": 0.2787,
      "step": 1460
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 19.625764846801758,
      "learning_rate": 3.229927007299271e-05,
      "loss": 0.238,
      "step": 1470
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 0.3761966824531555,
      "learning_rate": 3.211678832116788e-05,
      "loss": 0.2942,
      "step": 1480
    },
    {
      "epoch": 1.3796296296296298,
      "grad_norm": 0.3529838025569916,
      "learning_rate": 3.193430656934307e-05,
      "loss": 0.345,
      "step": 1490
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 10.097901344299316,
      "learning_rate": 3.175182481751825e-05,
      "loss": 0.227,
      "step": 1500
    },
    {
      "epoch": 1.3981481481481481,
      "grad_norm": 5.841483116149902,
      "learning_rate": 3.156934306569343e-05,
      "loss": 0.2786,
      "step": 1510
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 0.8458728194236755,
      "learning_rate": 3.138686131386862e-05,
      "loss": 0.3662,
      "step": 1520
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.9632157683372498,
      "learning_rate": 3.12043795620438e-05,
      "loss": 0.1843,
      "step": 1530
    },
    {
      "epoch": 1.425925925925926,
      "grad_norm": 14.629302024841309,
      "learning_rate": 3.102189781021898e-05,
      "loss": 0.1063,
      "step": 1540
    },
    {
      "epoch": 1.4351851851851851,
      "grad_norm": 0.168002650141716,
      "learning_rate": 3.083941605839416e-05,
      "loss": 0.0468,
      "step": 1550
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.13882769644260406,
      "learning_rate": 3.0656934306569346e-05,
      "loss": 0.2058,
      "step": 1560
    },
    {
      "epoch": 1.4537037037037037,
      "grad_norm": 0.48846906423568726,
      "learning_rate": 3.0474452554744527e-05,
      "loss": 0.3153,
      "step": 1570
    },
    {
      "epoch": 1.462962962962963,
      "grad_norm": 0.22548851370811462,
      "learning_rate": 3.029197080291971e-05,
      "loss": 0.1648,
      "step": 1580
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.22565826773643494,
      "learning_rate": 3.010948905109489e-05,
      "loss": 0.2897,
      "step": 1590
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 0.3517003357410431,
      "learning_rate": 2.992700729927008e-05,
      "loss": 0.1875,
      "step": 1600
    },
    {
      "epoch": 1.4907407407407407,
      "grad_norm": 0.3924640715122223,
      "learning_rate": 2.9744525547445256e-05,
      "loss": 0.1763,
      "step": 1610
    },
    {
      "epoch": 1.5,
      "grad_norm": 5.143990516662598,
      "learning_rate": 2.9562043795620443e-05,
      "loss": 0.2745,
      "step": 1620
    },
    {
      "epoch": 1.5092592592592593,
      "grad_norm": 10.758770942687988,
      "learning_rate": 2.9379562043795624e-05,
      "loss": 0.3032,
      "step": 1630
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 11.885019302368164,
      "learning_rate": 2.91970802919708e-05,
      "loss": 0.2743,
      "step": 1640
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 17.324167251586914,
      "learning_rate": 2.9014598540145988e-05,
      "loss": 0.1711,
      "step": 1650
    },
    {
      "epoch": 1.5370370370370372,
      "grad_norm": 0.5722947716712952,
      "learning_rate": 2.883211678832117e-05,
      "loss": 0.0966,
      "step": 1660
    },
    {
      "epoch": 1.5462962962962963,
      "grad_norm": 0.22799400985240936,
      "learning_rate": 2.8649635036496353e-05,
      "loss": 0.2257,
      "step": 1670
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 3.2924299240112305,
      "learning_rate": 2.8467153284671533e-05,
      "loss": 0.1044,
      "step": 1680
    },
    {
      "epoch": 1.5648148148148149,
      "grad_norm": 0.15602757036685944,
      "learning_rate": 2.8284671532846717e-05,
      "loss": 0.2086,
      "step": 1690
    },
    {
      "epoch": 1.574074074074074,
      "grad_norm": 12.203898429870605,
      "learning_rate": 2.8102189781021898e-05,
      "loss": 0.1204,
      "step": 1700
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.15461215376853943,
      "learning_rate": 2.7919708029197085e-05,
      "loss": 0.2819,
      "step": 1710
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 0.20281949639320374,
      "learning_rate": 2.7737226277372262e-05,
      "loss": 0.2688,
      "step": 1720
    },
    {
      "epoch": 1.6018518518518519,
      "grad_norm": 0.7523446679115295,
      "learning_rate": 2.755474452554745e-05,
      "loss": 0.152,
      "step": 1730
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.45575398206710815,
      "learning_rate": 2.737226277372263e-05,
      "loss": 0.3631,
      "step": 1740
    },
    {
      "epoch": 1.6203703703703702,
      "grad_norm": 0.11509936302900314,
      "learning_rate": 2.7189781021897807e-05,
      "loss": 0.0726,
      "step": 1750
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 0.1863016039133072,
      "learning_rate": 2.7007299270072995e-05,
      "loss": 0.1441,
      "step": 1760
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 0.6519466042518616,
      "learning_rate": 2.6824817518248175e-05,
      "loss": 0.178,
      "step": 1770
    },
    {
      "epoch": 1.6481481481481481,
      "grad_norm": 0.21318283677101135,
      "learning_rate": 2.664233576642336e-05,
      "loss": 0.0075,
      "step": 1780
    },
    {
      "epoch": 1.6574074074074074,
      "grad_norm": 6.689384460449219,
      "learning_rate": 2.645985401459854e-05,
      "loss": 0.2407,
      "step": 1790
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.08237403631210327,
      "learning_rate": 2.6277372262773724e-05,
      "loss": 0.208,
      "step": 1800
    },
    {
      "epoch": 1.675925925925926,
      "grad_norm": 0.23222419619560242,
      "learning_rate": 2.6094890510948904e-05,
      "loss": 0.1832,
      "step": 1810
    },
    {
      "epoch": 1.6851851851851851,
      "grad_norm": 0.17340774834156036,
      "learning_rate": 2.591240875912409e-05,
      "loss": 0.0913,
      "step": 1820
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 0.33473050594329834,
      "learning_rate": 2.572992700729927e-05,
      "loss": 0.0704,
      "step": 1830
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 0.10370326042175293,
      "learning_rate": 2.5547445255474456e-05,
      "loss": 0.3138,
      "step": 1840
    },
    {
      "epoch": 1.7129629629629628,
      "grad_norm": 0.07681365311145782,
      "learning_rate": 2.5364963503649637e-05,
      "loss": 0.0793,
      "step": 1850
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.08348649740219116,
      "learning_rate": 2.518248175182482e-05,
      "loss": 0.1058,
      "step": 1860
    },
    {
      "epoch": 1.7314814814814814,
      "grad_norm": 0.06613292545080185,
      "learning_rate": 2.5e-05,
      "loss": 0.1069,
      "step": 1870
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 4.002739429473877,
      "learning_rate": 2.4817518248175185e-05,
      "loss": 0.3665,
      "step": 1880
    },
    {
      "epoch": 1.75,
      "grad_norm": 6.431093215942383,
      "learning_rate": 2.4635036496350366e-05,
      "loss": 0.2402,
      "step": 1890
    },
    {
      "epoch": 1.7592592592592593,
      "grad_norm": 117.00359344482422,
      "learning_rate": 2.445255474452555e-05,
      "loss": 0.2372,
      "step": 1900
    },
    {
      "epoch": 1.7685185185185186,
      "grad_norm": 38.083717346191406,
      "learning_rate": 2.4270072992700734e-05,
      "loss": 0.3725,
      "step": 1910
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.16948658227920532,
      "learning_rate": 2.4087591240875914e-05,
      "loss": 0.1542,
      "step": 1920
    },
    {
      "epoch": 1.7870370370370372,
      "grad_norm": 69.4719467163086,
      "learning_rate": 2.3905109489051098e-05,
      "loss": 0.2674,
      "step": 1930
    },
    {
      "epoch": 1.7962962962962963,
      "grad_norm": 0.27897435426712036,
      "learning_rate": 2.372262773722628e-05,
      "loss": 0.2343,
      "step": 1940
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 30.42363929748535,
      "learning_rate": 2.354014598540146e-05,
      "loss": 0.3602,
      "step": 1950
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 13.312868118286133,
      "learning_rate": 2.3357664233576643e-05,
      "loss": 0.2008,
      "step": 1960
    },
    {
      "epoch": 1.824074074074074,
      "grad_norm": 2.6034669876098633,
      "learning_rate": 2.3175182481751824e-05,
      "loss": 0.231,
      "step": 1970
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.24791917204856873,
      "learning_rate": 2.2992700729927008e-05,
      "loss": 0.1837,
      "step": 1980
    },
    {
      "epoch": 1.8425925925925926,
      "grad_norm": 0.2985565960407257,
      "learning_rate": 2.2810218978102192e-05,
      "loss": 0.0504,
      "step": 1990
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 13.517767906188965,
      "learning_rate": 2.2627737226277372e-05,
      "loss": 0.2627,
      "step": 2000
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 0.3465861976146698,
      "learning_rate": 2.2445255474452556e-05,
      "loss": 0.2976,
      "step": 2010
    },
    {
      "epoch": 1.8703703703703702,
      "grad_norm": 0.11328116059303284,
      "learning_rate": 2.226277372262774e-05,
      "loss": 0.0378,
      "step": 2020
    },
    {
      "epoch": 1.8796296296296298,
      "grad_norm": 4.661475658416748,
      "learning_rate": 2.208029197080292e-05,
      "loss": 0.1235,
      "step": 2030
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.20670129358768463,
      "learning_rate": 2.1897810218978105e-05,
      "loss": 0.2767,
      "step": 2040
    },
    {
      "epoch": 1.8981481481481481,
      "grad_norm": 14.377973556518555,
      "learning_rate": 2.1715328467153285e-05,
      "loss": 0.2178,
      "step": 2050
    },
    {
      "epoch": 1.9074074074074074,
      "grad_norm": 47.3428955078125,
      "learning_rate": 2.1532846715328466e-05,
      "loss": 0.2265,
      "step": 2060
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.24683994054794312,
      "learning_rate": 2.135036496350365e-05,
      "loss": 0.2148,
      "step": 2070
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 0.5210151076316833,
      "learning_rate": 2.1167883211678834e-05,
      "loss": 0.072,
      "step": 2080
    },
    {
      "epoch": 1.9351851851851851,
      "grad_norm": 39.94437789916992,
      "learning_rate": 2.0985401459854014e-05,
      "loss": 0.1482,
      "step": 2090
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.11659982055425644,
      "learning_rate": 2.08029197080292e-05,
      "loss": 0.2507,
      "step": 2100
    },
    {
      "epoch": 1.9537037037037037,
      "grad_norm": 7.330910682678223,
      "learning_rate": 2.062043795620438e-05,
      "loss": 0.3402,
      "step": 2110
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 1.8657934665679932,
      "learning_rate": 2.0437956204379563e-05,
      "loss": 0.3429,
      "step": 2120
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 0.13937658071517944,
      "learning_rate": 2.0255474452554747e-05,
      "loss": 0.0792,
      "step": 2130
    },
    {
      "epoch": 1.9814814814814814,
      "grad_norm": 0.14091216027736664,
      "learning_rate": 2.0072992700729927e-05,
      "loss": 0.3123,
      "step": 2140
    },
    {
      "epoch": 1.9907407407407407,
      "grad_norm": 6.653958320617676,
      "learning_rate": 1.989051094890511e-05,
      "loss": 0.2287,
      "step": 2150
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2732711434364319,
      "learning_rate": 1.9708029197080295e-05,
      "loss": 0.2182,
      "step": 2160
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.19622977077960968,
      "eval_runtime": 6.8075,
      "eval_samples_per_second": 141.02,
      "eval_steps_per_second": 8.814,
      "step": 2160
    },
    {
      "epoch": 2.009259259259259,
      "grad_norm": 6.131613254547119,
      "learning_rate": 1.9525547445255476e-05,
      "loss": 0.2082,
      "step": 2170
    },
    {
      "epoch": 2.0185185185185186,
      "grad_norm": 0.2127508521080017,
      "learning_rate": 1.934306569343066e-05,
      "loss": 0.0362,
      "step": 2180
    },
    {
      "epoch": 2.0277777777777777,
      "grad_norm": 28.18622589111328,
      "learning_rate": 1.916058394160584e-05,
      "loss": 0.044,
      "step": 2190
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 26.14703941345215,
      "learning_rate": 1.897810218978102e-05,
      "loss": 0.196,
      "step": 2200
    },
    {
      "epoch": 2.0462962962962963,
      "grad_norm": 0.10493680089712143,
      "learning_rate": 1.8795620437956205e-05,
      "loss": 0.0818,
      "step": 2210
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 0.0856555849313736,
      "learning_rate": 1.861313868613139e-05,
      "loss": 0.1246,
      "step": 2220
    },
    {
      "epoch": 2.064814814814815,
      "grad_norm": 0.15654481947422028,
      "learning_rate": 1.843065693430657e-05,
      "loss": 0.2976,
      "step": 2230
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 25.631229400634766,
      "learning_rate": 1.8248175182481753e-05,
      "loss": 0.2786,
      "step": 2240
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.5144574642181396,
      "learning_rate": 1.8065693430656934e-05,
      "loss": 0.1549,
      "step": 2250
    },
    {
      "epoch": 2.0925925925925926,
      "grad_norm": 18.191234588623047,
      "learning_rate": 1.7883211678832118e-05,
      "loss": 0.1892,
      "step": 2260
    },
    {
      "epoch": 2.1018518518518516,
      "grad_norm": 2.1222219467163086,
      "learning_rate": 1.7700729927007302e-05,
      "loss": 0.2502,
      "step": 2270
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 9.474687576293945,
      "learning_rate": 1.7518248175182482e-05,
      "loss": 0.2221,
      "step": 2280
    },
    {
      "epoch": 2.1203703703703702,
      "grad_norm": 2.687744140625,
      "learning_rate": 1.7335766423357666e-05,
      "loss": 0.2424,
      "step": 2290
    },
    {
      "epoch": 2.1296296296296298,
      "grad_norm": 0.28762492537498474,
      "learning_rate": 1.715328467153285e-05,
      "loss": 0.2025,
      "step": 2300
    },
    {
      "epoch": 2.138888888888889,
      "grad_norm": 0.4254722595214844,
      "learning_rate": 1.6970802919708028e-05,
      "loss": 0.0721,
      "step": 2310
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 13.25196647644043,
      "learning_rate": 1.678832116788321e-05,
      "loss": 0.1255,
      "step": 2320
    },
    {
      "epoch": 2.1574074074074074,
      "grad_norm": 149.44381713867188,
      "learning_rate": 1.6605839416058395e-05,
      "loss": 0.2272,
      "step": 2330
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.5685735940933228,
      "learning_rate": 1.6423357664233576e-05,
      "loss": 0.0201,
      "step": 2340
    },
    {
      "epoch": 2.175925925925926,
      "grad_norm": 0.16107529401779175,
      "learning_rate": 1.624087591240876e-05,
      "loss": 0.0061,
      "step": 2350
    },
    {
      "epoch": 2.185185185185185,
      "grad_norm": 0.17616359889507294,
      "learning_rate": 1.605839416058394e-05,
      "loss": 0.1453,
      "step": 2360
    },
    {
      "epoch": 2.1944444444444446,
      "grad_norm": 0.11348415911197662,
      "learning_rate": 1.5875912408759125e-05,
      "loss": 0.1977,
      "step": 2370
    },
    {
      "epoch": 2.2037037037037037,
      "grad_norm": 0.07886960357427597,
      "learning_rate": 1.569343065693431e-05,
      "loss": 0.0933,
      "step": 2380
    },
    {
      "epoch": 2.212962962962963,
      "grad_norm": 0.16628465056419373,
      "learning_rate": 1.551094890510949e-05,
      "loss": 0.0842,
      "step": 2390
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 4.524201393127441,
      "learning_rate": 1.5328467153284673e-05,
      "loss": 0.1096,
      "step": 2400
    },
    {
      "epoch": 2.2314814814814814,
      "grad_norm": 16.510398864746094,
      "learning_rate": 1.5145985401459855e-05,
      "loss": 0.2716,
      "step": 2410
    },
    {
      "epoch": 2.240740740740741,
      "grad_norm": 0.1416519284248352,
      "learning_rate": 1.496350364963504e-05,
      "loss": 0.2094,
      "step": 2420
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.21612705290317535,
      "learning_rate": 1.4781021897810221e-05,
      "loss": 0.0628,
      "step": 2430
    },
    {
      "epoch": 2.259259259259259,
      "grad_norm": 0.19451077282428741,
      "learning_rate": 1.45985401459854e-05,
      "loss": 0.126,
      "step": 2440
    },
    {
      "epoch": 2.2685185185185186,
      "grad_norm": 0.22752487659454346,
      "learning_rate": 1.4416058394160584e-05,
      "loss": 0.1332,
      "step": 2450
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 0.14612002670764923,
      "learning_rate": 1.4233576642335767e-05,
      "loss": 0.1744,
      "step": 2460
    },
    {
      "epoch": 2.287037037037037,
      "grad_norm": 0.18854475021362305,
      "learning_rate": 1.4051094890510949e-05,
      "loss": 0.2045,
      "step": 2470
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 0.1649739295244217,
      "learning_rate": 1.3868613138686131e-05,
      "loss": 0.0983,
      "step": 2480
    },
    {
      "epoch": 2.3055555555555554,
      "grad_norm": 0.5343623161315918,
      "learning_rate": 1.3686131386861315e-05,
      "loss": 0.1721,
      "step": 2490
    },
    {
      "epoch": 2.314814814814815,
      "grad_norm": 0.12838880717754364,
      "learning_rate": 1.3503649635036497e-05,
      "loss": 0.1493,
      "step": 2500
    },
    {
      "epoch": 2.324074074074074,
      "grad_norm": 3.2386040687561035,
      "learning_rate": 1.332116788321168e-05,
      "loss": 0.0585,
      "step": 2510
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.16751916706562042,
      "learning_rate": 1.3138686131386862e-05,
      "loss": 0.2179,
      "step": 2520
    },
    {
      "epoch": 2.3425925925925926,
      "grad_norm": 76.24623107910156,
      "learning_rate": 1.2956204379562046e-05,
      "loss": 0.1734,
      "step": 2530
    },
    {
      "epoch": 2.351851851851852,
      "grad_norm": 24.200016021728516,
      "learning_rate": 1.2773722627737228e-05,
      "loss": 0.0099,
      "step": 2540
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 0.5744819045066833,
      "learning_rate": 1.259124087591241e-05,
      "loss": 0.121,
      "step": 2550
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 4.382321357727051,
      "learning_rate": 1.2408759124087593e-05,
      "loss": 0.0708,
      "step": 2560
    },
    {
      "epoch": 2.3796296296296298,
      "grad_norm": 0.1071663647890091,
      "learning_rate": 1.2226277372262775e-05,
      "loss": 0.2392,
      "step": 2570
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 0.15376433730125427,
      "learning_rate": 1.2043795620437957e-05,
      "loss": 0.1864,
      "step": 2580
    },
    {
      "epoch": 2.398148148148148,
      "grad_norm": 0.15750393271446228,
      "learning_rate": 1.186131386861314e-05,
      "loss": 0.0556,
      "step": 2590
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 1.6141995191574097,
      "learning_rate": 1.1678832116788322e-05,
      "loss": 0.0063,
      "step": 2600
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.12668044865131378,
      "learning_rate": 1.1496350364963504e-05,
      "loss": 0.3031,
      "step": 2610
    },
    {
      "epoch": 2.425925925925926,
      "grad_norm": 0.14192548394203186,
      "learning_rate": 1.1313868613138686e-05,
      "loss": 0.2612,
      "step": 2620
    },
    {
      "epoch": 2.435185185185185,
      "grad_norm": 4.101640224456787,
      "learning_rate": 1.113138686131387e-05,
      "loss": 0.276,
      "step": 2630
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.16724106669425964,
      "learning_rate": 1.0948905109489052e-05,
      "loss": 0.1584,
      "step": 2640
    },
    {
      "epoch": 2.4537037037037037,
      "grad_norm": 0.4115924537181854,
      "learning_rate": 1.0766423357664233e-05,
      "loss": 0.1473,
      "step": 2650
    },
    {
      "epoch": 2.462962962962963,
      "grad_norm": 0.1640041619539261,
      "learning_rate": 1.0583941605839417e-05,
      "loss": 0.1083,
      "step": 2660
    },
    {
      "epoch": 2.4722222222222223,
      "grad_norm": 5.482431411743164,
      "learning_rate": 1.04014598540146e-05,
      "loss": 0.1385,
      "step": 2670
    },
    {
      "epoch": 2.4814814814814814,
      "grad_norm": 0.1321045160293579,
      "learning_rate": 1.0218978102189781e-05,
      "loss": 0.2051,
      "step": 2680
    },
    {
      "epoch": 2.490740740740741,
      "grad_norm": 140.26022338867188,
      "learning_rate": 1.0036496350364964e-05,
      "loss": 0.0863,
      "step": 2690
    },
    {
      "epoch": 2.5,
      "grad_norm": 7.982410430908203,
      "learning_rate": 9.854014598540148e-06,
      "loss": 0.1137,
      "step": 2700
    },
    {
      "epoch": 2.5092592592592595,
      "grad_norm": 29.915699005126953,
      "learning_rate": 9.67153284671533e-06,
      "loss": 0.2374,
      "step": 2710
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 0.1559486836194992,
      "learning_rate": 9.48905109489051e-06,
      "loss": 0.1454,
      "step": 2720
    },
    {
      "epoch": 2.5277777777777777,
      "grad_norm": 40.425559997558594,
      "learning_rate": 9.306569343065694e-06,
      "loss": 0.1381,
      "step": 2730
    },
    {
      "epoch": 2.537037037037037,
      "grad_norm": 5.445763111114502,
      "learning_rate": 9.124087591240877e-06,
      "loss": 0.2722,
      "step": 2740
    },
    {
      "epoch": 2.5462962962962963,
      "grad_norm": 0.2062668353319168,
      "learning_rate": 8.941605839416059e-06,
      "loss": 0.1406,
      "step": 2750
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 4.183434009552002,
      "learning_rate": 8.759124087591241e-06,
      "loss": 0.1809,
      "step": 2760
    },
    {
      "epoch": 2.564814814814815,
      "grad_norm": 5.580758094787598,
      "learning_rate": 8.576642335766425e-06,
      "loss": 0.3869,
      "step": 2770
    },
    {
      "epoch": 2.574074074074074,
      "grad_norm": 14.560173988342285,
      "learning_rate": 8.394160583941606e-06,
      "loss": 0.1117,
      "step": 2780
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.21696455776691437,
      "learning_rate": 8.211678832116788e-06,
      "loss": 0.0477,
      "step": 2790
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 10.19603157043457,
      "learning_rate": 8.02919708029197e-06,
      "loss": 0.1899,
      "step": 2800
    },
    {
      "epoch": 2.601851851851852,
      "grad_norm": 0.21147294342517853,
      "learning_rate": 7.846715328467154e-06,
      "loss": 0.1454,
      "step": 2810
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 4.670087814331055,
      "learning_rate": 7.664233576642336e-06,
      "loss": 0.1249,
      "step": 2820
    },
    {
      "epoch": 2.6203703703703702,
      "grad_norm": 0.49945512413978577,
      "learning_rate": 7.48175182481752e-06,
      "loss": 0.0053,
      "step": 2830
    },
    {
      "epoch": 2.6296296296296298,
      "grad_norm": 1.113162875175476,
      "learning_rate": 7.2992700729927e-06,
      "loss": 0.0734,
      "step": 2840
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 16.079362869262695,
      "learning_rate": 7.116788321167883e-06,
      "loss": 0.2507,
      "step": 2850
    },
    {
      "epoch": 2.648148148148148,
      "grad_norm": 0.16069139540195465,
      "learning_rate": 6.9343065693430655e-06,
      "loss": 0.0676,
      "step": 2860
    },
    {
      "epoch": 2.6574074074074074,
      "grad_norm": 0.30900949239730835,
      "learning_rate": 6.751824817518249e-06,
      "loss": 0.2105,
      "step": 2870
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 5.330308437347412,
      "learning_rate": 6.569343065693431e-06,
      "loss": 0.1797,
      "step": 2880
    },
    {
      "epoch": 2.675925925925926,
      "grad_norm": 0.17068693041801453,
      "learning_rate": 6.386861313868614e-06,
      "loss": 0.1099,
      "step": 2890
    },
    {
      "epoch": 2.685185185185185,
      "grad_norm": 0.14722152054309845,
      "learning_rate": 6.204379562043796e-06,
      "loss": 0.0814,
      "step": 2900
    },
    {
      "epoch": 2.6944444444444446,
      "grad_norm": 0.13966116309165955,
      "learning_rate": 6.0218978102189786e-06,
      "loss": 0.1818,
      "step": 2910
    },
    {
      "epoch": 2.7037037037037037,
      "grad_norm": 4.102071285247803,
      "learning_rate": 5.839416058394161e-06,
      "loss": 0.2522,
      "step": 2920
    },
    {
      "epoch": 2.712962962962963,
      "grad_norm": 0.4492831528186798,
      "learning_rate": 5.656934306569343e-06,
      "loss": 0.1296,
      "step": 2930
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 0.3177385628223419,
      "learning_rate": 5.474452554744526e-06,
      "loss": 0.0842,
      "step": 2940
    },
    {
      "epoch": 2.7314814814814814,
      "grad_norm": 0.14991998672485352,
      "learning_rate": 5.2919708029197084e-06,
      "loss": 0.1027,
      "step": 2950
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 0.14106254279613495,
      "learning_rate": 5.109489051094891e-06,
      "loss": 0.0078,
      "step": 2960
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.16927199065685272,
      "learning_rate": 4.927007299270074e-06,
      "loss": 0.3504,
      "step": 2970
    },
    {
      "epoch": 2.7592592592592595,
      "grad_norm": 0.13542450964450836,
      "learning_rate": 4.744525547445255e-06,
      "loss": 0.1301,
      "step": 2980
    },
    {
      "epoch": 2.7685185185185186,
      "grad_norm": 31.56759262084961,
      "learning_rate": 4.562043795620438e-06,
      "loss": 0.2153,
      "step": 2990
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.12417874485254288,
      "learning_rate": 4.379562043795621e-06,
      "loss": 0.2786,
      "step": 3000
    },
    {
      "epoch": 2.787037037037037,
      "grad_norm": 1.2653040885925293,
      "learning_rate": 4.197080291970803e-06,
      "loss": 0.2092,
      "step": 3010
    },
    {
      "epoch": 2.7962962962962963,
      "grad_norm": 0.20608679950237274,
      "learning_rate": 4.014598540145985e-06,
      "loss": 0.1193,
      "step": 3020
    },
    {
      "epoch": 2.8055555555555554,
      "grad_norm": 0.26352784037590027,
      "learning_rate": 3.832116788321168e-06,
      "loss": 0.1305,
      "step": 3030
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 72.09034729003906,
      "learning_rate": 3.64963503649635e-06,
      "loss": 0.0806,
      "step": 3040
    },
    {
      "epoch": 2.824074074074074,
      "grad_norm": 0.17131070792675018,
      "learning_rate": 3.4671532846715328e-06,
      "loss": 0.0084,
      "step": 3050
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.19439946115016937,
      "learning_rate": 3.2846715328467155e-06,
      "loss": 0.1937,
      "step": 3060
    },
    {
      "epoch": 2.8425925925925926,
      "grad_norm": 0.2181774228811264,
      "learning_rate": 3.102189781021898e-06,
      "loss": 0.1301,
      "step": 3070
    },
    {
      "epoch": 2.851851851851852,
      "grad_norm": 0.2140974998474121,
      "learning_rate": 2.9197080291970804e-06,
      "loss": 0.0755,
      "step": 3080
    },
    {
      "epoch": 2.861111111111111,
      "grad_norm": 0.16936077177524567,
      "learning_rate": 2.737226277372263e-06,
      "loss": 0.1488,
      "step": 3090
    },
    {
      "epoch": 2.8703703703703702,
      "grad_norm": 0.11679724603891373,
      "learning_rate": 2.5547445255474454e-06,
      "loss": 0.1809,
      "step": 3100
    },
    {
      "epoch": 2.8796296296296298,
      "grad_norm": 0.14970898628234863,
      "learning_rate": 2.3722627737226276e-06,
      "loss": 0.0308,
      "step": 3110
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.11564654856920242,
      "learning_rate": 2.1897810218978103e-06,
      "loss": 0.1415,
      "step": 3120
    },
    {
      "epoch": 2.898148148148148,
      "grad_norm": 0.16392876207828522,
      "learning_rate": 2.0072992700729926e-06,
      "loss": 0.1374,
      "step": 3130
    },
    {
      "epoch": 2.9074074074074074,
      "grad_norm": 0.12648171186447144,
      "learning_rate": 1.824817518248175e-06,
      "loss": 0.0751,
      "step": 3140
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.12576685845851898,
      "learning_rate": 1.6423357664233577e-06,
      "loss": 0.1306,
      "step": 3150
    },
    {
      "epoch": 2.925925925925926,
      "grad_norm": 4.379049301147461,
      "learning_rate": 1.4598540145985402e-06,
      "loss": 0.1801,
      "step": 3160
    },
    {
      "epoch": 2.935185185185185,
      "grad_norm": 43.352813720703125,
      "learning_rate": 1.2773722627737227e-06,
      "loss": 0.1199,
      "step": 3170
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 7.880566120147705,
      "learning_rate": 1.0948905109489052e-06,
      "loss": 0.1084,
      "step": 3180
    },
    {
      "epoch": 2.9537037037037037,
      "grad_norm": 0.11372745037078857,
      "learning_rate": 9.124087591240875e-07,
      "loss": 0.1214,
      "step": 3190
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 0.1545768678188324,
      "learning_rate": 7.299270072992701e-07,
      "loss": 0.1044,
      "step": 3200
    },
    {
      "epoch": 2.9722222222222223,
      "grad_norm": 1.7141355276107788,
      "learning_rate": 5.474452554744526e-07,
      "loss": 0.1011,
      "step": 3210
    },
    {
      "epoch": 2.9814814814814814,
      "grad_norm": 0.3701731562614441,
      "learning_rate": 3.6496350364963505e-07,
      "loss": 0.2403,
      "step": 3220
    },
    {
      "epoch": 2.9907407407407405,
      "grad_norm": 0.12408819794654846,
      "learning_rate": 1.8248175182481753e-07,
      "loss": 0.0042,
      "step": 3230
    },
    {
      "epoch": 3.0,
      "grad_norm": 5.442073345184326,
      "learning_rate": 0.0,
      "loss": 0.1992,
      "step": 3240
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.24938319623470306,
      "eval_runtime": 6.8358,
      "eval_samples_per_second": 140.437,
      "eval_steps_per_second": 8.777,
      "step": 3240
    }
  ],
  "logging_steps": 10,
  "max_steps": 3240,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1704762305441280.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
