{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2160,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009259259259259259,
      "grad_norm": 4.550492286682129,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.6916,
      "step": 10
    },
    {
      "epoch": 0.018518518518518517,
      "grad_norm": 6.232330799102783,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6656,
      "step": 20
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 4.445793151855469,
      "learning_rate": 3e-06,
      "loss": 0.6812,
      "step": 30
    },
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 5.60944128036499,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.6474,
      "step": 40
    },
    {
      "epoch": 0.046296296296296294,
      "grad_norm": 3.5997769832611084,
      "learning_rate": 5e-06,
      "loss": 0.6418,
      "step": 50
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 7.984439373016357,
      "learning_rate": 6e-06,
      "loss": 0.5944,
      "step": 60
    },
    {
      "epoch": 0.06481481481481481,
      "grad_norm": 6.693225383758545,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.5496,
      "step": 70
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 8.582216262817383,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.4586,
      "step": 80
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 5.682147026062012,
      "learning_rate": 9e-06,
      "loss": 0.4478,
      "step": 90
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 7.547580718994141,
      "learning_rate": 1e-05,
      "loss": 0.441,
      "step": 100
    },
    {
      "epoch": 0.10185185185185185,
      "grad_norm": 3.3105263710021973,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.4048,
      "step": 110
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 6.392953395843506,
      "learning_rate": 1.2e-05,
      "loss": 0.3265,
      "step": 120
    },
    {
      "epoch": 0.12037037037037036,
      "grad_norm": 3.6467247009277344,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.3488,
      "step": 130
    },
    {
      "epoch": 0.12962962962962962,
      "grad_norm": 4.149466514587402,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.2552,
      "step": 140
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 22.582124710083008,
      "learning_rate": 1.5e-05,
      "loss": 0.2068,
      "step": 150
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 32.74368667602539,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.2276,
      "step": 160
    },
    {
      "epoch": 0.1574074074074074,
      "grad_norm": 16.098928451538086,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.1763,
      "step": 170
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 4.606746196746826,
      "learning_rate": 1.8e-05,
      "loss": 0.2836,
      "step": 180
    },
    {
      "epoch": 0.17592592592592593,
      "grad_norm": 10.924084663391113,
      "learning_rate": 1.9e-05,
      "loss": 0.2803,
      "step": 190
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 1.0930407047271729,
      "learning_rate": 2e-05,
      "loss": 0.2244,
      "step": 200
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 51.22146224975586,
      "learning_rate": 2.1e-05,
      "loss": 0.1463,
      "step": 210
    },
    {
      "epoch": 0.2037037037037037,
      "grad_norm": 3.85090970993042,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.2529,
      "step": 220
    },
    {
      "epoch": 0.21296296296296297,
      "grad_norm": 2.4174838066101074,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0817,
      "step": 230
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.6196881532669067,
      "learning_rate": 2.4e-05,
      "loss": 0.435,
      "step": 240
    },
    {
      "epoch": 0.23148148148148148,
      "grad_norm": 0.7118971943855286,
      "learning_rate": 2.5e-05,
      "loss": 0.1463,
      "step": 250
    },
    {
      "epoch": 0.24074074074074073,
      "grad_norm": 0.3623069226741791,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.2622,
      "step": 260
    },
    {
      "epoch": 0.25,
      "grad_norm": 20.690284729003906,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.482,
      "step": 270
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 20.324012756347656,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.426,
      "step": 280
    },
    {
      "epoch": 0.26851851851851855,
      "grad_norm": 24.113147735595703,
      "learning_rate": 2.9e-05,
      "loss": 0.2448,
      "step": 290
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 6.845015048980713,
      "learning_rate": 3e-05,
      "loss": 0.3658,
      "step": 300
    },
    {
      "epoch": 0.28703703703703703,
      "grad_norm": 2.4010114669799805,
      "learning_rate": 3.1e-05,
      "loss": 0.2103,
      "step": 310
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 24.51532745361328,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.3415,
      "step": 320
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 18.402429580688477,
      "learning_rate": 3.3e-05,
      "loss": 0.3464,
      "step": 330
    },
    {
      "epoch": 0.3148148148148148,
      "grad_norm": 1.2668803930282593,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.3851,
      "step": 340
    },
    {
      "epoch": 0.32407407407407407,
      "grad_norm": 15.53378677368164,
      "learning_rate": 3.5e-05,
      "loss": 0.1622,
      "step": 350
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.29672160744667053,
      "learning_rate": 3.6e-05,
      "loss": 0.2889,
      "step": 360
    },
    {
      "epoch": 0.3425925925925926,
      "grad_norm": 20.29379653930664,
      "learning_rate": 3.7e-05,
      "loss": 0.4608,
      "step": 370
    },
    {
      "epoch": 0.35185185185185186,
      "grad_norm": 7.3528151512146,
      "learning_rate": 3.8e-05,
      "loss": 0.3412,
      "step": 380
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 9.858256340026855,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.5653,
      "step": 390
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 11.61242961883545,
      "learning_rate": 4e-05,
      "loss": 0.3345,
      "step": 400
    },
    {
      "epoch": 0.37962962962962965,
      "grad_norm": 4.379803657531738,
      "learning_rate": 4.1e-05,
      "loss": 0.1512,
      "step": 410
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.32314178347587585,
      "learning_rate": 4.2e-05,
      "loss": 0.1792,
      "step": 420
    },
    {
      "epoch": 0.39814814814814814,
      "grad_norm": 29.356653213500977,
      "learning_rate": 4.3e-05,
      "loss": 0.4072,
      "step": 430
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 20.914627075195312,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.4486,
      "step": 440
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.5001393556594849,
      "learning_rate": 4.5e-05,
      "loss": 0.1879,
      "step": 450
    },
    {
      "epoch": 0.42592592592592593,
      "grad_norm": 16.836387634277344,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.3982,
      "step": 460
    },
    {
      "epoch": 0.4351851851851852,
      "grad_norm": 0.21681749820709229,
      "learning_rate": 4.7e-05,
      "loss": 0.1575,
      "step": 470
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.40036243200302124,
      "learning_rate": 4.8e-05,
      "loss": 0.2753,
      "step": 480
    },
    {
      "epoch": 0.4537037037037037,
      "grad_norm": 12.110563278198242,
      "learning_rate": 4.9e-05,
      "loss": 0.1984,
      "step": 490
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 7.3547468185424805,
      "learning_rate": 5e-05,
      "loss": 0.4596,
      "step": 500
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 3.484039783477783,
      "learning_rate": 4.981751824817518e-05,
      "loss": 0.3587,
      "step": 510
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 8.64892578125,
      "learning_rate": 4.963503649635037e-05,
      "loss": 0.2969,
      "step": 520
    },
    {
      "epoch": 0.49074074074074076,
      "grad_norm": 14.839223861694336,
      "learning_rate": 4.945255474452555e-05,
      "loss": 0.1554,
      "step": 530
    },
    {
      "epoch": 0.5,
      "grad_norm": 3.990370273590088,
      "learning_rate": 4.927007299270073e-05,
      "loss": 0.2158,
      "step": 540
    },
    {
      "epoch": 0.5092592592592593,
      "grad_norm": 0.3234359323978424,
      "learning_rate": 4.908759124087591e-05,
      "loss": 0.1685,
      "step": 550
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 0.23462995886802673,
      "learning_rate": 4.89051094890511e-05,
      "loss": 0.2804,
      "step": 560
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 4.2551422119140625,
      "learning_rate": 4.872262773722628e-05,
      "loss": 0.3071,
      "step": 570
    },
    {
      "epoch": 0.5370370370370371,
      "grad_norm": 3.469034433364868,
      "learning_rate": 4.854014598540147e-05,
      "loss": 0.3387,
      "step": 580
    },
    {
      "epoch": 0.5462962962962963,
      "grad_norm": 4.88865852355957,
      "learning_rate": 4.835766423357664e-05,
      "loss": 0.3919,
      "step": 590
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 4.449126720428467,
      "learning_rate": 4.817518248175183e-05,
      "loss": 0.2205,
      "step": 600
    },
    {
      "epoch": 0.5648148148148148,
      "grad_norm": 28.342832565307617,
      "learning_rate": 4.799270072992701e-05,
      "loss": 0.1963,
      "step": 610
    },
    {
      "epoch": 0.5740740740740741,
      "grad_norm": 0.346371591091156,
      "learning_rate": 4.7810218978102196e-05,
      "loss": 0.1804,
      "step": 620
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.3225829005241394,
      "learning_rate": 4.762773722627738e-05,
      "loss": 0.1624,
      "step": 630
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 13.402389526367188,
      "learning_rate": 4.744525547445256e-05,
      "loss": 0.4383,
      "step": 640
    },
    {
      "epoch": 0.6018518518518519,
      "grad_norm": 0.4054418206214905,
      "learning_rate": 4.726277372262774e-05,
      "loss": 0.2762,
      "step": 650
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.2290119081735611,
      "learning_rate": 4.708029197080292e-05,
      "loss": 0.1282,
      "step": 660
    },
    {
      "epoch": 0.6203703703703703,
      "grad_norm": 11.055265426635742,
      "learning_rate": 4.6897810218978106e-05,
      "loss": 0.1868,
      "step": 670
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 3.10210919380188,
      "learning_rate": 4.6715328467153287e-05,
      "loss": 0.2918,
      "step": 680
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 29.018381118774414,
      "learning_rate": 4.6532846715328474e-05,
      "loss": 0.6204,
      "step": 690
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 5.172561168670654,
      "learning_rate": 4.635036496350365e-05,
      "loss": 0.2357,
      "step": 700
    },
    {
      "epoch": 0.6574074074074074,
      "grad_norm": 2.1695382595062256,
      "learning_rate": 4.6167883211678835e-05,
      "loss": 0.4439,
      "step": 710
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 10.699932098388672,
      "learning_rate": 4.5985401459854016e-05,
      "loss": 0.2682,
      "step": 720
    },
    {
      "epoch": 0.6759259259259259,
      "grad_norm": 0.3140656352043152,
      "learning_rate": 4.58029197080292e-05,
      "loss": 0.1988,
      "step": 730
    },
    {
      "epoch": 0.6851851851851852,
      "grad_norm": 8.165070533752441,
      "learning_rate": 4.5620437956204383e-05,
      "loss": 0.2658,
      "step": 740
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.25319764018058777,
      "learning_rate": 4.5437956204379564e-05,
      "loss": 0.2199,
      "step": 750
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 0.4599943161010742,
      "learning_rate": 4.5255474452554745e-05,
      "loss": 0.192,
      "step": 760
    },
    {
      "epoch": 0.7129629629629629,
      "grad_norm": 8.760616302490234,
      "learning_rate": 4.5072992700729925e-05,
      "loss": 0.2613,
      "step": 770
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.4046090543270111,
      "learning_rate": 4.489051094890511e-05,
      "loss": 0.1515,
      "step": 780
    },
    {
      "epoch": 0.7314814814814815,
      "grad_norm": 6.534804344177246,
      "learning_rate": 4.470802919708029e-05,
      "loss": 0.2307,
      "step": 790
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 33.493465423583984,
      "learning_rate": 4.452554744525548e-05,
      "loss": 0.2796,
      "step": 800
    },
    {
      "epoch": 0.75,
      "grad_norm": 5.194834232330322,
      "learning_rate": 4.434306569343066e-05,
      "loss": 0.2841,
      "step": 810
    },
    {
      "epoch": 0.7592592592592593,
      "grad_norm": 0.43566134572029114,
      "learning_rate": 4.416058394160584e-05,
      "loss": 0.2849,
      "step": 820
    },
    {
      "epoch": 0.7685185185185185,
      "grad_norm": 0.9222869277000427,
      "learning_rate": 4.397810218978102e-05,
      "loss": 0.2522,
      "step": 830
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 8.53311824798584,
      "learning_rate": 4.379562043795621e-05,
      "loss": 0.3323,
      "step": 840
    },
    {
      "epoch": 0.7870370370370371,
      "grad_norm": 1.1674165725708008,
      "learning_rate": 4.361313868613139e-05,
      "loss": 0.1874,
      "step": 850
    },
    {
      "epoch": 0.7962962962962963,
      "grad_norm": 3.8547515869140625,
      "learning_rate": 4.343065693430657e-05,
      "loss": 0.1403,
      "step": 860
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 0.3197712302207947,
      "learning_rate": 4.324817518248175e-05,
      "loss": 0.1841,
      "step": 870
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 3.7556185722351074,
      "learning_rate": 4.306569343065693e-05,
      "loss": 0.2527,
      "step": 880
    },
    {
      "epoch": 0.8240740740740741,
      "grad_norm": 0.4065990746021271,
      "learning_rate": 4.288321167883212e-05,
      "loss": 0.202,
      "step": 890
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.5276533365249634,
      "learning_rate": 4.27007299270073e-05,
      "loss": 0.3149,
      "step": 900
    },
    {
      "epoch": 0.8425925925925926,
      "grad_norm": 0.5230464935302734,
      "learning_rate": 4.251824817518249e-05,
      "loss": 0.1537,
      "step": 910
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 0.975096583366394,
      "learning_rate": 4.233576642335767e-05,
      "loss": 0.4384,
      "step": 920
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 0.9170145988464355,
      "learning_rate": 4.215328467153285e-05,
      "loss": 0.1259,
      "step": 930
    },
    {
      "epoch": 0.8703703703703703,
      "grad_norm": 0.22495612502098083,
      "learning_rate": 4.197080291970803e-05,
      "loss": 0.2648,
      "step": 940
    },
    {
      "epoch": 0.8796296296296297,
      "grad_norm": 18.3745174407959,
      "learning_rate": 4.1788321167883216e-05,
      "loss": 0.2273,
      "step": 950
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.0534762144088745,
      "learning_rate": 4.16058394160584e-05,
      "loss": 0.2582,
      "step": 960
    },
    {
      "epoch": 0.8981481481481481,
      "grad_norm": 3.871079683303833,
      "learning_rate": 4.1423357664233584e-05,
      "loss": 0.2521,
      "step": 970
    },
    {
      "epoch": 0.9074074074074074,
      "grad_norm": 0.4285843074321747,
      "learning_rate": 4.124087591240876e-05,
      "loss": 0.1914,
      "step": 980
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.28197136521339417,
      "learning_rate": 4.1058394160583945e-05,
      "loss": 0.3096,
      "step": 990
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 1.2274856567382812,
      "learning_rate": 4.0875912408759126e-05,
      "loss": 0.1183,
      "step": 1000
    },
    {
      "epoch": 0.9351851851851852,
      "grad_norm": 32.35182189941406,
      "learning_rate": 4.0693430656934306e-05,
      "loss": 0.3863,
      "step": 1010
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 20.8210506439209,
      "learning_rate": 4.0510948905109494e-05,
      "loss": 0.3694,
      "step": 1020
    },
    {
      "epoch": 0.9537037037037037,
      "grad_norm": 0.30224674940109253,
      "learning_rate": 4.0328467153284674e-05,
      "loss": 0.2402,
      "step": 1030
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 30.352813720703125,
      "learning_rate": 4.0145985401459855e-05,
      "loss": 0.3605,
      "step": 1040
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 6.807135105133057,
      "learning_rate": 3.9963503649635035e-05,
      "loss": 0.1855,
      "step": 1050
    },
    {
      "epoch": 0.9814814814814815,
      "grad_norm": 12.593941688537598,
      "learning_rate": 3.978102189781022e-05,
      "loss": 0.2936,
      "step": 1060
    },
    {
      "epoch": 0.9907407407407407,
      "grad_norm": 0.15950970351696014,
      "learning_rate": 3.95985401459854e-05,
      "loss": 0.3429,
      "step": 1070
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8164092898368835,
      "learning_rate": 3.941605839416059e-05,
      "loss": 0.119,
      "step": 1080
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.20536135137081146,
      "eval_runtime": 6.8938,
      "eval_samples_per_second": 139.255,
      "eval_steps_per_second": 8.703,
      "step": 1080
    },
    {
      "epoch": 1.0092592592592593,
      "grad_norm": 0.3097158372402191,
      "learning_rate": 3.9233576642335764e-05,
      "loss": 0.1818,
      "step": 1090
    },
    {
      "epoch": 1.0185185185185186,
      "grad_norm": 50.9395866394043,
      "learning_rate": 3.905109489051095e-05,
      "loss": 0.3067,
      "step": 1100
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 3.985818386077881,
      "learning_rate": 3.886861313868613e-05,
      "loss": 0.3658,
      "step": 1110
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 0.5821428894996643,
      "learning_rate": 3.868613138686132e-05,
      "loss": 0.2305,
      "step": 1120
    },
    {
      "epoch": 1.0462962962962963,
      "grad_norm": 0.39441341161727905,
      "learning_rate": 3.85036496350365e-05,
      "loss": 0.1389,
      "step": 1130
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 18.54142951965332,
      "learning_rate": 3.832116788321168e-05,
      "loss": 0.3114,
      "step": 1140
    },
    {
      "epoch": 1.0648148148148149,
      "grad_norm": 1.2478320598602295,
      "learning_rate": 3.813868613138686e-05,
      "loss": 0.0899,
      "step": 1150
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 0.39251360297203064,
      "learning_rate": 3.795620437956204e-05,
      "loss": 0.1312,
      "step": 1160
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.3670021891593933,
      "learning_rate": 3.777372262773723e-05,
      "loss": 0.2359,
      "step": 1170
    },
    {
      "epoch": 1.0925925925925926,
      "grad_norm": 32.63560485839844,
      "learning_rate": 3.759124087591241e-05,
      "loss": 0.2043,
      "step": 1180
    },
    {
      "epoch": 1.1018518518518519,
      "grad_norm": 24.48093032836914,
      "learning_rate": 3.74087591240876e-05,
      "loss": 0.33,
      "step": 1190
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.21976271271705627,
      "learning_rate": 3.722627737226278e-05,
      "loss": 0.1404,
      "step": 1200
    },
    {
      "epoch": 1.1203703703703705,
      "grad_norm": 0.12458951026201248,
      "learning_rate": 3.704379562043796e-05,
      "loss": 0.2366,
      "step": 1210
    },
    {
      "epoch": 1.1296296296296295,
      "grad_norm": 3.090413808822632,
      "learning_rate": 3.686131386861314e-05,
      "loss": 0.0819,
      "step": 1220
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 34.408775329589844,
      "learning_rate": 3.6678832116788326e-05,
      "loss": 0.4742,
      "step": 1230
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 0.10971461981534958,
      "learning_rate": 3.649635036496351e-05,
      "loss": 0.2826,
      "step": 1240
    },
    {
      "epoch": 1.1574074074074074,
      "grad_norm": 0.27093034982681274,
      "learning_rate": 3.631386861313869e-05,
      "loss": 0.223,
      "step": 1250
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.8391903042793274,
      "learning_rate": 3.613138686131387e-05,
      "loss": 0.2742,
      "step": 1260
    },
    {
      "epoch": 1.175925925925926,
      "grad_norm": 8.285849571228027,
      "learning_rate": 3.594890510948905e-05,
      "loss": 0.1088,
      "step": 1270
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 10.635578155517578,
      "learning_rate": 3.5766423357664236e-05,
      "loss": 0.3244,
      "step": 1280
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 0.29189932346343994,
      "learning_rate": 3.5583941605839416e-05,
      "loss": 0.2264,
      "step": 1290
    },
    {
      "epoch": 1.2037037037037037,
      "grad_norm": 0.6712425351142883,
      "learning_rate": 3.5401459854014604e-05,
      "loss": 0.2409,
      "step": 1300
    },
    {
      "epoch": 1.212962962962963,
      "grad_norm": 0.37120822072029114,
      "learning_rate": 3.5218978102189784e-05,
      "loss": 0.1112,
      "step": 1310
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.0975128710269928,
      "learning_rate": 3.5036496350364965e-05,
      "loss": 0.0624,
      "step": 1320
    },
    {
      "epoch": 1.2314814814814814,
      "grad_norm": 0.4452650547027588,
      "learning_rate": 3.4854014598540145e-05,
      "loss": 0.1715,
      "step": 1330
    },
    {
      "epoch": 1.2407407407407407,
      "grad_norm": 0.6575754284858704,
      "learning_rate": 3.467153284671533e-05,
      "loss": 0.2457,
      "step": 1340
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.2158951759338379,
      "learning_rate": 3.448905109489051e-05,
      "loss": 0.2642,
      "step": 1350
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 0.23279589414596558,
      "learning_rate": 3.43065693430657e-05,
      "loss": 0.2027,
      "step": 1360
    },
    {
      "epoch": 1.2685185185185186,
      "grad_norm": 0.19058243930339813,
      "learning_rate": 3.4124087591240875e-05,
      "loss": 0.249,
      "step": 1370
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 50.673316955566406,
      "learning_rate": 3.3941605839416055e-05,
      "loss": 0.2586,
      "step": 1380
    },
    {
      "epoch": 1.287037037037037,
      "grad_norm": 24.35654640197754,
      "learning_rate": 3.375912408759124e-05,
      "loss": 0.1819,
      "step": 1390
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 40.97999572753906,
      "learning_rate": 3.357664233576642e-05,
      "loss": 0.2722,
      "step": 1400
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 0.09508853405714035,
      "learning_rate": 3.339416058394161e-05,
      "loss": 0.0471,
      "step": 1410
    },
    {
      "epoch": 1.3148148148148149,
      "grad_norm": 0.32565051317214966,
      "learning_rate": 3.321167883211679e-05,
      "loss": 0.3057,
      "step": 1420
    },
    {
      "epoch": 1.324074074074074,
      "grad_norm": 4.2595720291137695,
      "learning_rate": 3.302919708029197e-05,
      "loss": 0.1593,
      "step": 1430
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 6.147101402282715,
      "learning_rate": 3.284671532846715e-05,
      "loss": 0.2351,
      "step": 1440
    },
    {
      "epoch": 1.3425925925925926,
      "grad_norm": 0.46087366342544556,
      "learning_rate": 3.266423357664234e-05,
      "loss": 0.2452,
      "step": 1450
    },
    {
      "epoch": 1.3518518518518519,
      "grad_norm": 15.56445598602295,
      "learning_rate": 3.248175182481752e-05,
      "loss": 0.2787,
      "step": 1460
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 19.625764846801758,
      "learning_rate": 3.229927007299271e-05,
      "loss": 0.238,
      "step": 1470
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 0.3761966824531555,
      "learning_rate": 3.211678832116788e-05,
      "loss": 0.2942,
      "step": 1480
    },
    {
      "epoch": 1.3796296296296298,
      "grad_norm": 0.3529838025569916,
      "learning_rate": 3.193430656934307e-05,
      "loss": 0.345,
      "step": 1490
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 10.097901344299316,
      "learning_rate": 3.175182481751825e-05,
      "loss": 0.227,
      "step": 1500
    },
    {
      "epoch": 1.3981481481481481,
      "grad_norm": 5.841483116149902,
      "learning_rate": 3.156934306569343e-05,
      "loss": 0.2786,
      "step": 1510
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 0.8458728194236755,
      "learning_rate": 3.138686131386862e-05,
      "loss": 0.3662,
      "step": 1520
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.9632157683372498,
      "learning_rate": 3.12043795620438e-05,
      "loss": 0.1843,
      "step": 1530
    },
    {
      "epoch": 1.425925925925926,
      "grad_norm": 14.629302024841309,
      "learning_rate": 3.102189781021898e-05,
      "loss": 0.1063,
      "step": 1540
    },
    {
      "epoch": 1.4351851851851851,
      "grad_norm": 0.168002650141716,
      "learning_rate": 3.083941605839416e-05,
      "loss": 0.0468,
      "step": 1550
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.13882769644260406,
      "learning_rate": 3.0656934306569346e-05,
      "loss": 0.2058,
      "step": 1560
    },
    {
      "epoch": 1.4537037037037037,
      "grad_norm": 0.48846906423568726,
      "learning_rate": 3.0474452554744527e-05,
      "loss": 0.3153,
      "step": 1570
    },
    {
      "epoch": 1.462962962962963,
      "grad_norm": 0.22548851370811462,
      "learning_rate": 3.029197080291971e-05,
      "loss": 0.1648,
      "step": 1580
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.22565826773643494,
      "learning_rate": 3.010948905109489e-05,
      "loss": 0.2897,
      "step": 1590
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 0.3517003357410431,
      "learning_rate": 2.992700729927008e-05,
      "loss": 0.1875,
      "step": 1600
    },
    {
      "epoch": 1.4907407407407407,
      "grad_norm": 0.3924640715122223,
      "learning_rate": 2.9744525547445256e-05,
      "loss": 0.1763,
      "step": 1610
    },
    {
      "epoch": 1.5,
      "grad_norm": 5.143990516662598,
      "learning_rate": 2.9562043795620443e-05,
      "loss": 0.2745,
      "step": 1620
    },
    {
      "epoch": 1.5092592592592593,
      "grad_norm": 10.758770942687988,
      "learning_rate": 2.9379562043795624e-05,
      "loss": 0.3032,
      "step": 1630
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 11.885019302368164,
      "learning_rate": 2.91970802919708e-05,
      "loss": 0.2743,
      "step": 1640
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 17.324167251586914,
      "learning_rate": 2.9014598540145988e-05,
      "loss": 0.1711,
      "step": 1650
    },
    {
      "epoch": 1.5370370370370372,
      "grad_norm": 0.5722947716712952,
      "learning_rate": 2.883211678832117e-05,
      "loss": 0.0966,
      "step": 1660
    },
    {
      "epoch": 1.5462962962962963,
      "grad_norm": 0.22799400985240936,
      "learning_rate": 2.8649635036496353e-05,
      "loss": 0.2257,
      "step": 1670
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 3.2924299240112305,
      "learning_rate": 2.8467153284671533e-05,
      "loss": 0.1044,
      "step": 1680
    },
    {
      "epoch": 1.5648148148148149,
      "grad_norm": 0.15602757036685944,
      "learning_rate": 2.8284671532846717e-05,
      "loss": 0.2086,
      "step": 1690
    },
    {
      "epoch": 1.574074074074074,
      "grad_norm": 12.203898429870605,
      "learning_rate": 2.8102189781021898e-05,
      "loss": 0.1204,
      "step": 1700
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.15461215376853943,
      "learning_rate": 2.7919708029197085e-05,
      "loss": 0.2819,
      "step": 1710
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 0.20281949639320374,
      "learning_rate": 2.7737226277372262e-05,
      "loss": 0.2688,
      "step": 1720
    },
    {
      "epoch": 1.6018518518518519,
      "grad_norm": 0.7523446679115295,
      "learning_rate": 2.755474452554745e-05,
      "loss": 0.152,
      "step": 1730
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.45575398206710815,
      "learning_rate": 2.737226277372263e-05,
      "loss": 0.3631,
      "step": 1740
    },
    {
      "epoch": 1.6203703703703702,
      "grad_norm": 0.11509936302900314,
      "learning_rate": 2.7189781021897807e-05,
      "loss": 0.0726,
      "step": 1750
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 0.1863016039133072,
      "learning_rate": 2.7007299270072995e-05,
      "loss": 0.1441,
      "step": 1760
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 0.6519466042518616,
      "learning_rate": 2.6824817518248175e-05,
      "loss": 0.178,
      "step": 1770
    },
    {
      "epoch": 1.6481481481481481,
      "grad_norm": 0.21318283677101135,
      "learning_rate": 2.664233576642336e-05,
      "loss": 0.0075,
      "step": 1780
    },
    {
      "epoch": 1.6574074074074074,
      "grad_norm": 6.689384460449219,
      "learning_rate": 2.645985401459854e-05,
      "loss": 0.2407,
      "step": 1790
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.08237403631210327,
      "learning_rate": 2.6277372262773724e-05,
      "loss": 0.208,
      "step": 1800
    },
    {
      "epoch": 1.675925925925926,
      "grad_norm": 0.23222419619560242,
      "learning_rate": 2.6094890510948904e-05,
      "loss": 0.1832,
      "step": 1810
    },
    {
      "epoch": 1.6851851851851851,
      "grad_norm": 0.17340774834156036,
      "learning_rate": 2.591240875912409e-05,
      "loss": 0.0913,
      "step": 1820
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 0.33473050594329834,
      "learning_rate": 2.572992700729927e-05,
      "loss": 0.0704,
      "step": 1830
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 0.10370326042175293,
      "learning_rate": 2.5547445255474456e-05,
      "loss": 0.3138,
      "step": 1840
    },
    {
      "epoch": 1.7129629629629628,
      "grad_norm": 0.07681365311145782,
      "learning_rate": 2.5364963503649637e-05,
      "loss": 0.0793,
      "step": 1850
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.08348649740219116,
      "learning_rate": 2.518248175182482e-05,
      "loss": 0.1058,
      "step": 1860
    },
    {
      "epoch": 1.7314814814814814,
      "grad_norm": 0.06613292545080185,
      "learning_rate": 2.5e-05,
      "loss": 0.1069,
      "step": 1870
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 4.002739429473877,
      "learning_rate": 2.4817518248175185e-05,
      "loss": 0.3665,
      "step": 1880
    },
    {
      "epoch": 1.75,
      "grad_norm": 6.431093215942383,
      "learning_rate": 2.4635036496350366e-05,
      "loss": 0.2402,
      "step": 1890
    },
    {
      "epoch": 1.7592592592592593,
      "grad_norm": 117.00359344482422,
      "learning_rate": 2.445255474452555e-05,
      "loss": 0.2372,
      "step": 1900
    },
    {
      "epoch": 1.7685185185185186,
      "grad_norm": 38.083717346191406,
      "learning_rate": 2.4270072992700734e-05,
      "loss": 0.3725,
      "step": 1910
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.16948658227920532,
      "learning_rate": 2.4087591240875914e-05,
      "loss": 0.1542,
      "step": 1920
    },
    {
      "epoch": 1.7870370370370372,
      "grad_norm": 69.4719467163086,
      "learning_rate": 2.3905109489051098e-05,
      "loss": 0.2674,
      "step": 1930
    },
    {
      "epoch": 1.7962962962962963,
      "grad_norm": 0.27897435426712036,
      "learning_rate": 2.372262773722628e-05,
      "loss": 0.2343,
      "step": 1940
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 30.42363929748535,
      "learning_rate": 2.354014598540146e-05,
      "loss": 0.3602,
      "step": 1950
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 13.312868118286133,
      "learning_rate": 2.3357664233576643e-05,
      "loss": 0.2008,
      "step": 1960
    },
    {
      "epoch": 1.824074074074074,
      "grad_norm": 2.6034669876098633,
      "learning_rate": 2.3175182481751824e-05,
      "loss": 0.231,
      "step": 1970
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.24791917204856873,
      "learning_rate": 2.2992700729927008e-05,
      "loss": 0.1837,
      "step": 1980
    },
    {
      "epoch": 1.8425925925925926,
      "grad_norm": 0.2985565960407257,
      "learning_rate": 2.2810218978102192e-05,
      "loss": 0.0504,
      "step": 1990
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 13.517767906188965,
      "learning_rate": 2.2627737226277372e-05,
      "loss": 0.2627,
      "step": 2000
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 0.3465861976146698,
      "learning_rate": 2.2445255474452556e-05,
      "loss": 0.2976,
      "step": 2010
    },
    {
      "epoch": 1.8703703703703702,
      "grad_norm": 0.11328116059303284,
      "learning_rate": 2.226277372262774e-05,
      "loss": 0.0378,
      "step": 2020
    },
    {
      "epoch": 1.8796296296296298,
      "grad_norm": 4.661475658416748,
      "learning_rate": 2.208029197080292e-05,
      "loss": 0.1235,
      "step": 2030
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.20670129358768463,
      "learning_rate": 2.1897810218978105e-05,
      "loss": 0.2767,
      "step": 2040
    },
    {
      "epoch": 1.8981481481481481,
      "grad_norm": 14.377973556518555,
      "learning_rate": 2.1715328467153285e-05,
      "loss": 0.2178,
      "step": 2050
    },
    {
      "epoch": 1.9074074074074074,
      "grad_norm": 47.3428955078125,
      "learning_rate": 2.1532846715328466e-05,
      "loss": 0.2265,
      "step": 2060
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.24683994054794312,
      "learning_rate": 2.135036496350365e-05,
      "loss": 0.2148,
      "step": 2070
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 0.5210151076316833,
      "learning_rate": 2.1167883211678834e-05,
      "loss": 0.072,
      "step": 2080
    },
    {
      "epoch": 1.9351851851851851,
      "grad_norm": 39.94437789916992,
      "learning_rate": 2.0985401459854014e-05,
      "loss": 0.1482,
      "step": 2090
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.11659982055425644,
      "learning_rate": 2.08029197080292e-05,
      "loss": 0.2507,
      "step": 2100
    },
    {
      "epoch": 1.9537037037037037,
      "grad_norm": 7.330910682678223,
      "learning_rate": 2.062043795620438e-05,
      "loss": 0.3402,
      "step": 2110
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 1.8657934665679932,
      "learning_rate": 2.0437956204379563e-05,
      "loss": 0.3429,
      "step": 2120
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 0.13937658071517944,
      "learning_rate": 2.0255474452554747e-05,
      "loss": 0.0792,
      "step": 2130
    },
    {
      "epoch": 1.9814814814814814,
      "grad_norm": 0.14091216027736664,
      "learning_rate": 2.0072992700729927e-05,
      "loss": 0.3123,
      "step": 2140
    },
    {
      "epoch": 1.9907407407407407,
      "grad_norm": 6.653958320617676,
      "learning_rate": 1.989051094890511e-05,
      "loss": 0.2287,
      "step": 2150
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2732711434364319,
      "learning_rate": 1.9708029197080295e-05,
      "loss": 0.2182,
      "step": 2160
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.19622977077960968,
      "eval_runtime": 6.8075,
      "eval_samples_per_second": 141.02,
      "eval_steps_per_second": 8.814,
      "step": 2160
    }
  ],
  "logging_steps": 10,
  "max_steps": 3240,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1136508203627520.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
