{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "By December 15 you shall have implemented multiple baseline solutions to\n",
    "your main text classification task. These should include both deep learning (DL) based methods\n",
    "such as those introduced in Weeks 5-6 but also non-DL models such as those shown in Week 3.\n",
    "Baselines can also include simple rule-based methods (e.g. keyword matching or regular expres-\n",
    "sions). Each baseline should be evaluated both quantitatively and qualitatively, more details will\n",
    "be provided in the lecture on text classification (Week 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import conllu\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = 0_0\n",
      "# text = And this Conservative strategy has produced the angry and desperate wing-nuts like the fellow who called reporters 'lying pieces of Sh*t' this week.\n",
      "1\tAnd\tand\tCCONJ\tCC\t_\tNone\tNone\t_\t_\n",
      "2\tthis\tthis\tDET\tDT\tNumber=Sing|PronType=Dem\tNone\tNone\t_\t_\n",
      "3\tConservative\tConservative\tADJ\tJJ\tDegree=Pos\tNone\tNone\t_\t_\n",
      "4\tstrategy\tstrategy\tNOUN\tNN\tNumber=Sing\tNone\tNone\t_\t_\n",
      "5\thas\thave\tAUX\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\tNone\tNone\t_\t_\n",
      "6\tproduced\tproduce\tVERB\tVBN\tTense=Past|VerbForm=Part\tNone\tNone\t_\t_\n",
      "7\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\tNone\tNone\t_\t_\n",
      "8\tangry\tangry\tADJ\tJJ\tDegree=Pos\tNone\tNone\t_\t_\n"
     ]
    }
   ],
   "source": [
    "# Load the entire .conllu file into a Python variable\n",
    "conllu_data = []\n",
    "with open('Data/preprocessed_dataset.conllu', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        conllu_data.append(line.strip())\n",
    "\n",
    "# Display a sample of the loaded data (e.g., first 5 lines)\n",
    "print(\"\\n\".join(conllu_data[:10]))\n",
    "\n",
    "anns = pd.read_table('Data/annotations.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of sent_id : X_Y\n",
    "\n",
    "Where X is the id of the comment, while Y is the id of the sentence.\n",
    "\n",
    "Example : 2_3 means it's comment with id 2 and its 3rd sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First experiment: use of Naive Bayes classification (with original and pre-processed text)\n",
    "\n",
    "1. Extraction of the comment only\n",
    "2. Vectorization\n",
    "3. Splitting into training and testing data\n",
    "4. Training the model using different parameters\n",
    "5. Evaluating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_original_text(file_path):\n",
    "    \"\"\"\n",
    "    Load the .conllu file and group sentences by comment ID, returning a list of concatenated texts.\n",
    "    \"\"\"\n",
    "    comments_list = []\n",
    "    current_comment_id = None\n",
    "    current_text = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('# sent_id ='):\n",
    "                # Extract the comment ID (X) from 'sent_id = X_Y'\n",
    "                sent_id = line.split('=')[1].strip()\n",
    "                comment_id = sent_id.split('_')[0]\n",
    "\n",
    "                # Check if we've moved to a new comment\n",
    "                if current_comment_id is not None and comment_id != current_comment_id:\n",
    "                    # Store the completed text for the previous comment\n",
    "                    if current_text:\n",
    "                        comments_list.append(\" \".join(current_text))\n",
    "                    current_text = []\n",
    "\n",
    "                # Update the current comment ID\n",
    "                current_comment_id = comment_id\n",
    "\n",
    "            elif line.startswith('# text ='):\n",
    "                # Extract the text for the current sentence\n",
    "                sentence_text = line.split('=')[1].strip()\n",
    "                current_text.append(sentence_text)\n",
    "\n",
    "        # Add the last comment if any\n",
    "        if current_comment_id is not None and current_text:\n",
    "            comments_list.append(\" \".join(current_text))\n",
    "\n",
    "    return comments_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preprocessed_text(file_path):\n",
    "    \"\"\"\n",
    "    Load the .conllu file and group sentences by comment ID, returning a list of concatenated cleaned texts using lemmas.\n",
    "    \"\"\"\n",
    "    comments_list = []\n",
    "    current_comment_id = None\n",
    "    current_text = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('# sent_id ='):\n",
    "                # Extract the comment ID (X) from 'sent_id = X_Y'\n",
    "                sent_id = line.split('=')[1].strip()\n",
    "                comment_id = sent_id.split('_')[0]\n",
    "\n",
    "                # Check if we've moved to a new comment\n",
    "                if current_comment_id is not None and comment_id != current_comment_id:\n",
    "                    # Store the completed text for the previous comment\n",
    "                    if current_text:\n",
    "                        comments_list.append(\" \".join(current_text))\n",
    "                    current_text = []\n",
    "\n",
    "                current_comment_id = comment_id\n",
    "\n",
    "            elif not line.startswith('#') and line:\n",
    "                # Extract the lemma (3rd column)\n",
    "                columns = line.split('\\t')\n",
    "                if len(columns) > 2:\n",
    "                    lemma = columns[2].lower()  # Use the lemma column in lowercase\n",
    "                    current_text.append(lemma)\n",
    "\n",
    "        # Add the last comment if any\n",
    "        if current_comment_id is not None and current_text:\n",
    "            comments_list.append(\" \".join(current_text))\n",
    "\n",
    "    return comments_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extraction of the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"And this Conservative strategy has produced the angry and desperate wing-nuts like the fellow who called reporters 'lying pieces of Sh*t' this week. The fortunate thing is that reporters were able to report it and broadcast it - which may shake up a few folks who recognize a bit of themselves somewhere in there and do some reflecting. I live in hope.\",\n",
       " \"I commend Harper for holding the debates outside of a left-wing forum as this will help prevent the left from manipulating the debates to try to make Harper look bad. Indeed, we’ll finally have some fair debates. Trudeau is a coward and the only one who’s opposing this as he’s terrified about losing left-wing protection during the debates if the debates are held elsewhere. If Trudeau doesn’t have Chretien or Martin speaking for him or isn't currently in training to learn how to handle himself in a debate, he has May attending the debates to hold his little hand. If Trudeau can’t speak for himself or handle debates, how does he expect to run a country?\",\n",
       " \"What a joke Rachel Notley is. This is what was posted on the NDP website on the last World Press Freedom Day. She can't even follow her own leader. She should resign. Immediately. “Today, we pay tribute to journalists who have lost their lives or been injured in the line of duty. It is unacceptable that, in 2015, journalists are still prosecuted, detained and assassinated for doing their jobs. Independent media is of immeasurable value in a free and democratic society, and it is our duty to ensure that this continues to be respected and protected. “We also take this opportunity to stand in solidarity with journalists and Canadians who have been affected by the Conservative’s reckless cuts to CBC/ Radio-Canada. “When we attack the freedom of the press, we’re attacking our own people, democracy and freedom. “New Democrats thank journalists and the media for their invaluable work informing the public. Let’s work together to defend freedom of the press. ” http://www.ndp.ca/news/statement-ndp-world-press-freedom-day\"]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Data/preprocessed_dataset.conllu'\n",
    "original_list = extract_original_text(file_path)\n",
    "original_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"and this conservative strategy have produce the angry and desperate wing - nut like the fellow who call reporter 's lying piece of sh*tember ' this week . the fortunate thing be that reporter be able to report it and broadcast it - which may shake up a few folk who recognize a bit of themselves somewhere in there and do some reflect . i live in hope .\",\n",
       " 'i commend harper for hold the debate outside of a left - wing forum as this will help prevent the left from manipulate the debate to try to make harper look bad . indeed , we will finally have some fair debate . trudeau be a coward and the only one who be oppose this as he be terrified about lose left - wing protection during the debate if the debate be hold elsewhere . if trudeau do not have chretien or martin speak for he or be not currently in training to learn how to handle himself in a debate , he have may attend the debate to hold his little hand . if trudeau can not speak for himself or handle debate , how do he expect to run a country ?',\n",
       " \"what a joke rachel notley be . this be what be post on the ndp website on the last world press freedom day . she can not even follow her own leader . she should resign . immediately . '' today , we pay tribute to journalist who have lose their life or be injure in the line of duty . it be unacceptable that , in 2015 , journalist be still prosecute , detain and assassinate for do their job . independent media be of immeasurable value in a free and democratic society , and it be our duty to ensure that this continue to be respect and protect . '' we also take this opportunity to stand in solidarity with journalist and canadian who have be affect by the conservative 's reckless cut to cbc / radio - canada . '' when we attack the freedom of the press , we be attack our own person , democracy and freedom . '' new democrat thank journalist and the media for their invaluable work inform the public . let we work together to defend freedom of the press . '' http://www.ndp.ca/news/statement-ndp-world-press-freedom-day\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_list = extract_preprocessed_text(file_path)\n",
    "preprocessed_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n"
     ]
    }
   ],
   "source": [
    "print(len(original_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(anns) == len(original_list), \"Annotations and original texts are misaligned!\"\n",
    "assert len(anns) == len(preprocessed_list), \"Annotations and preprocessed texts are misaligned!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "Harper did do one class act... He gave jack Layton a state funeral, whether deserved or not. He should have praised Flora MacDonald and attended her funeral. Some of his beliefs are, I think, a reaction to the Liberal Party arrogance that they are the sole arbiters of what Canadian values are and what our country stands for in the world. This time , I will not be giving the Conservatives my vote but don't know who will.\n",
      "Unnamed: 0                                                           106\n",
      "comment_text           Harper did do one class act... He gave jack La...\n",
      "constructive_binary                                                    1\n",
      "text_length                                                          423\n",
      "avg_word_length                                                 4.435897\n",
      "Name: 106, dtype: object\n",
      "0    1\n",
      "Name: 106, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_nr = random.randrange(0, 11999)\n",
    "print(random_nr)\n",
    "\n",
    "df_anno = pd.read_csv(\"./Data/compare_file.csv\")\n",
    "\n",
    "print(original_list[random_nr])\n",
    "\n",
    "print(df_anno.loc[random_nr])\n",
    "\n",
    "print(anns.loc[random_nr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For original text\n",
    "\n",
    "2. Vectorization (using TF-IDF)\n",
    "3. Splitting into training and testing data\n",
    "4. Training the Naive Bayes classifier\n",
    "5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Best cross-validation accuracy: 0.6761\n",
      "Accuracy: 0.6875\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.48      0.58      1086\n",
      "           1       0.67      0.86      0.75      1314\n",
      "\n",
      "    accuracy                           0.69      2400\n",
      "   macro avg       0.70      0.67      0.67      2400\n",
      "weighted avg       0.70      0.69      0.67      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(original_list)\n",
    "y = anns\n",
    "\n",
    "#3 Splitting into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#4 Training the Naive Bayes classifier\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}  # Adjust the range as needed\n",
    "model = MultinomialNB()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "#5 Evaluate the model\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Best cross-validation accuracy: 0.6760\n",
      "Accuracy: 0.6808333333333333\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.47      0.57      1086\n",
      "           1       0.66      0.85      0.75      1314\n",
      "\n",
      "    accuracy                           0.68      2400\n",
      "   macro avg       0.69      0.66      0.66      2400\n",
      "weighted avg       0.69      0.68      0.67      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_list)\n",
    "y = anns\n",
    "\n",
    "#3 Splitting into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#4 Training the Naive Bayes classifier\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}  # Adjust the range as needed\n",
    "model = MultinomialNB()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "#5 Evaluate the model\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second experiment: Use Feature based models \n",
    "\n",
    "Idea: Use the features from the CONLL-U format to feed Machine Learning models (SVM, Random Forest, KNN and else) in order to classify the text\n",
    "\n",
    "1. Extraction of the features\n",
    "2. Splitting into training and testing data\n",
    "3. Training the models using different parameters\n",
    "4. Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_data(file_path):\n",
    "    comments = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        current_comment_id = None\n",
    "        pos_counts = Counter()\n",
    "        num_tokens = 0\n",
    "        total_word_length = 0\n",
    "        num_sentences = 0\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check if the line is a sentence ID line\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                sent_id = line.split(\"= \")[1]\n",
    "                current_comment_id = sent_id.split('_')[0]\n",
    "                \n",
    "                # Initialize a new comment entry if not already present\n",
    "                if current_comment_id not in comments:\n",
    "                    comments[current_comment_id] = {\n",
    "                        'num_tokens': 0,\n",
    "                        'total_word_length': 0,\n",
    "                        'pos_counts': Counter(),\n",
    "                        'num_sentences': 0\n",
    "                    }\n",
    "                    pos_counts = Counter()\n",
    "                    num_tokens = 0\n",
    "                    total_word_length = 0\n",
    "                    num_sentences = 0\n",
    "\n",
    "            # Check if the line is a text line\n",
    "            elif line.startswith(\"# text\"):\n",
    "                text = line.split(\"= \")[1]\n",
    "\n",
    "            # Process token lines\n",
    "            elif line and not line.startswith(\"#\"):\n",
    "                columns = line.split(\"\\t\")\n",
    "                if len(columns) >= 4:\n",
    "                    token = columns[1]\n",
    "                    pos_tag = columns[3]\n",
    "                    \n",
    "                    # Extract token-level features\n",
    "                    word_length = len(token)\n",
    "                    total_word_length += word_length\n",
    "                    num_tokens += 1\n",
    "                    pos_counts[pos_tag] += 1\n",
    "            \n",
    "            # End of a sentence block\n",
    "            if line == \"\" and current_comment_id is not None:\n",
    "                comments[current_comment_id]['num_tokens'] += num_tokens\n",
    "                comments[current_comment_id]['total_word_length'] += total_word_length\n",
    "                comments[current_comment_id]['pos_counts'].update(pos_counts)\n",
    "                comments[current_comment_id]['num_sentences'] += 1\n",
    "\n",
    "    # Convert the aggregated features to a DataFrame\n",
    "    features = []\n",
    "    for comment_id, data in comments.items():\n",
    "        pos_counts = data['pos_counts']\n",
    "        avg_word_length = data['total_word_length'] / data['num_tokens'] if data['num_tokens'] > 0 else 0\n",
    "        \n",
    "        feature_dict = {\n",
    "            'comment_id': comment_id,\n",
    "            'num_tokens': data['num_tokens'],\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'num_sentences': data['num_sentences'],\n",
    "            'num_nouns': pos_counts.get('NOUN', 0),\n",
    "            'num_verbs': pos_counts.get('VERB', 0),\n",
    "            'num_adjectives': pos_counts.get('ADJ', 0),\n",
    "            'num_adverbs': pos_counts.get('ADV', 0),\n",
    "            'num_pronouns': pos_counts.get('PRON', 0),\n",
    "            'num_conjunctions': pos_counts.get('CCONJ', 0),\n",
    "            'num_determiners': pos_counts.get('DET', 0),\n",
    "        }\n",
    "        features.append(feature_dict)\n",
    "    \n",
    "    df = pd.DataFrame(features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extraction of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Data/preprocessed_dataset.conllu'\n",
    "features_conllu = load_conllu_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Splitting into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_conllu.drop(['comment_id'], axis=1)\n",
    "y = anns \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training the models using different parameters\n",
    "4. Evaluating the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training knn...\n",
      "Best Parameters for knn: {'metric': 'manhattan', 'n_neighbors': 50, 'weights': 'uniform'}\n",
      "\n",
      "knn Accuracy: 0.9075\n",
      "\n",
      "knn Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      1086\n",
      "           1       0.91      0.92      0.92      1314\n",
      "\n",
      "    accuracy                           0.91      2400\n",
      "   macro avg       0.91      0.91      0.91      2400\n",
      "weighted avg       0.91      0.91      0.91      2400\n",
      "\n",
      "\n",
      "Training rf...\n",
      "Best Parameters for rf: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "\n",
      "rf Accuracy: 0.9233333333333333\n",
      "\n",
      "rf Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92      1086\n",
      "           1       0.93      0.93      0.93      1314\n",
      "\n",
      "    accuracy                           0.92      2400\n",
      "   macro avg       0.92      0.92      0.92      2400\n",
      "weighted avg       0.92      0.92      0.92      2400\n",
      "\n",
      "\n",
      "Training logisticregression...\n",
      "Best Parameters for logisticregression: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "logisticregression Accuracy: 0.9104166666666667\n",
      "\n",
      "logisticregression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90      1086\n",
      "           1       0.95      0.89      0.92      1314\n",
      "\n",
      "    accuracy                           0.91      2400\n",
      "   macro avg       0.91      0.91      0.91      2400\n",
      "weighted avg       0.91      0.91      0.91      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grids\n",
    "param_grids = {\n",
    "    'knn': {\n",
    "        'n_neighbors': [3, 5, 10, 15,20,50,100],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'rf': {\n",
    "        'n_estimators': [10,25, 50, 100, 150],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'logisticregression': {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and tune models\n",
    "models = {\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'rf': RandomForestClassifier(random_state=42),\n",
    "    'logisticregression': LogisticRegression()\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name.lower()], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[model_name] = best_model\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(f\"\\n{model_name} Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(f\"\\n{model_name} Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Comparison\n",
    "\n",
    "| Classifier              | Accuracy | Precision (0) | Recall (0) | F1-score (0) | Precision (1) | Recall (1) | F1-score (1) | Macro Avg Precision | Macro Avg Recall | Macro Avg F1-score |\n",
    "|-------------------------|----------|----------------|-------------|---------------|----------------|-------------|---------------|---------------------|-------------------|---------------------|\n",
    "| **Naive Bayes on original data**         | 0.6875   | 0.74           | 0.48        | 0.58          | 0.67           | 0.86        | 0.75          | 0.70                | 0.67              | 0.67                |\n",
    "| **Naive Bayes on preprocessed data**         | 0.6808   | 0.73           | 0.47        | 0.57         | 0.66           | 0.85        | 0.75          | 0.69                | 0.66              | 0.66                |\n",
    "| **K-Nearest Neighbors** | 0.9075   | 0.90           | 0.89        | 0.90          | 0.91           | 0.92        | 0.92          | 0.91                | 0.91              | 0.91                |\n",
    "| **Random Forest**       | 0.9233   | 0.91           | 0.92        | 0.92          | 0.93           | 0.93        | 0.93          | 0.92                | 0.92              | 0.92                |\n",
    "| **Logistic Regression** | 0.91     | 0.87           | 0.94        | 0.90          | 0.95           | 0.89        | 0.92          | 0.91                | 0.91              | 0.91                |\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of the Columns:\n",
    "- **Accuracy**: Overall accuracy of the model.\n",
    "- **Precision (0)**, **Recall (0)**, **F1-score (0)**: Metrics for class **0**.\n",
    "- **Precision (1)**, **Recall (1)**, **F1-score (1)**: Metrics for class **1**.\n",
    "- **Macro Avg Precision**, **Macro Avg Recall**, **Macro Avg F1-score**: Averages of precision, recall, and F1-score for both classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another interesting question is where do these model fails to correctly classify the texts, let's print some example from the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples:\n",
      "\n",
      "Category 'constructive' misclassified as being the opposite:\n",
      "\n",
      "- Trump is irrelevant. The US citizenry needs to clean out the corrupt government as required every several generations. It would have been Perot or Buchanan at another time. Get rid of Trump, and there will be a different name with the same agenda. And the media and establishment will oppose him or her too.\n",
      "- Until the Globe editorial board can explain why they endorsed the Harper government time and time again and at the same they are reporting on the corruption and lies and scams. These editorials just look foolish to me. Now their a problem with the Harper government?\n",
      "- '........... that small symbolic acts have great power, ..........'. Yeah, symbology like the fact that some of the strongest proponents for massive lifestyle change just happen to be amongst the biggest carbon gluttons on the planet. You listening Dave?\n",
      "\n",
      "Category 'not constructive' misclassified as being the opposite:\n",
      "\n",
      "- Margaret Wente is such an elitist snob at times. Judging from some of her columns I find her attitude that of a typical white-collar, middle to upper class, 'I went to university so I'm better than you are' kind. Some of the posters on here tonight are not much better.\n",
      "- Good column. To which I might add that although one can certainly wonder about the effectiveness of an air war by itself, it is perfectly clear that Kurdish and other ground forces will not be successful against ISIS without air support and that only western powers can supply that air support.\n",
      "- When my wife and I became citizens, in 1962, we both swore an oath as required, but in those days there was no US-inspired ceremony with Mounties and a politically-appointed citizenship judge - simply an oath taken before a commissioner for oaths.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retain 'comment_id' for reference\n",
    "X = features_conllu.drop(['comment_id'], axis=1)\n",
    "comment_ids = features_conllu['comment_id']\n",
    "\n",
    "# Split data, retaining comment IDs in the test set\n",
    "X_train, X_test, y_train, y_test, comment_ids_train, comment_ids_test = train_test_split(\n",
    "    X, y, comment_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Retrain the best RF model with specified parameters\n",
    "best_rf = RandomForestClassifier(max_depth=10, min_samples_split=2, n_estimators=50, random_state=42)\n",
    "best_rf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_test_array = y_test.values.ravel()\n",
    "# Identify misclassified indices\n",
    "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test_array, y_pred)) if true != pred]\n",
    "\n",
    "# Group misclassified samples by true class\n",
    "misclassified_by_category = {0: [], 1: []}\n",
    "\n",
    "for idx in misclassified_indices:\n",
    "    true_class = y_test_array[idx]\n",
    "    if true_class in misclassified_by_category:\n",
    "        misclassified_by_category[true_class].append(comment_ids_test.iloc[idx])\n",
    "\n",
    "# Print up to 3 misclassified comment IDs for each category\n",
    "print(\"Misclassified examples:\\n\")\n",
    "for category, comment_ids in misclassified_by_category.items():\n",
    "    category_name = \"constructive\" if category == 0 else \"not constructive\"\n",
    "    print(f\"Category '{category_name}' misclassified as being the opposite:\\n\")\n",
    "    for i, comment_id in enumerate(comment_ids[:3]):\n",
    "        print(f\"- {original_list[int(comment_id)]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Onto Deep Learning solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataser preprocessed\n",
    "X = vectorizer.fit_transform(preprocessed_list)\n",
    "y = anns\n",
    "\n",
    "# Training data for Deep Learning\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saitohiraga/Documents/GitHub/NLP-DetecConstComm-project/venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train.shape[1]), # Input layer\n",
    "    Dropout(0.2),  # Dropout to avoid overfitting\n",
    "    Dense(64, activation='relu'),  # Intermediate layer\n",
    "    Dropout(0.2),  # Dropout to avoid overfitting\n",
    "    Dense(32, activation='relu'),  # Intermediate layer\n",
    "    Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7007 - loss: 0.5128 - precision: 0.6740 - recall: 0.9226 - val_accuracy: 0.8661 - val_loss: 0.3013 - val_precision: 0.8942 - val_recall: 0.8537\n",
      "Epoch 2/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9620 - loss: 0.1122 - precision: 0.9677 - recall: 0.9616 - val_accuracy: 0.8562 - val_loss: 0.3519 - val_precision: 0.8523 - val_recall: 0.8884\n",
      "Epoch 3/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9895 - loss: 0.0365 - precision: 0.9937 - recall: 0.9871 - val_accuracy: 0.8578 - val_loss: 0.5207 - val_precision: 0.8383 - val_recall: 0.9134\n",
      "Epoch 4/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9984 - loss: 0.0066 - precision: 0.9994 - recall: 0.9976 - val_accuracy: 0.8510 - val_loss: 0.6279 - val_precision: 0.8359 - val_recall: 0.9018\n",
      "Epoch 5/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9995 - loss: 0.0020 - precision: 0.9997 - recall: 0.9995 - val_accuracy: 0.8557 - val_loss: 0.7107 - val_precision: 0.8521 - val_recall: 0.8874\n",
      "Epoch 6/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 5.6068e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8562 - val_loss: 0.7749 - val_precision: 0.8589 - val_recall: 0.8787\n",
      "Epoch 7/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.3795e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8510 - val_loss: 0.8325 - val_precision: 0.8420 - val_recall: 0.8922\n",
      "Epoch 8/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.1672e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8552 - val_loss: 0.8675 - val_precision: 0.8533 - val_recall: 0.8845\n",
      "Epoch 9/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.3165e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8536 - val_loss: 0.8758 - val_precision: 0.8637 - val_recall: 0.8662\n",
      "Epoch 10/10\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0011 - precision: 0.9996 - recall: 0.9998 - val_accuracy: 0.8500 - val_loss: 0.8696 - val_precision: 0.8566 - val_recall: 0.8681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16e54b310>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8415 - loss: 0.8937 - precision: 0.8629 - recall: 0.8496\n",
      "Loss: 0.9024\n",
      "Accuracy: 0.8446\n",
      "Precision: 0.8673\n",
      "Recall: 0.8455\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, precision, recall = nn_model.evaluate(X_test, y_test)\n",
    "\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split \u001b[38;5;28;01mas\u001b[39;00m split\n\u001b[0;32m----> 3\u001b[0m df_train, df_test \u001b[38;5;241m=\u001b[39m train_test_split(original_list, anns, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 8639, Validation size: 960, Test size: 2400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: Divide the original data into training+validation and test sets\n",
    "X_train_val_bert, X_test_bert, y_train_val_bert, y_test_bert = train_test_split(\n",
    "    original_list, anns, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Divide the training+validation set into training and validation sets\n",
    "X_train_bert, X_val_bert, y_train_bert, y_val_bert = train_test_split(\n",
    "    X_train_val_bert, y_train_val_bert, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Training size: {len(X_train_bert)}, Validation size: {len(X_val_bert)}, Test size: {len(X_test_bert)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train_bert = torch.tensor(y_train_bert.values, dtype=torch.long)\n",
    "y_val_bert = torch.tensor(y_val_bert.values, dtype=torch.long)\n",
    "y_test_bert = torch.tensor(y_test_bert.values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike traditional NLP pipelines, BERT's tokenizer requires no additional preprocessing:\n",
    "\n",
    "You don’t need to remove stopwords because BERT can handle their contextual importance.\n",
    "You don’t need to lemmatize or stem because subwords inherently capture morphological variations (e.g., \"run\" and \"running\").\n",
    "Case sensitivity is handled if you're using the case-sensitive version of BERT.\n",
    "This makes the process simpler and ensures that the model works with raw input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_texts(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train_bert, tokenizer)\n",
    "val_encodings = tokenize_texts(X_val_bert, tokenizer)\n",
    "test_encodings = tokenize_texts(X_test_bert, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification, TrainingArguments, RobertaForSequenceClassification, Trainer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CommentDataset(train_encodings, y_train_bert)\n",
    "val_dataset = CommentDataset(val_encodings, y_val_bert)\n",
    "test_dataset = CommentDataset(test_encodings, y_test_bert)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerhardkarbeutz/NLP-DetecConstComm-project/venv/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4185c2752d04828972274a2d1d195f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7277, 'grad_norm': 4.439280033111572, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.7228, 'grad_norm': 8.950873374938965, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6769, 'grad_norm': 3.9286797046661377, 'learning_rate': 3e-06, 'epoch': 0.03}\n",
      "{'loss': 0.6457, 'grad_norm': 8.793628692626953, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 0.5729, 'grad_norm': 5.118696689605713, 'learning_rate': 5e-06, 'epoch': 0.05}\n",
      "{'loss': 0.521, 'grad_norm': 9.963205337524414, 'learning_rate': 6e-06, 'epoch': 0.06}\n",
      "{'loss': 0.5126, 'grad_norm': 7.262749671936035, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 0.4464, 'grad_norm': 18.62522315979004, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 0.4239, 'grad_norm': 7.297521591186523, 'learning_rate': 9e-06, 'epoch': 0.08}\n",
      "{'loss': 0.3768, 'grad_norm': 9.70535659790039, 'learning_rate': 1e-05, 'epoch': 0.09}\n",
      "{'loss': 0.3766, 'grad_norm': 8.339592933654785, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3453, 'grad_norm': 8.104011535644531, 'learning_rate': 1.2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.3225, 'grad_norm': 3.6527509689331055, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2968, 'grad_norm': 3.9925076961517334, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2846, 'grad_norm': 14.856379508972168, 'learning_rate': 1.5e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2059, 'grad_norm': 26.045665740966797, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 0.182, 'grad_norm': 7.1177496910095215, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3152, 'grad_norm': 13.809427261352539, 'learning_rate': 1.8e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1773, 'grad_norm': 13.220126152038574, 'learning_rate': 1.9e-05, 'epoch': 0.18}\n",
      "{'loss': 0.279, 'grad_norm': 1.443801760673523, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1139, 'grad_norm': 34.226741790771484, 'learning_rate': 2.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3312, 'grad_norm': 11.968201637268066, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1139, 'grad_norm': 0.835019588470459, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4039, 'grad_norm': 0.294230580329895, 'learning_rate': 2.4e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0633, 'grad_norm': 2.2741663455963135, 'learning_rate': 2.5e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2107, 'grad_norm': 0.23479712009429932, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3893, 'grad_norm': 0.5181750059127808, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 0.315, 'grad_norm': 13.907724380493164, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1825, 'grad_norm': 3.4592015743255615, 'learning_rate': 2.9e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3639, 'grad_norm': 6.731849193572998, 'learning_rate': 3e-05, 'epoch': 0.28}\n",
      "{'loss': 0.2353, 'grad_norm': 0.5060593485832214, 'learning_rate': 3.1e-05, 'epoch': 0.29}\n",
      "{'loss': 0.362, 'grad_norm': 16.037940979003906, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3276, 'grad_norm': 21.230981826782227, 'learning_rate': 3.3e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5283, 'grad_norm': 1.3190861940383911, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2121, 'grad_norm': 44.62590789794922, 'learning_rate': 3.5e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2728, 'grad_norm': 0.5778866410255432, 'learning_rate': 3.6e-05, 'epoch': 0.33}\n",
      "{'loss': 0.451, 'grad_norm': 25.428504943847656, 'learning_rate': 3.7e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3977, 'grad_norm': 5.642666816711426, 'learning_rate': 3.8e-05, 'epoch': 0.35}\n",
      "{'loss': 0.4699, 'grad_norm': 0.845439076423645, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4322, 'grad_norm': 5.4814677238464355, 'learning_rate': 4e-05, 'epoch': 0.37}\n",
      "{'loss': 0.196, 'grad_norm': 4.990299224853516, 'learning_rate': 4.1e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1486, 'grad_norm': 0.4359472990036011, 'learning_rate': 4.2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.368, 'grad_norm': 12.695402145385742, 'learning_rate': 4.3e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4757, 'grad_norm': 17.017972946166992, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2318, 'grad_norm': 0.9060226678848267, 'learning_rate': 4.5e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3045, 'grad_norm': 32.93205261230469, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.43}\n",
      "{'loss': 0.089, 'grad_norm': 0.1469859927892685, 'learning_rate': 4.7e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2796, 'grad_norm': 0.09950914233922958, 'learning_rate': 4.8e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1928, 'grad_norm': 0.2805453836917877, 'learning_rate': 4.9e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4741, 'grad_norm': 16.2227840423584, 'learning_rate': 5e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3374, 'grad_norm': 0.7126441597938538, 'learning_rate': 4.981751824817518e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2822, 'grad_norm': 0.6404715776443481, 'learning_rate': 4.963503649635037e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1263, 'grad_norm': 1.6676695346832275, 'learning_rate': 4.945255474452555e-05, 'epoch': 0.49}\n",
      "{'loss': 0.1778, 'grad_norm': 4.41133975982666, 'learning_rate': 4.927007299270073e-05, 'epoch': 0.5}\n",
      "{'loss': 0.26, 'grad_norm': 0.1842283457517624, 'learning_rate': 4.908759124087591e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2939, 'grad_norm': 0.1986425369977951, 'learning_rate': 4.89051094890511e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2802, 'grad_norm': 0.8916694521903992, 'learning_rate': 4.872262773722628e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3836, 'grad_norm': 23.042749404907227, 'learning_rate': 4.854014598540147e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5146, 'grad_norm': 12.652247428894043, 'learning_rate': 4.835766423357664e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2038, 'grad_norm': 1.8540802001953125, 'learning_rate': 4.817518248175183e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3813, 'grad_norm': 5.484358787536621, 'learning_rate': 4.799270072992701e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1666, 'grad_norm': 0.2810669243335724, 'learning_rate': 4.7810218978102196e-05, 'epoch': 0.57}\n",
      "{'loss': 0.178, 'grad_norm': 0.2794305682182312, 'learning_rate': 4.762773722627738e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3932, 'grad_norm': 8.35552978515625, 'learning_rate': 4.744525547445256e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2676, 'grad_norm': 1.274025321006775, 'learning_rate': 4.726277372262774e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0823, 'grad_norm': 0.08667869865894318, 'learning_rate': 4.708029197080292e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2274, 'grad_norm': 33.97312927246094, 'learning_rate': 4.6897810218978106e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4919, 'grad_norm': 1.1098508834838867, 'learning_rate': 4.6715328467153287e-05, 'epoch': 0.63}\n",
      "{'loss': 0.454, 'grad_norm': 4.724392414093018, 'learning_rate': 4.6532846715328474e-05, 'epoch': 0.64}\n",
      "{'loss': 0.159, 'grad_norm': 4.841236591339111, 'learning_rate': 4.635036496350365e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3592, 'grad_norm': 0.6893365383148193, 'learning_rate': 4.6167883211678835e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3921, 'grad_norm': 7.267207622528076, 'learning_rate': 4.5985401459854016e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2319, 'grad_norm': 0.6260819435119629, 'learning_rate': 4.58029197080292e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2954, 'grad_norm': 6.07451057434082, 'learning_rate': 4.5620437956204383e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2651, 'grad_norm': 0.40631103515625, 'learning_rate': 4.5437956204379564e-05, 'epoch': 0.69}\n",
      "{'loss': 0.264, 'grad_norm': 0.30304569005966187, 'learning_rate': 4.5255474452554745e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1915, 'grad_norm': 7.231122016906738, 'learning_rate': 4.5072992700729925e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1831, 'grad_norm': 0.2893381118774414, 'learning_rate': 4.489051094890511e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2437, 'grad_norm': 10.34627914428711, 'learning_rate': 4.470802919708029e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3024, 'grad_norm': 4.937499046325684, 'learning_rate': 4.452554744525548e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2544, 'grad_norm': 6.9688191413879395, 'learning_rate': 4.434306569343066e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3285, 'grad_norm': 0.4539767801761627, 'learning_rate': 4.416058394160584e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2803, 'grad_norm': 0.8286867737770081, 'learning_rate': 4.397810218978102e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3551, 'grad_norm': 7.3748087882995605, 'learning_rate': 4.379562043795621e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2191, 'grad_norm': 2.4606430530548096, 'learning_rate': 4.361313868613139e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1092, 'grad_norm': 4.6889848709106445, 'learning_rate': 4.343065693430657e-05, 'epoch': 0.8}\n",
      "{'loss': 0.214, 'grad_norm': 0.4675692915916443, 'learning_rate': 4.324817518248175e-05, 'epoch': 0.81}\n",
      "{'loss': 0.255, 'grad_norm': 16.90774154663086, 'learning_rate': 4.306569343065693e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2293, 'grad_norm': 0.7255963087081909, 'learning_rate': 4.288321167883212e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2189, 'grad_norm': 0.1807517558336258, 'learning_rate': 4.27007299270073e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2353, 'grad_norm': 0.17579303681850433, 'learning_rate': 4.251824817518249e-05, 'epoch': 0.84}\n",
      "{'loss': 0.2998, 'grad_norm': 0.48084840178489685, 'learning_rate': 4.233576642335767e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1686, 'grad_norm': 4.438077449798584, 'learning_rate': 4.215328467153285e-05, 'epoch': 0.86}\n",
      "{'loss': 0.1852, 'grad_norm': 0.4420095384120941, 'learning_rate': 4.197080291970803e-05, 'epoch': 0.87}\n",
      "{'loss': 0.2445, 'grad_norm': 48.188045501708984, 'learning_rate': 4.1788321167883216e-05, 'epoch': 0.88}\n",
      "{'loss': 0.244, 'grad_norm': 0.7552152872085571, 'learning_rate': 4.16058394160584e-05, 'epoch': 0.89}\n",
      "{'loss': 0.268, 'grad_norm': 3.6436641216278076, 'learning_rate': 4.1423357664233584e-05, 'epoch': 0.9}\n",
      "{'loss': 0.2608, 'grad_norm': 0.30063924193382263, 'learning_rate': 4.124087591240876e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2863, 'grad_norm': 0.3663531541824341, 'learning_rate': 4.1058394160583945e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2105, 'grad_norm': 7.820816993713379, 'learning_rate': 4.0875912408759126e-05, 'epoch': 0.93}\n",
      "{'loss': 0.2019, 'grad_norm': 12.176673889160156, 'learning_rate': 4.0693430656934306e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3357, 'grad_norm': 10.591497421264648, 'learning_rate': 4.0510948905109494e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2952, 'grad_norm': 0.48256218433380127, 'learning_rate': 4.0328467153284674e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3076, 'grad_norm': 1.1705836057662964, 'learning_rate': 4.0145985401459855e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1307, 'grad_norm': 10.397361755371094, 'learning_rate': 3.9963503649635035e-05, 'epoch': 0.97}\n",
      "{'loss': 0.2612, 'grad_norm': 4.195181846618652, 'learning_rate': 3.978102189781022e-05, 'epoch': 0.98}\n",
      "{'loss': 0.4037, 'grad_norm': 0.3822416067123413, 'learning_rate': 3.95985401459854e-05, 'epoch': 0.99}\n",
      "{'loss': 0.1608, 'grad_norm': 76.28103637695312, 'learning_rate': 3.941605839416059e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5de2d8963d49a0a37d7215a1a91768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23229672014713287, 'eval_runtime': 7.0351, 'eval_samples_per_second': 136.459, 'eval_steps_per_second': 8.529, 'epoch': 1.0}\n",
      "{'loss': 0.2136, 'grad_norm': 0.5451074242591858, 'learning_rate': 3.9233576642335764e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3003, 'grad_norm': 70.72721862792969, 'learning_rate': 3.905109489051095e-05, 'epoch': 1.02}\n",
      "{'loss': 0.4374, 'grad_norm': 15.410810470581055, 'learning_rate': 3.886861313868613e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3728, 'grad_norm': 9.219738960266113, 'learning_rate': 3.868613138686132e-05, 'epoch': 1.04}\n",
      "{'loss': 0.2057, 'grad_norm': 4.477297306060791, 'learning_rate': 3.85036496350365e-05, 'epoch': 1.05}\n",
      "{'loss': 0.34, 'grad_norm': 1.0985395908355713, 'learning_rate': 3.832116788321168e-05, 'epoch': 1.06}\n",
      "{'loss': 0.1282, 'grad_norm': 0.7224826216697693, 'learning_rate': 3.813868613138686e-05, 'epoch': 1.06}\n",
      "{'loss': 0.1826, 'grad_norm': 0.3357544541358948, 'learning_rate': 3.795620437956204e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2714, 'grad_norm': 5.047911643981934, 'learning_rate': 3.777372262773723e-05, 'epoch': 1.08}\n",
      "{'loss': 0.1997, 'grad_norm': 0.5495461821556091, 'learning_rate': 3.759124087591241e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3091, 'grad_norm': 10.729269981384277, 'learning_rate': 3.74087591240876e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0942, 'grad_norm': 0.12493829429149628, 'learning_rate': 3.722627737226278e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2733, 'grad_norm': 0.14373348653316498, 'learning_rate': 3.704379562043796e-05, 'epoch': 1.12}\n",
      "{'loss': 0.1632, 'grad_norm': 7.311362266540527, 'learning_rate': 3.686131386861314e-05, 'epoch': 1.13}\n",
      "{'loss': 0.6002, 'grad_norm': 8.133875846862793, 'learning_rate': 3.6678832116788326e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2669, 'grad_norm': 0.5473870635032654, 'learning_rate': 3.649635036496351e-05, 'epoch': 1.15}\n",
      "{'loss': 0.2427, 'grad_norm': 10.900237083435059, 'learning_rate': 3.631386861313869e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2181, 'grad_norm': 0.6567394733428955, 'learning_rate': 3.613138686131387e-05, 'epoch': 1.17}\n",
      "{'loss': 0.2947, 'grad_norm': 0.9485209584236145, 'learning_rate': 3.594890510948905e-05, 'epoch': 1.18}\n",
      "{'loss': 0.2967, 'grad_norm': 3.9583888053894043, 'learning_rate': 3.5766423357664236e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2153, 'grad_norm': 0.8656381964683533, 'learning_rate': 3.5583941605839416e-05, 'epoch': 1.19}\n",
      "{'loss': 0.3087, 'grad_norm': 0.5189133286476135, 'learning_rate': 3.5401459854014604e-05, 'epoch': 1.2}\n",
      "{'loss': 0.1915, 'grad_norm': 0.31142711639404297, 'learning_rate': 3.5218978102189784e-05, 'epoch': 1.21}\n",
      "{'loss': 0.166, 'grad_norm': 0.2655770182609558, 'learning_rate': 3.5036496350364965e-05, 'epoch': 1.22}\n",
      "{'loss': 0.1113, 'grad_norm': 0.2360234558582306, 'learning_rate': 3.4854014598540145e-05, 'epoch': 1.23}\n",
      "{'loss': 0.2323, 'grad_norm': 2.618722438812256, 'learning_rate': 3.467153284671533e-05, 'epoch': 1.24}\n",
      "{'loss': 0.2922, 'grad_norm': 0.6978002190589905, 'learning_rate': 3.448905109489051e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2527, 'grad_norm': 12.812004089355469, 'learning_rate': 3.43065693430657e-05, 'epoch': 1.26}\n",
      "{'loss': 0.2072, 'grad_norm': 0.287331759929657, 'learning_rate': 3.4124087591240875e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3383, 'grad_norm': 0.8153550028800964, 'learning_rate': 3.3941605839416055e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1344, 'grad_norm': 0.49449458718299866, 'learning_rate': 3.375912408759124e-05, 'epoch': 1.29}\n",
      "{'loss': 0.4218, 'grad_norm': 45.83900451660156, 'learning_rate': 3.357664233576642e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0696, 'grad_norm': 0.1926574558019638, 'learning_rate': 3.339416058394161e-05, 'epoch': 1.31}\n",
      "{'loss': 0.4258, 'grad_norm': 0.22313913702964783, 'learning_rate': 3.321167883211679e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2013, 'grad_norm': 4.107224464416504, 'learning_rate': 3.302919708029197e-05, 'epoch': 1.32}\n",
      "{'loss': 0.1957, 'grad_norm': 6.190626621246338, 'learning_rate': 3.284671532846715e-05, 'epoch': 1.33}\n",
      "{'loss': 0.2911, 'grad_norm': 0.31058794260025024, 'learning_rate': 3.266423357664234e-05, 'epoch': 1.34}\n",
      "{'loss': 0.2278, 'grad_norm': 26.323379516601562, 'learning_rate': 3.248175182481752e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1943, 'grad_norm': 36.89936065673828, 'learning_rate': 3.229927007299271e-05, 'epoch': 1.36}\n",
      "{'loss': 0.2322, 'grad_norm': 0.47398218512535095, 'learning_rate': 3.211678832116788e-05, 'epoch': 1.37}\n",
      "{'loss': 0.4456, 'grad_norm': 0.34481504559516907, 'learning_rate': 3.193430656934307e-05, 'epoch': 1.38}\n",
      "{'loss': 0.335, 'grad_norm': 20.06075668334961, 'learning_rate': 3.175182481751825e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3709, 'grad_norm': 3.8078272342681885, 'learning_rate': 3.156934306569343e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3249, 'grad_norm': 0.914741575717926, 'learning_rate': 3.138686131386862e-05, 'epoch': 1.41}\n",
      "{'loss': 0.2307, 'grad_norm': 0.8248832821846008, 'learning_rate': 3.12043795620438e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0799, 'grad_norm': 4.541648864746094, 'learning_rate': 3.102189781021898e-05, 'epoch': 1.43}\n",
      "{'loss': 0.098, 'grad_norm': 0.19060221314430237, 'learning_rate': 3.083941605839416e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2541, 'grad_norm': 0.1483784019947052, 'learning_rate': 3.0656934306569346e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2929, 'grad_norm': 0.25329700112342834, 'learning_rate': 3.0474452554744527e-05, 'epoch': 1.45}\n",
      "{'loss': 0.215, 'grad_norm': 0.2854469418525696, 'learning_rate': 3.029197080291971e-05, 'epoch': 1.46}\n",
      "{'loss': 0.3316, 'grad_norm': 0.4885724186897278, 'learning_rate': 3.010948905109489e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1669, 'grad_norm': 1.107353925704956, 'learning_rate': 2.992700729927008e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2457, 'grad_norm': 0.7464072108268738, 'learning_rate': 2.9744525547445256e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3256, 'grad_norm': 8.983071327209473, 'learning_rate': 2.9562043795620443e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2942, 'grad_norm': 17.070825576782227, 'learning_rate': 2.9379562043795624e-05, 'epoch': 1.51}\n",
      "{'loss': 0.1436, 'grad_norm': 8.516557693481445, 'learning_rate': 2.91970802919708e-05, 'epoch': 1.52}\n",
      "{'loss': 0.409, 'grad_norm': 22.638832092285156, 'learning_rate': 2.9014598540145988e-05, 'epoch': 1.53}\n",
      "{'loss': 0.2724, 'grad_norm': 9.128459930419922, 'learning_rate': 2.883211678832117e-05, 'epoch': 1.54}\n",
      "{'loss': 0.3317, 'grad_norm': 0.7110962271690369, 'learning_rate': 2.8649635036496353e-05, 'epoch': 1.55}\n",
      "{'loss': 0.1346, 'grad_norm': 2.2748355865478516, 'learning_rate': 2.8467153284671533e-05, 'epoch': 1.56}\n",
      "{'loss': 0.1863, 'grad_norm': 4.999617576599121, 'learning_rate': 2.8284671532846717e-05, 'epoch': 1.56}\n",
      "{'loss': 0.1161, 'grad_norm': 12.128273963928223, 'learning_rate': 2.8102189781021898e-05, 'epoch': 1.57}\n",
      "{'loss': 0.1683, 'grad_norm': 6.144622325897217, 'learning_rate': 2.7919708029197085e-05, 'epoch': 1.58}\n",
      "{'loss': 0.1749, 'grad_norm': 2.374804973602295, 'learning_rate': 2.7737226277372262e-05, 'epoch': 1.59}\n",
      "{'loss': 0.1792, 'grad_norm': 3.0614876747131348, 'learning_rate': 2.755474452554745e-05, 'epoch': 1.6}\n",
      "{'loss': 0.4572, 'grad_norm': 29.24116325378418, 'learning_rate': 2.737226277372263e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0852, 'grad_norm': 0.34388449788093567, 'learning_rate': 2.7189781021897807e-05, 'epoch': 1.62}\n",
      "{'loss': 0.2039, 'grad_norm': 1.5720161199569702, 'learning_rate': 2.7007299270072995e-05, 'epoch': 1.63}\n",
      "{'loss': 0.1614, 'grad_norm': 1.541945457458496, 'learning_rate': 2.6824817518248175e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0271, 'grad_norm': 0.6747615933418274, 'learning_rate': 2.664233576642336e-05, 'epoch': 1.65}\n",
      "{'loss': 0.1801, 'grad_norm': 21.335628509521484, 'learning_rate': 2.645985401459854e-05, 'epoch': 1.66}\n",
      "{'loss': 0.1954, 'grad_norm': 0.11601562052965164, 'learning_rate': 2.6277372262773724e-05, 'epoch': 1.67}\n",
      "{'loss': 0.3, 'grad_norm': 0.18991528451442719, 'learning_rate': 2.6094890510948904e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0742, 'grad_norm': 0.1529638171195984, 'learning_rate': 2.591240875912409e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0717, 'grad_norm': 2.776864767074585, 'learning_rate': 2.572992700729927e-05, 'epoch': 1.69}\n",
      "{'loss': 0.3575, 'grad_norm': 0.14020057022571564, 'learning_rate': 2.5547445255474456e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0764, 'grad_norm': 0.16283315420150757, 'learning_rate': 2.5364963503649637e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0841, 'grad_norm': 0.1283709704875946, 'learning_rate': 2.518248175182482e-05, 'epoch': 1.72}\n",
      "{'loss': 0.2123, 'grad_norm': 0.11467273533344269, 'learning_rate': 2.5e-05, 'epoch': 1.73}\n",
      "{'loss': 0.3342, 'grad_norm': 4.040525436401367, 'learning_rate': 2.4817518248175185e-05, 'epoch': 1.74}\n",
      "{'loss': 0.2401, 'grad_norm': 4.591585636138916, 'learning_rate': 2.4635036496350366e-05, 'epoch': 1.75}\n",
      "{'loss': 0.2139, 'grad_norm': 5.797786712646484, 'learning_rate': 2.445255474452555e-05, 'epoch': 1.76}\n",
      "{'loss': 0.3124, 'grad_norm': 4.243651866912842, 'learning_rate': 2.4270072992700734e-05, 'epoch': 1.77}\n",
      "{'loss': 0.1942, 'grad_norm': 0.3549323081970215, 'learning_rate': 2.4087591240875914e-05, 'epoch': 1.78}\n",
      "{'loss': 0.3216, 'grad_norm': 25.92784309387207, 'learning_rate': 2.3905109489051098e-05, 'epoch': 1.79}\n",
      "{'loss': 0.1865, 'grad_norm': 1.55165696144104, 'learning_rate': 2.372262773722628e-05, 'epoch': 1.8}\n",
      "{'loss': 0.2988, 'grad_norm': 3.722073554992676, 'learning_rate': 2.354014598540146e-05, 'epoch': 1.81}\n",
      "{'loss': 0.1923, 'grad_norm': 29.899763107299805, 'learning_rate': 2.3357664233576643e-05, 'epoch': 1.81}\n",
      "{'loss': 0.1925, 'grad_norm': 14.896507263183594, 'learning_rate': 2.3175182481751824e-05, 'epoch': 1.82}\n",
      "{'loss': 0.1539, 'grad_norm': 0.21017082035541534, 'learning_rate': 2.2992700729927008e-05, 'epoch': 1.83}\n",
      "{'loss': 0.1087, 'grad_norm': 2.2367942333221436, 'learning_rate': 2.2810218978102192e-05, 'epoch': 1.84}\n",
      "{'loss': 0.326, 'grad_norm': 7.382864475250244, 'learning_rate': 2.2627737226277372e-05, 'epoch': 1.85}\n",
      "{'loss': 0.4711, 'grad_norm': 0.3131502568721771, 'learning_rate': 2.2445255474452556e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0845, 'grad_norm': 0.2502964735031128, 'learning_rate': 2.226277372262774e-05, 'epoch': 1.87}\n",
      "{'loss': 0.1631, 'grad_norm': 5.2541961669921875, 'learning_rate': 2.208029197080292e-05, 'epoch': 1.88}\n",
      "{'loss': 0.2944, 'grad_norm': 0.5123298764228821, 'learning_rate': 2.1897810218978105e-05, 'epoch': 1.89}\n",
      "{'loss': 0.2515, 'grad_norm': 6.264063835144043, 'learning_rate': 2.1715328467153285e-05, 'epoch': 1.9}\n",
      "{'loss': 0.1502, 'grad_norm': 49.08097457885742, 'learning_rate': 2.1532846715328466e-05, 'epoch': 1.91}\n",
      "{'loss': 0.1541, 'grad_norm': 0.28697410225868225, 'learning_rate': 2.135036496350365e-05, 'epoch': 1.92}\n",
      "{'loss': 0.1398, 'grad_norm': 24.845882415771484, 'learning_rate': 2.1167883211678834e-05, 'epoch': 1.93}\n",
      "{'loss': 0.1408, 'grad_norm': 13.570403099060059, 'learning_rate': 2.0985401459854014e-05, 'epoch': 1.94}\n",
      "{'loss': 0.2202, 'grad_norm': 0.6536846160888672, 'learning_rate': 2.08029197080292e-05, 'epoch': 1.94}\n",
      "{'loss': 0.2606, 'grad_norm': 5.0920915603637695, 'learning_rate': 2.062043795620438e-05, 'epoch': 1.95}\n",
      "{'loss': 0.2779, 'grad_norm': 2.5583908557891846, 'learning_rate': 2.0437956204379563e-05, 'epoch': 1.96}\n",
      "{'loss': 0.1099, 'grad_norm': 0.15033558011054993, 'learning_rate': 2.0255474452554747e-05, 'epoch': 1.97}\n",
      "{'loss': 0.3431, 'grad_norm': 0.08266875892877579, 'learning_rate': 2.0072992700729927e-05, 'epoch': 1.98}\n",
      "{'loss': 0.2793, 'grad_norm': 1.0091238021850586, 'learning_rate': 1.989051094890511e-05, 'epoch': 1.99}\n",
      "{'loss': 0.202, 'grad_norm': 0.2615530788898468, 'learning_rate': 1.9708029197080295e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75c3d577d4e4f10885d1e5c10b2789d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19398297369480133, 'eval_runtime': 6.839, 'eval_samples_per_second': 140.372, 'eval_steps_per_second': 8.773, 'epoch': 2.0}\n",
      "{'loss': 0.2459, 'grad_norm': 4.240015983581543, 'learning_rate': 1.9525547445255476e-05, 'epoch': 2.01}\n",
      "{'loss': 0.1069, 'grad_norm': 0.8892775177955627, 'learning_rate': 1.934306569343066e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0925, 'grad_norm': 17.839080810546875, 'learning_rate': 1.916058394160584e-05, 'epoch': 2.03}\n",
      "{'loss': 0.2696, 'grad_norm': 4.107739448547363, 'learning_rate': 1.897810218978102e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0876, 'grad_norm': 0.11767221987247467, 'learning_rate': 1.8795620437956205e-05, 'epoch': 2.05}\n",
      "{'loss': 0.1582, 'grad_norm': 0.12730038166046143, 'learning_rate': 1.861313868613139e-05, 'epoch': 2.06}\n",
      "{'loss': 0.2463, 'grad_norm': 0.17337104678153992, 'learning_rate': 1.843065693430657e-05, 'epoch': 2.06}\n",
      "{'loss': 0.4185, 'grad_norm': 18.36329460144043, 'learning_rate': 1.8248175182481753e-05, 'epoch': 2.07}\n",
      "{'loss': 0.1457, 'grad_norm': 0.5044969320297241, 'learning_rate': 1.8065693430656934e-05, 'epoch': 2.08}\n",
      "{'loss': 0.236, 'grad_norm': 13.390515327453613, 'learning_rate': 1.7883211678832118e-05, 'epoch': 2.09}\n",
      "{'loss': 0.263, 'grad_norm': 0.5791191458702087, 'learning_rate': 1.7700729927007302e-05, 'epoch': 2.1}\n",
      "{'loss': 0.306, 'grad_norm': 48.14034652709961, 'learning_rate': 1.7518248175182482e-05, 'epoch': 2.11}\n",
      "{'loss': 0.2995, 'grad_norm': 0.4147697985172272, 'learning_rate': 1.7335766423357666e-05, 'epoch': 2.12}\n",
      "{'loss': 0.1643, 'grad_norm': 0.226301372051239, 'learning_rate': 1.715328467153285e-05, 'epoch': 2.13}\n",
      "{'loss': 0.1249, 'grad_norm': 1.0502383708953857, 'learning_rate': 1.6970802919708028e-05, 'epoch': 2.14}\n",
      "{'loss': 0.125, 'grad_norm': 10.04446792602539, 'learning_rate': 1.678832116788321e-05, 'epoch': 2.15}\n",
      "{'loss': 0.282, 'grad_norm': 6.581087589263916, 'learning_rate': 1.6605839416058395e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0086, 'grad_norm': 0.6406977772712708, 'learning_rate': 1.6423357664233576e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0759, 'grad_norm': 0.09758570790290833, 'learning_rate': 1.624087591240876e-05, 'epoch': 2.18}\n",
      "{'loss': 0.187, 'grad_norm': 0.20422443747520447, 'learning_rate': 1.605839416058394e-05, 'epoch': 2.19}\n",
      "{'loss': 0.148, 'grad_norm': 0.14086861908435822, 'learning_rate': 1.5875912408759125e-05, 'epoch': 2.19}\n",
      "{'loss': 0.153, 'grad_norm': 0.1213226392865181, 'learning_rate': 1.569343065693431e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1588, 'grad_norm': 2.6055896282196045, 'learning_rate': 1.551094890510949e-05, 'epoch': 2.21}\n",
      "{'loss': 0.179, 'grad_norm': 18.1203670501709, 'learning_rate': 1.5328467153284673e-05, 'epoch': 2.22}\n",
      "{'loss': 0.3036, 'grad_norm': 4.0438923835754395, 'learning_rate': 1.5145985401459855e-05, 'epoch': 2.23}\n",
      "{'loss': 0.107, 'grad_norm': 0.14823326468467712, 'learning_rate': 1.496350364963504e-05, 'epoch': 2.24}\n",
      "{'loss': 0.079, 'grad_norm': 0.2694200873374939, 'learning_rate': 1.4781021897810221e-05, 'epoch': 2.25}\n",
      "{'loss': 0.1142, 'grad_norm': 0.5480892658233643, 'learning_rate': 1.45985401459854e-05, 'epoch': 2.26}\n",
      "{'loss': 0.3024, 'grad_norm': 7.612310886383057, 'learning_rate': 1.4416058394160584e-05, 'epoch': 2.27}\n",
      "{'loss': 0.3197, 'grad_norm': 12.109585762023926, 'learning_rate': 1.4233576642335767e-05, 'epoch': 2.28}\n",
      "{'loss': 0.2161, 'grad_norm': 0.45524102449417114, 'learning_rate': 1.4051094890510949e-05, 'epoch': 2.29}\n",
      "{'loss': 0.215, 'grad_norm': 0.11474527418613434, 'learning_rate': 1.3868613138686131e-05, 'epoch': 2.3}\n",
      "{'loss': 0.2239, 'grad_norm': 0.7469318509101868, 'learning_rate': 1.3686131386861315e-05, 'epoch': 2.31}\n",
      "{'loss': 0.1584, 'grad_norm': 0.4865613281726837, 'learning_rate': 1.3503649635036497e-05, 'epoch': 2.31}\n",
      "{'loss': 0.1124, 'grad_norm': 0.4829758107662201, 'learning_rate': 1.332116788321168e-05, 'epoch': 2.32}\n",
      "{'loss': 0.3387, 'grad_norm': 0.2056412249803543, 'learning_rate': 1.3138686131386862e-05, 'epoch': 2.33}\n",
      "{'loss': 0.146, 'grad_norm': 96.59685516357422, 'learning_rate': 1.2956204379562046e-05, 'epoch': 2.34}\n",
      "{'loss': 0.1342, 'grad_norm': 15.029600143432617, 'learning_rate': 1.2773722627737228e-05, 'epoch': 2.35}\n",
      "{'loss': 0.1408, 'grad_norm': 3.972198963165283, 'learning_rate': 1.259124087591241e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0899, 'grad_norm': 16.707111358642578, 'learning_rate': 1.2408759124087593e-05, 'epoch': 2.37}\n",
      "{'loss': 0.2043, 'grad_norm': 0.1578131467103958, 'learning_rate': 1.2226277372262775e-05, 'epoch': 2.38}\n",
      "{'loss': 0.1586, 'grad_norm': 0.08522962778806686, 'learning_rate': 1.2043795620437957e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0787, 'grad_norm': 0.12188362330198288, 'learning_rate': 1.186131386861314e-05, 'epoch': 2.4}\n",
      "{'loss': 0.101, 'grad_norm': 6.093868732452393, 'learning_rate': 1.1678832116788322e-05, 'epoch': 2.41}\n",
      "{'loss': 0.2694, 'grad_norm': 0.20033028721809387, 'learning_rate': 1.1496350364963504e-05, 'epoch': 2.42}\n",
      "{'loss': 0.2955, 'grad_norm': 0.2674160897731781, 'learning_rate': 1.1313868613138686e-05, 'epoch': 2.43}\n",
      "{'loss': 0.3028, 'grad_norm': 33.90544891357422, 'learning_rate': 1.113138686131387e-05, 'epoch': 2.44}\n",
      "{'loss': 0.1907, 'grad_norm': 0.18016457557678223, 'learning_rate': 1.0948905109489052e-05, 'epoch': 2.44}\n",
      "{'loss': 0.2333, 'grad_norm': 0.24831433594226837, 'learning_rate': 1.0766423357664233e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0757, 'grad_norm': 1.6836082935333252, 'learning_rate': 1.0583941605839417e-05, 'epoch': 2.46}\n",
      "{'loss': 0.1656, 'grad_norm': 3.8062033653259277, 'learning_rate': 1.04014598540146e-05, 'epoch': 2.47}\n",
      "{'loss': 0.3144, 'grad_norm': 0.21045559644699097, 'learning_rate': 1.0218978102189781e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0768, 'grad_norm': 4.108794212341309, 'learning_rate': 1.0036496350364964e-05, 'epoch': 2.49}\n",
      "{'loss': 0.3227, 'grad_norm': 34.025787353515625, 'learning_rate': 9.854014598540148e-06, 'epoch': 2.5}\n",
      "{'loss': 0.2436, 'grad_norm': 0.4783933162689209, 'learning_rate': 9.67153284671533e-06, 'epoch': 2.51}\n",
      "{'loss': 0.2321, 'grad_norm': 2.1021382808685303, 'learning_rate': 9.48905109489051e-06, 'epoch': 2.52}\n",
      "{'loss': 0.1753, 'grad_norm': 24.647502899169922, 'learning_rate': 9.306569343065694e-06, 'epoch': 2.53}\n",
      "{'loss': 0.2536, 'grad_norm': 22.161121368408203, 'learning_rate': 9.124087591240877e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0857, 'grad_norm': 0.18267226219177246, 'learning_rate': 8.941605839416059e-06, 'epoch': 2.55}\n",
      "{'loss': 0.243, 'grad_norm': 8.810466766357422, 'learning_rate': 8.759124087591241e-06, 'epoch': 2.56}\n",
      "{'loss': 0.3726, 'grad_norm': 3.9875662326812744, 'learning_rate': 8.576642335766425e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1297, 'grad_norm': 8.026987075805664, 'learning_rate': 8.394160583941606e-06, 'epoch': 2.57}\n",
      "{'loss': 0.2845, 'grad_norm': 4.002756595611572, 'learning_rate': 8.211678832116788e-06, 'epoch': 2.58}\n",
      "{'loss': 0.4791, 'grad_norm': 38.08171081542969, 'learning_rate': 8.02919708029197e-06, 'epoch': 2.59}\n",
      "{'loss': 0.1247, 'grad_norm': 0.26937660574913025, 'learning_rate': 7.846715328467154e-06, 'epoch': 2.6}\n",
      "{'loss': 0.1869, 'grad_norm': 4.020223617553711, 'learning_rate': 7.664233576642336e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0446, 'grad_norm': 65.686767578125, 'learning_rate': 7.48175182481752e-06, 'epoch': 2.62}\n",
      "{'loss': 0.1274, 'grad_norm': 0.29415470361709595, 'learning_rate': 7.2992700729927e-06, 'epoch': 2.63}\n",
      "{'loss': 0.2385, 'grad_norm': 7.765114784240723, 'learning_rate': 7.116788321167883e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0688, 'grad_norm': 0.1306288242340088, 'learning_rate': 6.9343065693430655e-06, 'epoch': 2.65}\n",
      "{'loss': 0.2519, 'grad_norm': 0.6288871169090271, 'learning_rate': 6.751824817518249e-06, 'epoch': 2.66}\n",
      "{'loss': 0.2439, 'grad_norm': 3.9508187770843506, 'learning_rate': 6.569343065693431e-06, 'epoch': 2.67}\n",
      "{'loss': 0.1114, 'grad_norm': 0.16552723944187164, 'learning_rate': 6.386861313868614e-06, 'epoch': 2.68}\n",
      "{'loss': 0.2009, 'grad_norm': 0.1259353905916214, 'learning_rate': 6.204379562043796e-06, 'epoch': 2.69}\n",
      "{'loss': 0.1917, 'grad_norm': 0.244041308760643, 'learning_rate': 6.0218978102189786e-06, 'epoch': 2.69}\n",
      "{'loss': 0.2633, 'grad_norm': 3.7213332653045654, 'learning_rate': 5.839416058394161e-06, 'epoch': 2.7}\n",
      "{'loss': 0.2138, 'grad_norm': 0.404834121465683, 'learning_rate': 5.656934306569343e-06, 'epoch': 2.71}\n",
      "{'loss': 0.1047, 'grad_norm': 0.3822720944881439, 'learning_rate': 5.474452554744526e-06, 'epoch': 2.72}\n",
      "{'loss': 0.1623, 'grad_norm': 0.604572594165802, 'learning_rate': 5.2919708029197084e-06, 'epoch': 2.73}\n",
      "{'loss': 0.1073, 'grad_norm': 0.1382262259721756, 'learning_rate': 5.109489051094891e-06, 'epoch': 2.74}\n",
      "{'loss': 0.3864, 'grad_norm': 3.3636763095855713, 'learning_rate': 4.927007299270074e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0698, 'grad_norm': 0.14923423528671265, 'learning_rate': 4.744525547445255e-06, 'epoch': 2.76}\n",
      "{'loss': 0.2616, 'grad_norm': 22.737110137939453, 'learning_rate': 4.562043795620438e-06, 'epoch': 2.77}\n",
      "{'loss': 0.3066, 'grad_norm': 0.18129505217075348, 'learning_rate': 4.379562043795621e-06, 'epoch': 2.78}\n",
      "{'loss': 0.2876, 'grad_norm': 25.055286407470703, 'learning_rate': 4.197080291970803e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1641, 'grad_norm': 0.2261570245027542, 'learning_rate': 4.014598540145985e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1961, 'grad_norm': 38.17695236206055, 'learning_rate': 3.832116788321168e-06, 'epoch': 2.81}\n",
      "{'loss': 0.1574, 'grad_norm': 0.6032215356826782, 'learning_rate': 3.64963503649635e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0978, 'grad_norm': 0.21763397753238678, 'learning_rate': 3.4671532846715328e-06, 'epoch': 2.82}\n",
      "{'loss': 0.1698, 'grad_norm': 0.1729101538658142, 'learning_rate': 3.2846715328467155e-06, 'epoch': 2.83}\n",
      "{'loss': 0.1452, 'grad_norm': 0.22007280588150024, 'learning_rate': 3.102189781021898e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0549, 'grad_norm': 0.313767671585083, 'learning_rate': 2.9197080291970804e-06, 'epoch': 2.85}\n",
      "{'loss': 0.2112, 'grad_norm': 0.42687293887138367, 'learning_rate': 2.737226277372263e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0869, 'grad_norm': 0.10430824756622314, 'learning_rate': 2.5547445255474454e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0762, 'grad_norm': 0.11928918957710266, 'learning_rate': 2.3722627737226276e-06, 'epoch': 2.88}\n",
      "{'loss': 0.1422, 'grad_norm': 0.13053879141807556, 'learning_rate': 2.1897810218978103e-06, 'epoch': 2.89}\n",
      "{'loss': 0.1148, 'grad_norm': 0.12232078611850739, 'learning_rate': 2.0072992700729926e-06, 'epoch': 2.9}\n",
      "{'loss': 0.2246, 'grad_norm': 0.1609068512916565, 'learning_rate': 1.824817518248175e-06, 'epoch': 2.91}\n",
      "{'loss': 0.146, 'grad_norm': 0.15394479036331177, 'learning_rate': 1.6423357664233577e-06, 'epoch': 2.92}\n",
      "{'loss': 0.1973, 'grad_norm': 3.9535927772521973, 'learning_rate': 1.4598540145985402e-06, 'epoch': 2.93}\n",
      "{'loss': 0.2102, 'grad_norm': 66.45625305175781, 'learning_rate': 1.2773722627737227e-06, 'epoch': 2.94}\n",
      "{'loss': 0.1752, 'grad_norm': 13.113862037658691, 'learning_rate': 1.0948905109489052e-06, 'epoch': 2.94}\n",
      "{'loss': 0.175, 'grad_norm': 0.1340145617723465, 'learning_rate': 9.124087591240875e-07, 'epoch': 2.95}\n",
      "{'loss': 0.1577, 'grad_norm': 0.15904024243354797, 'learning_rate': 7.299270072992701e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0971, 'grad_norm': 0.2619703710079193, 'learning_rate': 5.474452554744526e-07, 'epoch': 2.97}\n",
      "{'loss': 0.2786, 'grad_norm': 0.5985182523727417, 'learning_rate': 3.6496350364963505e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0679, 'grad_norm': 0.2590939402580261, 'learning_rate': 1.8248175182481753e-07, 'epoch': 2.99}\n",
      "{'loss': 0.2867, 'grad_norm': 4.633773326873779, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7bcb6f950c4c06ade3269db18104a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2287292629480362, 'eval_runtime': 6.8262, 'eval_samples_per_second': 140.634, 'eval_steps_per_second': 8.79, 'epoch': 3.0}\n",
      "{'train_runtime': 888.4348, 'train_samples_per_second': 29.172, 'train_steps_per_second': 3.647, 'train_loss': 0.24201513725666352, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3240, training_loss=0.24201513725666352, metrics={'train_runtime': 888.4348, 'train_samples_per_second': 29.172, 'train_steps_per_second': 3.647, 'total_flos': 1704762305441280.0, 'train_loss': 0.24201513725666352, 'epoch': 3.0})"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c12e4201614688aa6c348bf864948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1208.8350\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predictions and calculate accuracy\n",
    "predicted_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
    "accuracy = (predicted_labels == y_test_bert).sum().item() / len(y_test_bert)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.602404  -2.5929542] tensor([0])\n"
     ]
    }
   ],
   "source": [
    "print(predictions.predictions[0], y_test_bert[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1)  # Shape: [2400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bert = y_test_bert.view(-1)  # Shape: [2400]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predicted_labels.to(dtype=torch.long)\n",
    "y_test_bert = y_test_bert.to(dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9363\n"
     ]
    }
   ],
   "source": [
    "accuracy = (predicted_labels == y_test_bert).sum().item() / len(y_test_bert)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels Shape: torch.Size([2400])\n",
      "y_test_bert Shape: torch.Size([2400])\n",
      "Sample Predictions: tensor([0, 1, 0, 1, 1])\n",
      "Sample Ground Truth: tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted Labels Shape: {predicted_labels.shape}\")\n",
    "print(f\"y_test_bert Shape: {y_test_bert.shape}\")\n",
    "print(f\"Sample Predictions: {predicted_labels[:5]}\")\n",
    "print(f\"Sample Ground Truth: {y_test_bert[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9363\n",
      "Precision (0): 0.9214, Recall (0): 0.9392, F1-score (0): 0.9302\n",
      "Precision (1): 0.9490, Recall (1): 0.9338, F1-score (1): 0.9413\n",
      "Macro Avg Precision: 0.9352, Macro Avg Recall: 0.9365, Macro Avg F1-score: 0.9358\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.94      0.93      1086\n",
      "     Class 1       0.95      0.93      0.94      1314\n",
      "\n",
      "    accuracy                           0.94      2400\n",
      "   macro avg       0.94      0.94      0.94      2400\n",
      "weighted avg       0.94      0.94      0.94      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Assuming predicted_labels and y_test_bert are already defined and aligned\n",
    "\n",
    "# Convert tensors to NumPy arrays for compatibility with sklearn\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "y_test_bert_np = y_test_bert.cpu().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_bert_np, predicted_labels_np)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_bert_np, predicted_labels_np, average=None, labels=[0, 1])\n",
    "macro_avg = precision_recall_fscore_support(y_test_bert_np, predicted_labels_np, average='macro')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (0): {precision[0]:.4f}, Recall (0): {recall[0]:.4f}, F1-score (0): {f1[0]:.4f}\")\n",
    "print(f\"Precision (1): {precision[1]:.4f}, Recall (1): {recall[1]:.4f}, F1-score (1): {f1[1]:.4f}\")\n",
    "print(f\"Macro Avg Precision: {macro_avg[0]:.4f}, Macro Avg Recall: {macro_avg[1]:.4f}, Macro Avg F1-score: {macro_avg[2]:.4f}\")\n",
    "\n",
    "# Optional: Generate and display a classification report\n",
    "report = classification_report(y_test_bert_np, predicted_labels_np, target_names=['Class 0', 'Class 1'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
