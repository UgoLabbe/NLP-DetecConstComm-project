{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "By December 15 you shall have implemented multiple baseline solutions to\n",
    "your main text classification task. These should include both deep learning (DL) based methods\n",
    "such as those introduced in Weeks 5-6 but also non-DL models such as those shown in Week 3.\n",
    "Baselines can also include simple rule-based methods (e.g. keyword matching or regular expres-\n",
    "sions). Each baseline should be evaluated both quantitatively and qualitatively, more details will\n",
    "be provided in the lecture on text classification (Week 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import conllu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = 0_0\n",
      "# text = And this Conservative strategy has produced the angry and desperate wing-nuts like the fellow who called reporters 'lying pieces of Sh*t' this week.\n",
      "1\tAnd\tand\tCCONJ\tCC\t_\tNone\tNone\t_\t_\n",
      "2\tthis\tthis\tDET\tDT\tNumber=Sing|PronType=Dem\tNone\tNone\t_\t_\n",
      "3\tConservative\tConservative\tADJ\tJJ\tDegree=Pos\tNone\tNone\t_\t_\n"
     ]
    }
   ],
   "source": [
    "# Load the entire .conllu file into a Python variable\n",
    "conllu_data = []\n",
    "with open('Data/preprocessed_dataset.conllu', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        conllu_data.append(line.strip())\n",
    "\n",
    "# Display a sample of the loaded data (e.g., first 5 lines)\n",
    "print(\"\\n\".join(conllu_data[:5]))\n",
    "\n",
    "anns = pd.read_table('Data/annotations.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of sent_id : X_Y\n",
    "\n",
    "Where X is the id of the comment, while Y is the id of the sentence.\n",
    "\n",
    "Example : 2_3 means it's comment with id 2 and its 3rd sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First experiment: use of Naive Bayes classification\n",
    "\n",
    "1. Extraction of the comment only\n",
    "2. Vectorization\n",
    "3. Splitting into training and testing data\n",
    "4. Training the model using different parameters\n",
    "5. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_group_conllu_to_list(file_path):\n",
    "    \"\"\"\n",
    "    Load the .conllu file and group sentences by comment ID, returning a list of concatenated texts.\n",
    "    \"\"\"\n",
    "    comments_list = []\n",
    "    current_comment_id = None\n",
    "    current_text = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('# sent_id ='):\n",
    "                # Extract the comment ID (X) from 'sent_id = X_Y'\n",
    "                sent_id = line.split('=')[1].strip()\n",
    "                comment_id = sent_id.split('_')[0]\n",
    "\n",
    "                # Check if we've moved to a new comment\n",
    "                if current_comment_id is not None and comment_id != current_comment_id:\n",
    "                    # Store the completed text for the previous comment\n",
    "                    if current_text:\n",
    "                        comments_list.append(\" \".join(current_text))\n",
    "                    current_text = []\n",
    "\n",
    "                # Update the current comment ID\n",
    "                current_comment_id = comment_id\n",
    "\n",
    "            elif line.startswith('# text ='):\n",
    "                # Extract the text for the current sentence\n",
    "                sentence_text = line.split('=')[1].strip()\n",
    "                current_text.append(sentence_text)\n",
    "\n",
    "        # Add the last comment if any\n",
    "        if current_comment_id is not None and current_text:\n",
    "            comments_list.append(\" \".join(current_text))\n",
    "\n",
    "    return comments_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extraction of the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"And this Conservative strategy has produced the angry and desperate wing-nuts like the fellow who called reporters 'lying pieces of Sh*t' this week. The fortunate thing is that reporters were able to report it and broadcast it - which may shake up a few folks who recognize a bit of themselves somewhere in there and do some reflecting. I live in hope.\",\n",
       " \"I commend Harper for holding the debates outside of a left-wing forum as this will help prevent the left from manipulating the debates to try to make Harper look bad. Indeed, we’ll finally have some fair debates. Trudeau is a coward and the only one who’s opposing this as he’s terrified about losing left-wing protection during the debates if the debates are held elsewhere. If Trudeau doesn’t have Chretien or Martin speaking for him or isn't currently in training to learn how to handle himself in a debate, he has May attending the debates to hold his little hand. If Trudeau can’t speak for himself or handle debates, how does he expect to run a country?\",\n",
       " \"What a joke Rachel Notley is. This is what was posted on the NDP website on the last World Press Freedom Day. She can't even follow her own leader. She should resign. Immediately. “Today, we pay tribute to journalists who have lost their lives or been injured in the line of duty. It is unacceptable that, in 2015, journalists are still prosecuted, detained and assassinated for doing their jobs. Independent media is of immeasurable value in a free and democratic society, and it is our duty to ensure that this continues to be respected and protected. “We also take this opportunity to stand in solidarity with journalists and Canadians who have been affected by the Conservative’s reckless cuts to CBC/ Radio-Canada. “When we attack the freedom of the press, we’re attacking our own people, democracy and freedom. “New Democrats thank journalists and the media for their invaluable work informing the public. Let’s work together to defend freedom of the press. ” http://www.ndp.ca/news/statement-ndp-world-press-freedom-day\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Data/preprocessed_dataset.conllu'\n",
    "comments_list = load_and_group_conllu_to_list(file_path)\n",
    "comments_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vectorization (using TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(comments_list)\n",
    "y = anns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Splitting into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Best cross-validation accuracy: 0.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ugo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}  # Adjust the range as needed\n",
    "model = MultinomialNB()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6875\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.48      0.58      1086\n",
      "           1       0.67      0.86      0.75      1314\n",
      "\n",
      "    accuracy                           0.69      2400\n",
      "   macro avg       0.70      0.67      0.67      2400\n",
      "weighted avg       0.70      0.69      0.67      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second experiment: Use Feature based models \n",
    "\n",
    "Idea: Use the features from the CONLL-U format to feed Machine Learning models (SVM, Random Forest, KNN and else) in order to classify the text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
