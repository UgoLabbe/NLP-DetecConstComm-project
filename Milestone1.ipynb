{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic X: Bring your own topic!\n",
    "You are encouraged to propose your own topic! Please note the following criteria:\n",
    "\n",
    "• the topic should include a text classification task at its core and there should be some\n",
    "annotated training data available for this task, otherwise milestones 1 and 2 cannot be\n",
    "completed. If you are unsure whether your topic is suitable, we are happy to advise you.\n",
    "\n",
    "• you are still required to work in teams of 4, so you should assemble a team to work on the\n",
    "project (if necessary you can also bring in external members who are not registered for the\n",
    "course)\n",
    "\n",
    "• you should contact the exercise coordinator (G ́abor Recski) about your topic proposal, we\n",
    "can discuss your ideas and recommend 1-2 instructors who can act as your mentors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "data_path = os.path.join('Data', 'C3_anonymized.csv')\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between column 'comment_text' and 'pp_comment_text':\n",
    "\n",
    "Seems like it's a \"pre-cleaned\" text column:\n",
    "- remove hyphen (')\n",
    "- Added whitespace before-after points, coma, apostrophe \n",
    "- kept - in words such as 'left-wing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    print(\"Comment text:\\n\",df[\"comment_text\"][i],\"\\n\\npp Comment text:\\n\",df[\"pp_comment_text\"][i],\"\\n-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset with untreated comment annotation about constructiveness (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno = df[['comment_text','constructive_binary']].copy()\n",
    "\n",
    "#Change the constructive binary column to int (1 or 0)\n",
    "df_anno['constructive_binary'] = df_anno['constructive_binary'].astype(int)\n",
    "\n",
    "print(df_anno['constructive_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the constructive and non-constructive comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Randomly sample 3 constructive comments\n",
    "random_constructive = df_anno[df_anno['constructive_binary'] == 1].sample(n=3, random_state=42)\n",
    "\n",
    "# Randomly sample 3 non-constructive comments\n",
    "random_non_constructive = df_anno[df_anno['constructive_binary'] == 0].sample(n=3, random_state=42)\n",
    "\n",
    "# Display the results\n",
    "print(\"Random Constructive Comments:\")\n",
    "print(random_constructive['comment_text'])\n",
    "print(\"\\nRandom Non-Constructive Comments:\")\n",
    "print(random_non_constructive['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some possible issues that we saw:\n",
    "- use of slang like \"gonna, gunna\" for \"going to\"\n",
    "- abreviations\n",
    "- spelling mistakes\n",
    "- telling if a comment is constructive or not can be highly subjective. That's most likely why a non-binary annotation column exist, most fitted for a regression task\n",
    "\n",
    "but overall the texts seems cleans in general\n",
    "\n",
    "Now let's build a function to see which words are the most used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies and download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_most_used_words(text_list, top_n=10, language='english'):\n",
    "    #Summarizes the most used words in a list of text, excluding stopwords.\n",
    "\n",
    "    # Load the stopwords for the given language\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    \n",
    "    # Combine all texts into one large string\n",
    "    all_text = ' '.join(text_list)\n",
    "    \n",
    "    # Convert to lowercase and remove punctuation using regex\n",
    "    all_text_cleaned = re.sub(r'[^\\w\\s]', '', all_text.lower())\n",
    "    \n",
    "    # Split into words\n",
    "    words = word_tokenize(all_text_cleaned, language='english')\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Count word frequencies and get the most common\n",
    "    word_counts = Counter(filtered_words)\n",
    "    most_common_words = word_counts.most_common(top_n)\n",
    "    \n",
    "    return most_common_words, word_counts.__len__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the most used words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global\n",
    "print(\"top 10 most used words (without stopwords):\\n\")\n",
    "print(summarize_most_used_words(df_anno['comment_text'], top_n=10, language='english'))\n",
    "\n",
    "#Constructive comments\n",
    "print(\"\\ntop 10 most used words (without stopwords) in constructive comments:\\n\")\n",
    "print(summarize_most_used_words(df_anno[df_anno['constructive_binary']==1]['comment_text'], top_n=10, language='english'))\n",
    "\n",
    "#Non-constructive comments\n",
    "print(\"\\ntop 10 most used words (without stopwords) in non-constructive comments:\\n\")\n",
    "print(summarize_most_used_words(df_anno[df_anno['constructive_binary']==0]['comment_text'], top_n=10, language='english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential issue with this data is that many of the most frequently used words appear in both constructive and non-constructive comments. This occurs in different proportions because, as shown below, constructive comments tend to be longer than non-constructive ones.\n",
    "\n",
    "\n",
    "| Rank | General Words         | Constructive Words       | Non-Constructive Words     |\n",
    "|------|-----------------------|--------------------------|----------------------------|\n",
    "| 1    | people (2817)         | people (2464)            | harper (607)               |\n",
    "| 2    | would (2666)          | would (2265)             | canada (428)               |\n",
    "| 3    | canada (2528)         | canada (2100)            | like (422)                 |\n",
    "| 4    | harper (2444)         | harper (1837)            | would (401)                |\n",
    "| 5    | one (2190)            | one (1824)               | one (366)                  |\n",
    "| 6    | like (2128)           | like (1706)              | people (353)               |\n",
    "| 7    | us (1833)             | us (1597)                | get (295)                  |\n",
    "| 8    | dont (1691)           | government (1427)        | globe (288)                |\n",
    "| 9    | government (1654)     | dont (1404)              | dont (287)                 |\n",
    "| 10   | get (1566)            | get (1271)               | trudeau (276)              |\n",
    "\n",
    "Also, it's likely that due to the length of the comments, constructive comments will have a larger vocabulary. This might cause a model to become biased, interpreting unknown words as constructive words.\n",
    "\n",
    "| Category                | General Words | Constructive Words | Non-Constructive Words |\n",
    "|-------------------------|---------------|--------------------|------------------------|\n",
    "| Total Number of Words   | 36,493        | 33,124             | 13,562                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if the character length and the average word length is different depending on if the comment is constructive or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the avg word length in a comment\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 0:\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Create a length column\n",
    "df_anno['text_length'] = df_anno['comment_text'].apply(len)\n",
    "df_anno['avg_word_length'] = df_anno['comment_text'].apply(avg_word_length)\n",
    "\n",
    "\n",
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# First plot: Jittered strip plot for text length\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.stripplot(x='constructive_binary', y='text_length', data=df_anno, jitter=True, alpha=0.5)\n",
    "plt.title('Jittered Strip Plot of Text Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Text Length (in characters)')\n",
    "\n",
    "# Second plot: Jittered strip plot for average word length\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.stripplot(x='constructive_binary', y='avg_word_length', data=df_anno, jitter=True, alpha=0.5)\n",
    "plt.title('Jittered Strip Plot of Average Word Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Average Word Length')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite easy to see that constructive comments can lead to a big text, only a few non constructive comments go over 1000 characters while it's quite common for constructive ones. However the average world length plot is quite similar between the 2 classes, even though we see longer words in non constructive comments, which likely are mistakes (it would be surprising to find a comment where the average word length is above 100!). Let's find out about that last hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# First plot: Box plot for text length\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='constructive_binary', y='text_length', data=df_anno)\n",
    "plt.title('Box Plot of Text Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Text Length (in characters)')\n",
    "\n",
    "# Second plot: Box plot for average word length\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='constructive_binary', y='avg_word_length', data=df_anno)\n",
    "plt.title('Box Plot of Average Word Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Average Word Length')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Plot proportion of constructive comments vs non-constructive comments\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.countplot(x='constructive_binary', data=df_anno)\n",
    "plt.title('Proportion of Constructive Comments vs Non-Constructive Comments')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Number of Comments')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_word_comments = df_anno[df_anno['avg_word_length'] > 35]\n",
    "for comment in long_word_comments['comment_text']:\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for Pre-processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_text_length = df_anno['text_length'].min()\n",
    "min_text_length\n",
    "## All comments have at least 10 characters, which is acceptable.\n",
    "\n",
    "# Check for japanese characters\n",
    "japanese_comments = df_anno[df_anno['comment_text'].str.contains('[\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff\\uff66-\\uff9f]')]\n",
    "print(japanese_comments)\n",
    "\n",
    "# Drop the japanese comments\n",
    "df_anno = df_anno.drop(japanese_comments.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "from stanza.utils.conll import CoNLL\n",
    "from stanza.models.common.doc import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langeuage of the input text\n",
    "language = 'english'\n",
    "\n",
    "# Load the stopwords for the given language\n",
    "stop_words = set(stopwords.words(language))\n",
    "\n",
    "# Create a Stanza pipeline for the English language\n",
    "nlp = stanza.Pipeline(lang=language, processors='tokenize,lemma,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tokenization(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    return words\n",
    "\n",
    "def text_normalization(words,stop_words):\n",
    "    # Remove punctuation\n",
    "    normalized_words = [word for word in words if re.match('\\w', word)]\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in normalized_words if word not in stop_words]\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "def apply_lemmatization(nlp, words, language='english'):\n",
    "    # Apply lemmatization to a list of words concatenated as a string\n",
    "    doc = nlp(' '.join(words))\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, nlp, stop_words, language='english'):\n",
    "    # Tokenization\n",
    "    words  = apply_tokenization(text)\n",
    "\n",
    "    # Normalization\n",
    "    words_filtered = text_normalization(words, stop_words)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_text = apply_lemmatization(nlp, words_filtered, language)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes, we will preprocess the first 5 comments\n",
    "# df_anno_sample = df_anno.head(5).copy()\n",
    "# docs = (df_anno_sample['comment_text'].apply(lambda x: preprocess_text(x, nlp, stop_words))).tolist()\n",
    "\n",
    "# Actual preprocessing of all comments - this is slow for testing use the above code and comment this out\n",
    "docs = (df_anno['comment_text'].apply(lambda x: preprocess_text(x, nlp, stop_words))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\n",
    "    word.lemma\n",
    "    for comment in docs \n",
    "    for sentence in comment.sentences \n",
    "    for word in sentence.words\n",
    "        if word.lemma.lower() not in stop_words and re.match('\\w', word.lemma)\n",
    ").most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "for comment in docs[:5]:\n",
    "    for sentence in comment.sentences:\n",
    "        for word in sentence.words:\n",
    "            print(f'Word: {word.text.ljust(15)}\\tLemma: {word.lemma.ljust(15)}\\tPOS: {word.pos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CoNLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(docs))\n",
    "print(type(docs[0]))\n",
    "print(type(docs[0].sentences))\n",
    "print(type(docs[0].sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to `preprocessed_text.conllu`\n",
    "preprocessed_data_path = os.path.join('output', 'data', 'preprocessed_text.conllu')\n",
    "\n",
    "# Store preprocessed text \n",
    "def process_comments_to_conllu(docs, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for doc in docs:\n",
    "            # Convert the document to CoNLL format\n",
    "            conll_text = CoNLL.convert_dict(doc.to_dict())\n",
    "            \n",
    "            # Write the CoNLL formatted text to the output file\n",
    "            for sentence in conll_text:\n",
    "                for token in sentence:\n",
    "                    outfile.write('\\t'.join(token) + \"\\n\")\n",
    "                outfile.write(\"\\n\")  # Line break to separate sentences\n",
    "            outfile.write(\"\\n\")  # Line break to separate documents\n",
    "\n",
    "\n",
    "process_comments_to_conllu(docs, preprocessed_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CoNLL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to `preprocessed_text.conllu`\n",
    "preprocessed_data_path = os.path.join('output', 'data', 'preprocessed_text.conllu')\n",
    "\n",
    "# Open and parse the file\n",
    "with open(preprocessed_data_path, 'r', encoding='utf-8') as f:\n",
    "    print(''.join(f.readlines()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
