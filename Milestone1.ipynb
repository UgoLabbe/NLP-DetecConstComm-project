{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic X: Bring your own topic!\n",
    "You are encouraged to propose your own topic! Please note the following criteria:\n",
    "\n",
    "• the topic should include a text classification task at its core and there should be some\n",
    "annotated training data available for this task, otherwise milestones 1 and 2 cannot be\n",
    "completed. If you are unsure whether your topic is suitable, we are happy to advise you.\n",
    "\n",
    "• you are still required to work in teams of 4, so you should assemble a team to work on the\n",
    "project (if necessary you can also bring in external members who are not registered for the\n",
    "course)\n",
    "\n",
    "• you should contact the exercise coordinator (G ́abor Recski) about your topic proposal, we\n",
    "can discuss your ideas and recommend 1-2 instructors who can act as your mentors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "data_path = os.path.join('Data', 'C3_anonymized.csv')\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between column 'comment_text' and 'pp_comment_text':\n",
    "\n",
    "Seems like it's a \"pre-cleaned\" text column:\n",
    "- remove hyphen (')\n",
    "- Added whitespace before-after points, coma, apostrophe \n",
    "- kept - in words such as 'left-wing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    print(\"Comment text:\\n\",df[\"comment_text\"][i],\"\\n\\npp Comment text:\\n\",df[\"pp_comment_text\"][i],\"\\n-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset with untreated comment annotation about constructiveness (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno = df[['comment_text','constructive_binary']].copy()\n",
    "\n",
    "#Change the constructive binary column to int (1 or 0)\n",
    "df_anno['constructive_binary'] = df_anno['constructive_binary'].astype(int)\n",
    "\n",
    "print(df_anno['constructive_binary'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the constructive and non-constructive comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Randomly sample 3 constructive comments\n",
    "random_constructive = df_anno[df_anno['constructive_binary'] == 1].sample(n=3, random_state=42)\n",
    "\n",
    "# Randomly sample 3 non-constructive comments\n",
    "random_non_constructive = df_anno[df_anno['constructive_binary'] == 0].sample(n=3, random_state=42)\n",
    "\n",
    "# Display the results\n",
    "print(\"Random Constructive Comments:\")\n",
    "print(random_constructive['comment_text'])\n",
    "print(\"\\nRandom Non-Constructive Comments:\")\n",
    "print(random_non_constructive['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some possible issues that we saw:\n",
    "- use of slang like \"gonna, gunna\" for \"going to\"\n",
    "- abreviations\n",
    "- spelling mistakes\n",
    "- telling if a comment is constructive or not can be highly subjective. That's most likely why a non-binary annotation column exist, most fitted for a regression task\n",
    "\n",
    "but overall the texts seems cleans in general\n",
    "\n",
    "Now let's build a function to see which words are the most used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies and download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_most_used_words(text_list, top_n=10, language='english'):\n",
    "    #Summarizes the most used words in a list of text, excluding stopwords.\n",
    "\n",
    "    # Load the stopwords for the given language\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    \n",
    "    # Combine all texts into one large string\n",
    "    all_text = ' '.join(text_list)\n",
    "    \n",
    "    # Convert to lowercase and remove punctuation using regex\n",
    "    all_text_cleaned = re.sub(r'[^\\w\\s]', '', all_text.lower())\n",
    "    \n",
    "    # Split into words\n",
    "    words = word_tokenize(all_text_cleaned, language='english')\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Count word frequencies and get the most common\n",
    "    word_counts = Counter(filtered_words)\n",
    "    most_common_words = word_counts.most_common(top_n)\n",
    "    \n",
    "    return most_common_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the most used words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global\n",
    "print(\"top 10 most used words (without stopwords):\\n\")\n",
    "print(summarize_most_used_words(df_anno['comment_text'], top_n=10, language='english'))\n",
    "\n",
    "#Constructive comments\n",
    "print(\"\\ntop 10 most used words (without stopwords) in constructive comments:\\n\")\n",
    "print(summarize_most_used_words(df_anno[df_anno['constructive_binary']==1]['comment_text'], top_n=10, language='english'))\n",
    "\n",
    "#Non-constructive comments\n",
    "print(\"\\ntop 10 most used words (without stopwords) in non-constructive comments:\\n\")\n",
    "print(summarize_most_used_words(df_anno[df_anno['constructive_binary']==0]['comment_text'], top_n=10, language='english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if the character length and the average word length is different depending on if the comment is constructive or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the avg word length in a comment\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 0:\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Create a length column\n",
    "df_anno['text_length'] = df_anno['comment_text'].apply(len)\n",
    "df_anno['avg_word_length'] = df_anno['comment_text'].apply(avg_word_length)\n",
    "\n",
    "\n",
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# First plot: Jittered strip plot for text length\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.stripplot(x='constructive_binary', y='text_length', data=df_anno, jitter=True, alpha=0.5)\n",
    "plt.title('Jittered Strip Plot of Text Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Text Length (in characters)')\n",
    "\n",
    "# Second plot: Jittered strip plot for average word length\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.stripplot(x='constructive_binary', y='avg_word_length', data=df_anno, jitter=True, alpha=0.5)\n",
    "plt.title('Jittered Strip Plot of Average Word Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Average Word Length')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite easy to see that constructive comments can lead to a big text, only a few non constructive comments go over 1000 characters while it's quite common for constructive ones. However the average world length plot is quite similar between the 2 classes, even though we see longer words in non constructive comments, which likely are mistakes (it would be surprising to find a comment where the average word length is above 100!). Let's find out about that last hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# First plot: Box plot for text length\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='constructive_binary', y='text_length', data=df_anno)\n",
    "plt.title('Box Plot of Text Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Text Length (in characters)')\n",
    "\n",
    "# Second plot: Box plot for average word length\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='constructive_binary', y='avg_word_length', data=df_anno)\n",
    "plt.title('Box Plot of Average Word Length by Constructiveness')\n",
    "plt.xlabel('Constructive (0 = Not Constructive, 1 = Constructive)')\n",
    "plt.ylabel('Average Word Length')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_word_comments = df_anno[df_anno['avg_word_length'] > 35]\n",
    "for comment in long_word_comments['comment_text']:\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for Pre-processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
